<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.8">
<title>Nodes</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | http://asciidoctor.org */
/* Uncomment @import statement below to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}
audio,canvas,video{display:inline-block}
audio:not([controls]){display:none;height:0}
script{display:none!important}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:transparent}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:none}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite::before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt,table tr:nth-of-type(even){background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
*:not(pre)>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background-color:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed;word-wrap:break-word}
*:not(pre)>code.nobreak{word-wrap:normal}
*:not(pre)>code.nowrap{white-space:nowrap}
pre,pre>code{line-height:1.45;color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;text-rendering:optimizeSpeed}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background-color:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background-color:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background-color:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock pre:not(.highlight),.listingblock pre[class="highlight"],.listingblock pre[class^="highlight "],.listingblock pre.CodeRay,.listingblock pre.prettyprint{background:#f7f7f8}
.sidebarblock .literalblock pre,.sidebarblock .listingblock pre:not(.highlight),.sidebarblock .listingblock pre[class="highlight"],.sidebarblock .listingblock pre[class^="highlight "],.sidebarblock .listingblock pre.CodeRay,.sidebarblock .listingblock pre.prettyprint{background:#f2f1f1}
.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:1em}}
.literalblock pre.nowrap,.literalblock pre.nowrap pre,.listingblock pre.nowrap,.listingblock pre.nowrap pre{white-space:pre;word-wrap:normal}
.literalblock.output pre{color:#f7f7f8;background-color:rgba(0,0,0,.9)}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:#999}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:#999}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
table.pyhltable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.pyhltable td{vertical-align:top;padding-top:0;padding-bottom:0;line-height:1.45}
table.pyhltable td.code{padding-left:.75em;padding-right:0}
pre.pygments .lineno,table.pyhltable td:not(.code){color:#999;padding-left:0;padding-right:.5em;border-right:1px solid #dddddf}
pre.pygments .lineno{display:inline-block;margin-right:.25em}
table.pyhltable .linenodiv{background:none!important;padding-right:0!important}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt,.quoteblock .quoteblock{margin:0 0 1.25em;padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;text-align:left;margin-right:0}
table.tableblock{max-width:100%;border-collapse:separate}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>thead>tr>.tableblock,table.grid-all>tbody>tr>.tableblock{border-width:0 1px 1px 0}
table.grid-all>tfoot>tr>.tableblock{border-width:1px 1px 0 0}
table.grid-cols>*>tr>.tableblock{border-width:0 1px 0 0}
table.grid-rows>thead>tr>.tableblock,table.grid-rows>tbody>tr>.tableblock{border-width:0 0 1px}
table.grid-rows>tfoot>tr>.tableblock{border-width:1px 0 0}
table.grid-all>*>tr>.tableblock:last-child,table.grid-cols>*>tr>.tableblock:last-child{border-right-width:0}
table.grid-all>tbody>tr:last-child>.tableblock,table.grid-all>thead:last-child>tr>.tableblock,table.grid-rows>tbody>tr:last-child>.tableblock,table.grid-rows>thead:last-child>tr>.tableblock{border-bottom-width:0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot,table.frame-ends{border-width:1px 0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd){background:#f8f8f7}
table.stripes-none tr,table.stripes-odd tr:nth-of-type(even){background:none}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
td>div.verse{white-space:pre}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background-color:#00fafa}
.black{color:#000}
.black-background{background-color:#000}
.blue{color:#0000bf}
.blue-background{background-color:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background-color:#fa00fa}
.gray{color:#606060}
.gray-background{background-color:#7d7d7d}
.green{color:#006000}
.green-background{background-color:#007d00}
.lime{color:#00bf00}
.lime-background{background-color:#00fa00}
.maroon{color:#600000}
.maroon-background{background-color:#7d0000}
.navy{color:#000060}
.navy-background{background-color:#00007d}
.olive{color:#606000}
.olive-background{background-color:#7d7d00}
.purple{color:#600060}
.purple-background{background-color:#7d007d}
.red{color:#bf0000}
.red-background{background-color:#fa0000}
.silver{color:#909090}
.silver-background{background-color:#bcbcbc}
.teal{color:#006060}
.teal-background{background-color:#007d7d}
.white{color:#bfbfbf}
.white-background{background-color:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background-color:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background-color:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background-color:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
</head>
<body class="article">
<div id="header">
<h1>Nodes</h1>
</div>
<div id="content">
<div class="sect1">
<h2 id="_working-with-pods">Working with pods</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="about-pods">Using pods in OpenShift Enterprise</h3>
<div class="paragraph">
<p>A <em>pod</em> is one or more containers deployed together on one host, and the smallest compute unit that can be defined,
deployed, and managed.</p>
</div>
<div class="sect3">
<h4 id="nodes-pods-using-about-nodes-pods-using">Understanding pods in OpenShift Enterprise</h4>
<div class="paragraph">
<p>Pods are the rough equivalent of a machine instance (physical or virtual) to a Container. Each pod is allocated its own internal IP address, therefore owning its entire port space, and Containers within pods can share their local storage and networking.</p>
</div>
<div class="paragraph">
<p>Pods have a lifecycle; they are defined, then they are assigned to run on
a node, then they run until their Container(s) exit or they are removed
for some other reason. Pods, depending on policy and exit code, might be
removed after exiting, or can be retained in order to enable access to
the logs of their Containers.</p>
</div>
<div class="paragraph">
<p>OpenShift Enterprise treats pods as largely immutable; changes cannot be made to
a pod definition while it is running. OpenShift Enterprise implements changes by
terminating an existing pod and recreating it with modified configuration,
base image(s), or both. Pods are also treated as expendable, and do not
maintain state when recreated. Therefore pods should usually be managed by
higher-level controllers, rather than directly by users.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>For the maximum number of pods per OpenShift Enterprise node host, see the Cluster Limits.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Bare pods that are not managed by a replication controller will be not rescheduled upon node disruption.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-using-example-nodes-pods-using">Example pod configurations in OpenShift Enterprise</h4>
<div class="paragraph">
<p>OpenShift Enterprise leverages the Kubernetes concept of a <em>pod</em>, which is one or more Containers deployed
together on one host, and the smallest compute unit that can be defined,
deployed, and managed.</p>
</div>
<div class="paragraph">
<p>The following is an example definition of a pod that provides a long-running
service, which is actually a part of the OpenShift Enterprise infrastructure: the
integrated Container image registry. It demonstrates many features of pods, most of
which are discussed in other topics and thus only briefly mentioned here:</p>
</div>
<div id="example-pod-definition-nodes-pods-using" class="listingblock">
<div class="title">Pod Object Definition (YAML)</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: Pod
apiVersion: v1
metadata:
  name: example
  namespace: default
  selfLink: /api/v1/namespaces/default/pods/example
  uid: 5cc30063-0265780783bc
  resourceVersion: '165032'
  creationTimestamp: '2019-02-13T20:31:37Z'
  labels:                  <b class="conum">(1)</b>
    app: hello-openshift
  annotations:
    openshift.io/scc: anyuid
spec:
  restartPolicy: Always      <b class="conum">(2)</b>
  serviceAccountName: default
  imagePullSecrets:
    - name: default-dockercfg-5zrhb
  priority: 0
  schedulerName: default-scheduler
  terminationGracePeriodSeconds: 30
  nodeName: ip-10-0-140-16.us-east-2.compute.internal
  securityContext:     <b class="conum">(3)</b>
    seLinuxOptions:
      level: 's0:c11,c10'
  containers:          <b class="conum">(4)</b>
    - resources: {}
      terminationMessagePath: /dev/termination-log
      name: hello-openshift
      securityContext:
        capabilities:
          drop:
            - MKNOD
        procMount: Default
      ports:
        - containerPort: 8080
          protocol: TCP
      imagePullPolicy: Always
      volumeMounts:             <b class="conum">(5)</b>
        - name: default-token-wbqsl
          readOnly: true
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      terminationMessagePolicy: File
      image: quay.io/openshift/origin-logging-eventrouter:latest <b class="conum">(6)</b>
  serviceAccount: default     <b class="conum">(7)</b>
  volumes:                    <b class="conum">(8)</b>
    - name: default-token-wbqsl
      secret:
        secretName: default-token-wbqsl
        defaultMode: 420
  dnsPolicy: ClusterFirst
status:
  phase: Pending
  conditions:
    - type: Initialized
      status: 'True'
      lastProbeTime: null
      lastTransitionTime: '2019-02-13T20:31:37Z'
    - type: Ready
      status: 'False'
      lastProbeTime: null
      lastTransitionTime: '2019-02-13T20:31:37Z'
      reason: ContainersNotReady
      message: 'containers with unready status: [hello-openshift]'
    - type: ContainersReady
      status: 'False'
      lastProbeTime: null
      lastTransitionTime: '2019-02-13T20:31:37Z'
      reason: ContainersNotReady
      message: 'containers with unready status: [hello-openshift]'
    - type: PodScheduled
      status: 'True'
      lastProbeTime: null
      lastTransitionTime: '2019-02-13T20:31:37Z'
  hostIP: 10.0.140.16
  startTime: '2019-02-13T20:31:37Z'
  containerStatuses:
    - name: hello-openshift
      state:
        waiting:
          reason: ContainerCreating
      lastState: {}
      ready: false
      restartCount: 0
      image: openshift/hello-openshift
      imageID: ''
  qosClass: BestEffort</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Pods can be "tagged" with one or more labels, which can then
be used to select and manage groups of pods in a single operation. The labels
are stored in key/value format in the <code><strong>metadata</strong></code> hash. One label in this
example is <strong>registry=default</strong>.</p>
</li>
<li>
<p>The pod restart policy with possible values <code>Always</code>, <code>OnFailure</code>, and <code>Never</code>. The default value is <code>Always</code>.</p>
</li>
<li>
<p>OpenShift Enterprise defines a security context for Containers which specifies whether they are allowed to run as
privileged Containers, run as a user of their choice, and more. The default context is very restrictive
but administrators can modify this as needed.</p>
</li>
<li>
<p><code><strong>containers</strong></code> specifies an array of Container definitions; in this case (as
with most), just one.</p>
</li>
<li>
<p>The Container specifies where external storage volumes should be mounted
within the Container. In this case, there is a volume for storing the registry&#8217;s
data, and one for access to credentials the registry needs for making requests
against the OpenShift Enterprise API.</p>
</li>
<li>
<p>Each Container in the pod is instantiated from its own Container image.</p>
</li>
<li>
<p>Pods making requests against the OpenShift Enterprise API is a common enough pattern
that there is a <code><strong>serviceAccount</strong></code> field for specifying which service account user the pod should
authenticate as when making the requests. This enables fine-grained access
control for custom infrastructure components.</p>
</li>
<li>
<p>The pod defines storage volumes that are available to its Container(s) to
use. In this case, it provides an ephemeral volume for the registry storage and
a <code><strong>secret</strong></code> volume containing the service account credentials.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>This pod definition does not include attributes that
are filled by OpenShift Enterprise automatically after the pod is created and
its lifecycle begins. The
<a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Kubernetes pod documentation</a> has details about the functionality and purpose of pods.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="viewing-pods">Viewing pods in OpenShift Enterprise</h3>
<div class="paragraph">
<p>As an administrator, you can view the pods in your cluster and to determine the health of those pods and the cluster as a whole.</p>
</div>
<div class="sect3">
<h4 id="nodes-pods-about-nodes-pods-viewing">About pods in OpenShift Enterprise</h4>
<div class="paragraph">
<p>OpenShift Enterprise leverages the Kubernetes concept of a <em>pod</em>, which is one or more containers deployed
together on one host, and the smallest compute unit that can be defined,
deployed, and managed. Pods are the rough equivalent of a machine instance (physical or virtual) to a container.</p>
</div>
<div class="paragraph">
<p>You can view a list of pods associated with a specific project or view usage statistics about pods.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-viewing-project-nodes-pods-viewing">Viewing pods in a project</h4>
<div class="paragraph">
<p>You can view a list of pods associated with the current project, including the number of replica, the current status, number or restarts and the age of the pod.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To view the pods in your cluster:</p>
</div>
<div class="paragraph">
<p>View the pods in the current project:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pods</pre>
</div>
</div>
<div class="paragraph">
<p>+
For example:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pods -n openshift-console
NAME                       READY   STATUS    RESTARTS   AGE
console-698d866b78-bnshf   1/1     Running   2          165m
console-698d866b78-m87pm   1/1     Running   2          165m</pre>
</div>
</div>
<div class="paragraph">
<p>+
Add the <code>-o wide</code> flags to view the pod IP address and the node where the pod is located.</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pods -o wide

NAME                       READY   STATUS    RESTARTS   AGE    IP            NODE                           NOMINATED NODE
console-698d866b78-bnshf   1/1     Running   2          166m   10.128.0.24   ip-10-0-152-71.ec2.internal    &lt;none&gt;
console-698d866b78-m87pm   1/1     Running   2          166m   10.129.0.23   ip-10-0-173-237.ec2.internal   &lt;none&gt;</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-viewing-usage-nodes-pods-viewing">Viewing pod usage statistics</h4>
<div class="paragraph">
<p>You can display usage statistics about pods, which provide the runtime
environments for Containers. These usage statistics include CPU, memory, and
storage consumption.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You must have <code>cluster-reader</code> permission to view the usage statistics.</p>
</li>
<li>
<p>Metrics must be installed to view the usage statistics.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To view the usage statistics:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Run the following command:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm top pods</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm top pods -n openshift-console
NAME                         CPU(cores)   MEMORY(bytes)
console-7f58c69899-q8c8k     0m           22Mi
console-7f58c69899-xhbgg     0m           25Mi
downloads-594fcccf94-bcxk8   3m           18Mi
downloads-594fcccf94-kv4p6   2m           15Mi</pre>
</div>
</div>
</li>
<li>
<p>Run the following command to view the usage statistics for pods with labels:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm top pod --selector=''</pre>
</div>
</div>
<div class="paragraph">
<p>You must choose the selector (label query) to filter on. Supports <code>=</code>, <code>==</code>, and <code>!=</code>.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="configuring-a-cluster-for-pods">Configuring an OpenShift Enterprise cluster for pods</h3>
<div class="paragraph">
<p>As an administrator, you can create and maintain an efficient cluster for pods.</p>
</div>
<div class="paragraph">
<p>By keeping your cluster efficient, you can provide a better environment for your developers using
such tools as what a pod does when it exits, ensuring that the required number of pods is always running,
when to restart pods designed to run only once, limit the bandwidth available to pods, and how to keep
pods running during disruptions.</p>
</div>
<div class="sect3">
<h4 id="nodes-pods-configuring-restart-nodes-pods-configuring">Configuring how pods behave after restart</h4>
<div class="paragraph">
<p>A pod restart policy determines how OpenShift Enterprise responds when Containers in that pod exit.
The policy applies to all Containers in that pod.</p>
</div>
<div class="paragraph">
<p>The possible values are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Always</code> - Tries restarting a successfully exited Container on the pod continuously, with an exponential back-off delay (10s, 20s, 40s) until the pod is restarted. The default is <code>Always</code>.</p>
</li>
<li>
<p><code>OnFailure</code> - Tries restarting a failed Container on the pod with an exponential back-off delay (10s, 20s, 40s) capped at 5 minutes.</p>
</li>
<li>
<p><code>Never</code> - Does not try to restart exited or failed Containers on the pod. Pods immediately fail and exit.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>After the pod is bound to a node, the pod will never be bound to another node. This means that a controller is necessary in order for a pod to survive node failure:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Condition</th>
<th class="tableblock halign-left valign-top">Controller Type</th>
<th class="tableblock halign-left valign-top">Restart Policy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pods that are expected to terminate (such as batch computations)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Job</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>OnFailure</code> or <code>Never</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pods that are expected to not terminate (such as web servers)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Replication Controller</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Always</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pods that need to run one-per-machine</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Daemonset</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Any</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>If a Container on a pod fails and the restart policy is set to <code>OnFailure</code>, the pod stays on the node and the Container is restarted. If you do not want the Container to
restart, use a restart policy of <code>Never</code>.</p>
</div>
<div class="paragraph">
<p>If an entire pod fails, OpenShift Enterprise starts a new pod. Developers need to address the possibility that applications might be restarted in a new pod. In particular,
applications need to handle temporary files, locks, incomplete output, and so forth caused by previous runs.</p>
</div>
<div class="paragraph">
<p>For details on how OpenShift Enterprise uses restart policy with failed Containers, see
the <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states">Example States</a> in the Kubernetes documentation.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-configuring-run-once-nodes-pods-configuring">Limiting the duration of run-once pods</h4>
<div class="paragraph">
<p>OpenShift Enterprise relies on run-once pods to perform tasks such as deploying a pod
or performing a build. Run-once pods are pods that have a <code>RestartPolicy</code> of
<code>Never</code> or <code>OnFailure</code>.</p>
</div>
<div class="paragraph">
<p>The cluster administrator can use the <code>RunOnceDuration</code> admission control
plug-in to force a limit on the time that those run-once pods can be active.
Once the time limit expires, the cluster will try to actively terminate those
pods. The main reason to have such a limit is to prevent tasks such as builds to
run for an excessive amount of time.</p>
</div>
<div class="paragraph">
<p>The plug-in configuration should include the default active deadline for
run-once pods. This deadline is enforced globally, but can be superseded on
a per-project basis.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To install the <code>RunOnceDuration</code> admission controller:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create an <strong>AdmissionConfiguration</strong> object that references the file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: AdmissionConfiguration
apiVersion: apiserver.k8s.io/v1alpha1
plugins:
- name: RunOnceDurationConfig
  activeDeadlineSecondsOverride: 3600 <b class="conum">(1)</b></code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify the global default for run-once pods in seconds.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-configuring-bandwidth-nodes-pods-configuring">Limiting the bandwidth available to pods</h4>
<div class="paragraph">
<p>You can apply quality-of-service traffic shaping to a pod and effectively limit
its available bandwidth. Egress traffic (from the pod) is handled by policing,
which simply drops packets in excess of the configured rate. Ingress traffic (to
the pod) is handled by shaping queued packets to effectively handle data. The
limits you place on a pod do not affect the bandwidth of other pods.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To limit the bandwidth on a pod:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Write an object definition JSON file, and specify the data traffic speed using
<code>kubernetes.io/ingress-bandwidth</code> and <code>kubernetes.io/egress-bandwidth</code>
annotations. For example, to limit both pod egress and ingress bandwidth to 10M/s:</p>
<div class="listingblock">
<div class="title">Limited Pod Object Definition</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">{
    "kind": "Pod",
    "spec": {
        "containers": [
            {
                "image": "openshift/hello-openshift",
                "name": "hello-openshift"
            }
        ]
    },
    "apiVersion": "v1",
    "metadata": {
        "name": "iperf-slow",
        "annotations": {
            "kubernetes.io/ingress-bandwidth": "10M",
            "kubernetes.io/egress-bandwidth": "10M"
        }
    }
}</code></pre>
</div>
</div>
</li>
<li>
<p>Create the pod using the object definition:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file_or_dir_path&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-configuring-pod-distruption-about-nodes-pods-configuring">Understanding how to use pod disruption budgets to specify the number of pods that must be up</h4>
<div class="paragraph">
<p>A <em>pod disruption budget</em> is part of the
<a href="http://kubernetes.io/docs/admin/disruptions/">Kubernetes</a> API, which can be
managed with <code>oc</code> commands like other object types. They
allow the specification of safety constraints on pods during operations, such as
draining a node for maintenance.</p>
</div>
<div class="paragraph">
<p><code>PodDisruptionBudget</code> is an API object that specifies the minimum number or
percentage of replicas that must be up at a time. Setting these in projects can
be helpful during node maintenance (such as scaling a cluster down or a cluster
upgrade) and is only honored on voluntary evictions (not on node failures).</p>
</div>
<div class="paragraph">
<p>A <code>PodDisruptionBudget</code> object&#8217;s configuration consists of the following key
parts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A label selector, which is a label query over a set of pods.</p>
</li>
<li>
<p>An availability level, which specifies the minimum number of pods that must be
available simultaneously.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can check for pod disruption budgets across all projects with the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get poddisruptionbudget --all-namespaces

NAMESPACE         NAME          MIN-AVAILABLE   SELECTOR
another-project   another-pdb   4               bar=foo
test-project      my-pdb        2               foo=bar</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>PodDisruptionBudget</code> is considered healthy when there are at least
<code>minAvailable</code> pods running in the system. Every pod above that limit can be evicted.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Depending on your pod priority and preemption settings,
lower-priority pods might be removed despite their pod disruption budget requirements.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="nodes-pods-configuring-pod-distruption-configuring-nodes-pods-configuring">Specifying the number of pods that must be up with pod disruption budgets</h5>
<div class="paragraph">
<p>You can use a <code>PodDisruptionBudget</code> object to specify the minimum number or
percentage of replicas that must be up at a time.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To configure a pod disruption budget:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a YAML file with the an object definition similar to the following:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: policy/v1beta1 <b class="conum">(1)</b>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  selector:  <b class="conum">(2)</b>
    matchLabels:
      foo: bar
  minAvailable: 2  <b class="conum">(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>PodDisruptionBudget</code> is part of the <code>policy/v1beta1</code> API group.</p>
</li>
<li>
<p>A label query over a set of resources. The result of <code>matchLabels</code> and
<code>matchExpressions</code> are logically conjoined.</p>
</li>
<li>
<p>The minimum number of pods that must be available simultaneously. This can
be either an integer or a string specifying a percentage (for example, <code>20%</code>).</p>
</li>
</ol>
</div>
</li>
<li>
<p>Run the following command to add the object to project:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;/path/to/file&gt; -n &lt;project_name&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-configuring-critical-nodes-pods-configuring">Preventing pod removal using critical pods</h4>
<div class="paragraph">
<p>There are a number of core components that are critical to a fully functional cluster,
but, run on a regular cluster node rather than the master. A cluster might stop working properly if a critical add-on is evicted.</p>
</div>
<div class="paragraph">
<p>Pods marked as critical are not allowed to be evicted.</p>
</div>
<div class="paragraph">
<p>,Procedure</p>
</div>
<div class="paragraph">
<p>To make a pod critical:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a pod specification or edit existing pods to include the <code>system-cluster-critical</code> priority class:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  template:
    metadata:
      name: critical-pod
    priorityClassName: system-cluster-critical <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Default priority class for pods that should never be evicted from a node.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Alternatively, you can specify <code>system-node-critical</code> for pods that are important to the cluster
but can be removed if necessary.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="providing-sensitive-data-to-pods">Providing sensitive data to pods in OpenShift Enterprise</h3>
<div class="paragraph">
<p>Some applications need sensitive information, such as passwords and user names, that you do not want developers to have.</p>
</div>
<div class="paragraph">
<p>As an administrator, you can use <em>Secret</em> objects to provide this information without exposing that information in clear text.</p>
</div>
<div class="sect3">
<h4 id="nodes-pods-secrets-about-nodes-pods-secrets">Understanding secrets</h4>
<div class="paragraph">
<p>The <code>Secret</code> object type provides a mechanism to hold sensitive information such
as passwords, OpenShift Enterprise client configuration files,
private source repository credentials, and so on. Secrets decouple sensitive
content from the pods. You can mount secrets into Containers using a volume
plug-in or the system can use secrets to perform actions on behalf of a pod.</p>
</div>
<div class="paragraph">
<p>Key properties include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Secret data can be referenced independently from its definition.</p>
</li>
<li>
<p>Secret data volumes are backed by temporary file-storage facilities (tmpfs) and never come to rest on a node.</p>
</li>
<li>
<p>Secret data can be shared within a namespace.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">YAML Secret Object Definition</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
  name: test-secret
  namespace: my-namespace
type: Opaque <b class="conum">(1)</b>
data: <b class="conum">(2)</b>
  username: dmFsdWUtMQ0K <b class="conum">(3)</b>
  password: dmFsdWUtMg0KDQo=
stringData: <b class="conum">(4)</b>
  hostname: myapp.mydomain.com <b class="conum">(5)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Indicates the structure of the secrets key names and values.</p>
</li>
<li>
<p>The allowable format for the keys in the <code>data</code> field must meet the
guidelines in the <strong>DNS_SUBDOMAIN</strong> value in
<a href="https://github.com/kubernetes/kubernetes/blob/v1.0.0/docs/design/identifiers.md">the
Kubernetes identifiers glossary</a>.</p>
</li>
<li>
<p>The value associated with keys in the <code><strong>data</strong></code> map must be base64 encoded.</p>
</li>
<li>
<p>Entries in the <code><strong>stringData</strong></code> map are converted to base64
and the entry will then be moved to the <code><strong>data</strong></code> map automatically. This field
is write-only; the value will only be returned via the <code><strong>data</strong></code> field.</p>
</li>
<li>
<p>The value associated with keys in the <code><strong>stringData</strong></code> map is made up of
plain text strings.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>You must create a secret before creating the pods that depend on that secret.</p>
</div>
<div class="paragraph">
<p>When creating secrets:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create a secret object with secret data.</p>
</li>
<li>
<p>Update the pod&#8217;s service account to allow the reference to the secret.</p>
</li>
<li>
<p>Create a pod, which consumes the secret as an environment variable or as a file
(using a <code>secret</code> volume).</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="nodes-pods-secrets-about-types-nodes-pods-secrets">Types of secrets</h5>
<div class="paragraph">
<p>The value in the <code>type</code> field indicates the structure of the secret&#8217;s key names and values. The type can be used to
enforce the presence of user names and keys in the secret object. If you do not want validation, use the <strong>opaque</strong> type,
which is the default.</p>
</div>
<div class="paragraph">
<p>Specify one of the following types to trigger minimal server-side validation to ensure the presence of specific key names in the secret data:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>kubernetes.io/service-account-token</code>. Uses a service account token.</p>
</li>
<li>
<p><code>kubernetes.io/basic-auth</code>. Use with Basic Authentication.</p>
</li>
<li>
<p><code>kubernetes.io/ssh-auth</code>. Use with SSH Key Authentication.</p>
</li>
<li>
<p><code>kubernetes.io/tls</code>. Use with TLS certificate authorities.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Specify <code>type: Opaque</code> if you do not want validation, which means the secret does not claim to conform to any convention for key names or values.
An <em>opaque</em> secret, allows for unstructured <code>key:value</code> pairs that can contain arbitrary values.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>You can specify other arbitrary types, such as <code>example.com/my-secret-type</code>. These types are not enforced server-side,
but indicate that the creator of the secret intended to conform to the key/value requirements of that type.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For examples of differet secret types, see the code samples in <em>Using Secrets</em>.</p>
</div>
</div>
<div class="sect4">
<h5 id="nodes-pods-secrets-about-examples-nodes-pods-secrets">Example secret configurations</h5>
<div class="paragraph">
<p>The following are sample secret configuration files.</p>
</div>
<div class="listingblock">
<div class="title">YAML Secret That Will Create Four Files</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
  name: test-secret
data:
  username: dmFsdWUtMQ0K     <b class="conum">(1)</b>
  password: dmFsdWUtMQ0KDQo= <b class="conum">(2)</b>
stringData:
  hostname: myapp.mydomain.com <b class="conum">(3)</b>
  secret.properties: |-     <b class="conum">(4)</b>
    property1=valueA
    property2=valueB</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>File contains decoded values.</p>
</li>
<li>
<p>File contains decoded values.</p>
</li>
<li>
<p>File contains the provided string.</p>
</li>
<li>
<p>File contains the provided data.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">YAML of a Pod Populating Files in a Volume with Secret Data</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: secret-example-pod
spec:
  containers:
    - name: secret-test-container
      image: busybox
      command: [ "/bin/sh", "-c", "cat /etc/secret-volume/*" ]
      volumeMounts:
          # name must match the volume name below
          - name: secret-volume
            mountPath: /etc/secret-volume
            readOnly: true
  volumes:
    - name: secret-volume
      secret:
        secretName: test-secret
  restartPolicy: Never</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">YAML of a Pod Populating Environment Variables with Secret Data</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: secret-example-pod
spec:
  containers:
    - name: secret-test-container
      image: busybox
      command: [ "/bin/sh", "-c", "export" ]
      env:
        - name: TEST_SECRET_USERNAME_ENV_VAR
          valueFrom:
            secretKeyRef:
              name: test-secret
              key: username
  restartPolicy: Never</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">YAML of a Build Config Populating Environment Variables with Secret Data</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: BuildConfig
metadata:
  name: secret-example-bc
spec:
  strategy:
    sourceStrategy:
      env:
      - name: TEST_SECRET_USERNAME_ENV_VAR
        valueFrom:
          secretKeyRef:
            name: test-secret
            key: username</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="nodes-pods-secrets-about-keys-nodes-pods-secrets">Secret data keys</h5>
<div class="paragraph">
<p>Secret keys must be in a DNS subdomain.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-secrets-creating-nodes-pods-secrets">Understanding how to create secrets</h4>
<div class="paragraph">
<p>As an administrator you must create a secret before developers can create the pods that depend on that secret.</p>
</div>
<div class="paragraph">
<p>When creating secrets:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create a secret object with secret data.</p>
</li>
<li>
<p>Update the pod&#8217;s service account to allow the reference to the secret.</p>
</li>
<li>
<p>Create a pod, which consumes the secret as an environment variable or as a file
(using a <code>secret</code> volume).</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="_secret-creation-restrictions">Secret creation restrictions</h5>
<div class="paragraph">
<p>To use a secret, a pod needs to reference the secret. A secret can be used with
a pod in three ways:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To populate environment variables for Containers.</p>
</li>
<li>
<p>As files in a volume mounted on one or more of its Containers.</p>
</li>
<li>
<p>By kubelet when pulling images for the pod.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Volume type secrets write data into the Container as a file using the volume
mechanism. Image pull secrets use service accounts for the automatic injection of
the secret into all pods in a namespaces.</p>
</div>
<div class="paragraph">
<p>When a template contains a secret definition, the only way for the template to
use the provided secret is to ensure that the secret volume sources are
validated and that the specified object reference actually points to an object
of type <code><strong>Secret</strong></code>. Therefore, a secret needs to be created before any pods that
depend on it. The most effective way to ensure this is to have it get injected
automatically through the use of a service account.</p>
</div>
<div class="paragraph">
<p>Secret API objects reside in a namespace. They can only be referenced by pods in
that same namespace.</p>
</div>
<div class="paragraph">
<p>Individual secrets are limited to 1MB in size. This is to discourage the
creation of large secrets that could exhaust apiserver and kubelet memory.
However, creation of a number of smaller secrets could also exhaust memory.</p>
</div>
</div>
<div class="sect4">
<h5 id="nodes-pods-secrets-creating-opaque-nodes-pods-secrets">Creating an opaque secret</h5>
<div class="paragraph">
<p>As an administrator, you can create a opaque secret, which allows for unstructured <code>key:value</code> pairs that can contain arbitrary values.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a secret object in a YAML file on master.</p>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque <b class="conum">(1)</b>
data:
  username: dXNlci1uYW1l
  password: cGFzc3dvcmQ=</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specifies an opaque secret.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Use the following command to create a secret object:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;filename&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Then:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Update the service account for the pod where you want to use the secret to allow the reference to the secret.</p>
</li>
<li>
<p>Create the pod, which consumes the secret as an environment variable or as a file
(using a <code>secret</code> volume).</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-secrets-updating-nodes-pods-secrets">Understanding how to update secrets</h4>
<div class="paragraph">
<p>When you modify the value of a secret, the value (used by an already running
pod) will not dynamically change. To change a secret, you must delete the
original pod and create a new pod (perhaps with an identical PodSpec).</p>
</div>
<div class="paragraph">
<p>Updating a secret follows the same workflow as deploying a new Container image.
You can use the <code>kubectl rolling-update</code> command.</p>
</div>
<div class="paragraph">
<p>The <code>resourceVersion</code> value in a secret is not specified when it is referenced.
Therefore, if a secret is updated at the same time as pods are starting, then
the version of the secret will be used for the pod will not be defined.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Currently, it is not possible to check the resource version of a secret object
that was used when a pod was created. It is planned that pods will report this
information, so that a controller could restart ones using a old
<code><strong>resourceVersion</strong></code>. In the interim, do not update the data of existing secrets,
but create new ones with distinct names.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-secrets-certificates-about-nodes-pods-secrets">About using signed certificates with secrets</h4>
<div class="paragraph">
<p>To secure communication to your service, you can configure OpenShift Enterprise to generate a signed
serving certificate/key pair that you can add into a secret in a project.</p>
</div>
<div class="paragraph">
<p>A <em>service serving certificate secret</em> is intended to support complex middleware
applications that need out-of-the-box certificates. It has the same settings as
the server certificates generated by the administrator tooling for nodes and
masters.</p>
</div>
<div class="listingblock">
<div class="title">Service pod specification configured for a service serving certificates secret.</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
  kind: Service
  metadata:
    name: registry
    annotations:
      service.alpha.openshift.io/serving-cert-secret-name: registry-cert<b class="conum">(1)</b>
....</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify the name for the certificate</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Other pods can trust cluster-created certificates (which are only signed for
internal DNS names), by using the CA bundle in the
<strong><em>/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt</em></strong> file that is
automatically mounted in their pod.</p>
</div>
<div class="paragraph">
<p>The signature algorithm for this feature is <code>x509.SHA256WithRSA</code>. To manually
rotate, delete the generated secret. A new certificate is created.</p>
</div>
<div class="sect4">
<h5 id="nodes-pods-secrets-certificates-creating-nodes-pods-secrets">Generating signed certificates for use with secrets</h5>
<div class="paragraph">
<p>To use a signed serving certificate/key pair with a pod, create or edit the service to add
the <code>service.alpha.openshift.io/serving-cert-secret-name</code> annotation, then add the secret to the pod.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To create a <em>service serving certificate secret</em>:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Edit the pod specification for your service.</p>
</li>
<li>
<p>Add the <code>service.alpha.openshift.io/serving-cert-secret-name</code> annotation
with the name you want to use for your secret.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: Service
apiVersion: v1
metadata:
  name: my-service
  annotations:
      service.alpha.openshift.io/serving-cert-secret-name: my-cert <b class="conum">(1)</b>
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376</code></pre>
</div>
</div>
<div class="paragraph">
<p>The certificate and key are in PEM format, stored in <code>tls.crt</code> and <code>tls.key</code>
respectively.</p>
</div>
</li>
<li>
<p>Create the service:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
<li>
<p>View the secret to make sure it was created:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get secrets

NAME                         TYPE                                  DATA      AGE
my-cert                  kubernetes.io/tls                     2         9m

$ oc describe secret my-service-pod
Name:         my-service-pod
Namespace:    openshift-console
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/service-account.name: builder
              kubernetes.io/service-account.uid: ab-11e9-988a-0eb4e1b4a396

Type:  kubernetes.io/service-account-token

Data

ca.crt:     5802 bytes
namespace:  17 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Ii
wia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJvcGVuc2hpZnQtY29uc29sZSIsImt1YmVyb
cnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhYmE4Y2UyZC00MzVlLTExZTktOTg4YS0wZWI0ZTFiNGEz
OTYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6b3BlbnNoaWZ</pre>
</div>
</div>
</li>
<li>
<p>Edit your pod specification with that secret.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-service-pod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
  volumes:
  - name: foo
    secret:
      secretName: my-cert
      items:
      - key: username
        path: my-group/my-username
        mode: 511</code></pre>
</div>
</div>
<div class="paragraph">
<p>When it is available, your pod will run.
The certificate will be good for the internal service DNS name,
<code>&lt;service.name&gt;.&lt;service.namespace&gt;.svc</code>.</p>
</div>
<div class="paragraph">
<p>The certificate/key pair is automatically replaced when it gets
close to expiration. View the expiration date in the
<code>service.alpha.openshift.io/expiry</code> annotation on the secret, which is in
RFC3339 format.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>In most cases, the service DNS name
<code>&lt;service.name&gt;.&lt;service.namespace&gt;.svc</code> is not externally routable. The
primary use of <code>&lt;service.name&gt;.&lt;service.namespace&gt;.svc</code> is for intracluster or
intraservice communication, and with re-encrypt routes.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-secrets-troubleshooting-nodes-pods-secrets">Troubleshooting secrets</h4>
<div class="paragraph">
<p>If a service certificate generation fails with (service&#8217;s
<code>service.alpha.openshift.io/serving-cert-generation-error</code> annotation
contains):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">secret/ssl-key references serviceUID 62ad25ca-d703-11e6-9d6f-0e9c0057b608, which does not match 77b6dd80-d716-11e6-9d6f-0e9c0057b60</pre>
</div>
</div>
<div class="paragraph">
<p>The service that generated the certificate no longer exists, or has a different
<code>serviceUID</code>. You must force certificates regeneration by removing the old
secret, and clearing the following annotations on the service
<code>service.alpha.openshift.io/serving-cert-generation-error</code>,
<code>service.alpha.openshift.io/serving-cert-generation-error-num</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc delete secret &lt;secret_name&gt;
$ oc annotate service &lt;service_name&gt; service.alpha.openshift.io/serving-cert-generation-error-1
$ oc annotate service &lt;service_name&gt; service.alpha.openshift.io/serving-cert-generation-error-num-1</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The command removing annotation has a <code><strong>-</strong></code> after the annotation name to be
removed.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="using-device-manager-to-make-devices-available-to-nodes">Using device plug-ins to access external resouces with pods in OpenShift Enterprise</h3>
<div class="paragraph">
<p>Device plug-ins allow you to use a particular device type (GPU, InfiniBand,
or other similar computing resources that require vendor-specific initialization
and setup) in your OpenShift Enterprise pod without needing to write custom code.</p>
</div>
<div class="sect3">
<h4 id="nodes-pods-plugins-about-nodes-pods-device">Understanding device plug-ins</h4>
<div class="paragraph">
<p>The device plug-in provides a consistent and portable solution to consume hardware
devices across clusters. The device plug-in provides support for these devices
through an extension mechanism, which makes these devices available to
Containers, provides health checks of these devices, and securely shares them.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>OpenShift Enterprise supports the device plug-in API, but the device plug-in
Containers are supported by individual vendors.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>A device plug-in is a gRPC service running on the nodes (external to
<code>atomic-openshift-node.service</code>) that is responsible for managing specific
hardware resources. Any device plug-in must support following remote procedure
calls (RPCs):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-golang" data-lang="golang">service DevicePlugin {
      // GetDevicePluginOptions returns options to be communicated with Device
      // Manager
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch returns a stream of List of Devices
      // Whenever a Device state change or a Device disappears, ListAndWatch
      // returns the new list
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate is called during container creation so that the Device
      // Plug-in can run device specific operations and instruct Kubelet
      // of the steps to make the Device available in the container
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // PreStartcontainer is called, if indicated by Device Plug-in during
      // registration phase, before each container start. Device plug-in
      // can run device specific operations such as reseting the device
      // before making devices available to the container
      rpc PreStartcontainer(PreStartcontainerRequest) returns (PreStartcontainerResponse) {}
}</code></pre>
</div>
</div>
<h6 id="_example-device-plug-ins" class="discrete">Example device plug-ins</h6>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/Container-engine-accelerators/tree/master/cmd/nvidia_gpu">Nvidia GPU device plug-in for COS-based operating system</a></p>
</li>
<li>
<p><a href="https://github.com/NVIDIA/k8s-device-plugin">Nvidia official GPU device plug-in</a></p>
</li>
<li>
<p><a href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare device plug-in</a></p>
</li>
<li>
<p><a href="https://github.com/kubevirt/kubernetes-device-plugins">KubeVirt device plug-ins: vfio and kvm</a></p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>For easy device plug-in reference implementation, there is a stub device plug-in
in the Device Manager code:
<strong><em>vendor/k8s.io/kubernetes/pkg/kubelet/cm/deviceplugin/device_plugin_stub.go</em></strong>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="_methods-for-deploying-a-device-plug-in">Methods for deploying a device plug-in</h5>
<div class="ulist">
<ul>
<li>
<p>Daemonsets are the recommended approach for device plug-in deployments.</p>
</li>
<li>
<p>Upon start, the device plug-in will try to create a UNIX domain socket at
<strong><em>/var/lib/kubelet/device-plugin/</em></strong> on the node to serve RPCs from Device Manager.</p>
</li>
<li>
<p>Since device plug-ins need to manage hardware resources, access to the host
file system, as well as socket creation, they must be run in a privileged
security context.</p>
</li>
<li>
<p>More specific details regarding deployment steps can be found with each device
plug-in implementation.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-plugins-device-mgr-nodes-pods-device">Understanding the Device Manager</h4>
<div class="paragraph">
<p>Device Manager provides a mechanism for advertising specialized node hardware resources
with the help of plug-ins known as device plug-ins.</p>
</div>
<div class="paragraph">
<p>You can advertise specialized hardware without requiring any upstream code changes.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>OpenShift Enterprise supports the device plug-in API, but the device plug-in
Containers are supported by individual vendors.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Device Manager advertises devices as <strong>Extended Resources</strong>. User pods can consume
devices, advertised by Device Manager, using the same <strong>Limit/Request</strong> mechanism,
which is used for requesting any other <strong>Extended Resource</strong>.</p>
</div>
<div class="paragraph">
<p>Upon start, the device plug-in registers itself with Device Manager invoking <code>Register</code> on the
<strong><em>/var/lib/kubelet/device-plugins/kubelet.sock</em></strong> and starts a gRPC service at
<strong><em>/var/lib/kubelet/device-plugins/&lt;plugin&gt;.sock</em></strong> for serving Device Manager
requests.</p>
</div>
<div class="paragraph">
<p>Device Manager, while processing a new registration request, invokes
<code>ListAndWatch</code> remote procedure call (RPC) at the device plug-in service. In
response, Device Manger gets a list of <strong>Device</strong> objects from the plug-in over a
gRPC stream. Device Manager will keep watching on the stream for new updates
from the plug-in. On the plug-in side, the plug-in will also keep the stream
open and whenever there is a change in the state of any of the devices, a new
device list is sent to the Device Manager over the same streaming connection.</p>
</div>
<div class="paragraph">
<p>While handling a new pod admission request, Kubelet passes requested <code>Extended
Resources</code> to the Device Manager for device allocation. Device Manager checks in
its database to verify if a corresponding plug-in exists or not. If the plug-in exists
and there are free allocatable devices as well as per local cache, <code>Allocate</code>
RPC is invoked at that particular device plug-in.</p>
</div>
<div class="paragraph">
<p>Additionally, device plug-ins can also perform several other device-specific
operations, such as driver installation, device initialization, and device
resets. These functionalities vary from implementation to implementation.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-plugins-install-nodes-pods-device">Enabling Device Manager</h4>
<div class="paragraph">
<p>Enable Device Manager to implement a device plug-in to advertise specialized
hardware without any upstream code changes.</p>
</div>
<div class="paragraph">
<p>Device Manager provides a mechanism for advertising specialized node hardware resources
with the help of plug-ins known as device plug-ins.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Obtain the label associated with the static Machine Config Pool CRD for the type of node you want to configure.
Perform one of the following steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>View the Machine Config:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap"># oc describe machineconfig &lt;name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml"># oc describe machineconfig 00-worker

oc describe machineconfig 00-worker
Name:         00-worker
Namespace:
Labels:       machineconfiguration.openshift.io/role=worker <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Label required for the device manager.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a Custom Resource (CR) for your configuration change.</p>
<div class="listingblock">
<div class="title">Sample configuration for a Device Manager CR</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: devicemgr <b class="conum">(1)</b>
spec:
  machineConfigPoolSelector:
    matchLabels:
       machine.openshift.io/cluster-api-machine-type: devicemgr <b class="conum">(2)</b>
  kubeletConfig:
    feature-gates:
      - DevicePlugins=true <b class="conum">(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Assign a name to CR.</p>
</li>
<li>
<p>Enter the label from the Machine Config Pool.</p>
</li>
<li>
<p>Set <code>DevicePlugins</code> to 'true`.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the device manager:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f devicemgr.yaml

kube
letconfig.machineconfiguration.openshift.io/devicemgr created</pre>
</div>
</div>
</li>
<li>
<p>Ensure that Device Manager was actually enabled by confirming that
<strong><em>/var/lib/kubelet/device-plugins/kubelet.sock</em></strong> is created on the node. This is
the UNIX domain socket on which the Device Manager gRPC server listens for new
plug-in registrations. This sock file is created when the Kubelet is started
only if Device Manager is enabled.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="including-pod-priority-in-pod-scheduling-decisions">Including pod priority in pod scheduling decisions in OpenShift Enterprise</h3>
<div class="paragraph">
<p>You can enable pod priority and preemption in your cluster. Pod priority indicates the importance of a pod relative to other pods and queues the pods based on that priority. Pod preemption allows the cluster to evict, or preempt, lower-priority pods so that higher-priority pods can be scheduled if there is no available space on a suitable node
Pod priority also affects the scheduling order of pods and out-of-resource eviction ordering on the node.</p>
</div>
<div class="paragraph">
<p>To use priority and preemption, you create priority classes that define the relative weight of your pods. Then, reference a priority class in the pod specification to apply that weight for scheduling.</p>
</div>
<div class="paragraph">
<p>Preemption is controlled by the <code>disablePreemption</code> parameter in the scheduler configuration file, which is set to <code>false</code> by default.</p>
</div>
<div class="sect3">
<h4 id="nodes-pods-priority-about-nodes-pods-priority">Understanding pod priority in OpenShift Enterprise</h4>
<div class="paragraph">
<p>When you use the Pod Priority and Preemption feature, the scheduler orders pending pods by their priority, and a pending pod is placed ahead of other pending pods with lower priority in the scheduling queue. As a result, the higher priority pod might be scheduled sooner than pods with lower priority if its scheduling requirements are met. If a pod cannot be scheduled, scheduler continues to schedule other lower priority pods.</p>
</div>
<div class="sect4">
<h5 id="admin-guide-priority-preemption-priority-class-nodes-pods-priority">Pod priority classes</h5>
<div class="paragraph">
<p>You can assign pods a priority class, which is a non-namespaced object that defines a mapping from a name to the integer value of the priority. The higher the value, the higher the priority.</p>
</div>
<div class="paragraph">
<p>A priority class object can take any 32-bit integer value smaller than or equal to 1000000000 (one billion). Reserve numbers larger than one billion for critical pods that should not be preempted or evicted. By default, OpenShift Enterprise has two reserved priority classes for critical system pods to have guaranteed scheduling.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get priorityclasses
NAME                      CREATED AT
cluster-logging           2019-03-13T14:45:12Z
system-cluster-critical   2019-03-13T14:01:10Z
system-node-critical      2019-03-13T14:01:10Z</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>system-node-critical</strong> - This priority class has a value of 2000001000 and is used for all pods that should never be evicted from a node. Examples of pods that have this priority class are <code>sdn-ovs</code>, <code>sdn</code>, and so forth.</p>
</li>
<li>
<p><strong>system-cluster-critical</strong> - This priority class has a value of 2000000000 (two billion) and is used with pods that are important for the cluster. Pods with this priority class can be evicted from a node in certain circumstances. For example, pods configured with the <code>system-node-critical</code> priority class can take priority. However, this priority class does ensure guaranteed scheduling. Examples of pods that can have this priority class are fluentd, add-on components like descheduler, and so forth.</p>
<div class="paragraph">
<p>A number of critical components include the <code>system-cluster-critical</code> priority class by default, for example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>master-api</p>
</li>
<li>
<p>master-controller</p>
</li>
<li>
<p>master-etcd</p>
</li>
<li>
<p>sdn</p>
</li>
<li>
<p>sdn-ovs</p>
</li>
<li>
<p>sync</p>
</li>
<li>
<p>fluentd</p>
</li>
<li>
<p>metrics-server</p>
</li>
<li>
<p>descheduler</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>cluster-logging</strong> - This priority is used by Fluentd to make sure Fluentd pods are scheduled to nodes over other apps.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>If you upgrade your existing cluster, the priority of your existing pods is effectively zero. However, existing pods with
the <code>scheduler.alpha.kubernetes.io/critical-pod</code> annotation are automatically converted to <code>system-cluster-critical</code> class.
Fluentd cluster logging pods with the annotation are converted to the <code>cluster-logging</code> priority class.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect4">
<h5 id="admin-guide-priority-preemption-names-nodes-pods-priority">Pod priority names</h5>
<div class="paragraph">
<p>After you have one or more priority classes, you can create pods that specify a priority class name in a pod specification. The priority admission controller uses the priority class name field to populate the integer value of the priority. If the named priority class is not found, the pod is rejected.</p>
</div>
<div class="paragraph">
<p>The following YAML is an example of a pod configuration that uses the priority class created in the preceding example. The priority admission controller checks the specification and resolves the priority of the pod to 1000000.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-priority-preempt-about-nodes-pods-priority">Understanding pod preemption in OpenShift Enterprise</h4>
<div class="paragraph">
<p>When a developer creates a pod, the pod goes into a queue. If the developer configured the pod for pod priority or preemption, the scheduler picks a pod from the queue and tries to schedule the pod on a node. If the scheduler cannot find space on an appropriate node that satisfies all the specified requirements of the pod, preemption logic is triggered for the pending pod.</p>
</div>
<div class="paragraph">
<p>When the scheduler preempts one or more pods on a node, the <code>nominatedNodeName</code> field of higher-priority pod specification is set to the name of the node, along with the <code>nodename</code> field. The scheduler uses the <code>nominatedNodeName</code> field to keep track of the resources reserved for pods and also provides information to the user about preemptions in the clusters.</p>
</div>
<div class="paragraph">
<p>After the scheduler preempts a lower-priority pod, the scheduler honors the graceful termination period of the pod. If another node becomes available while scheduler is waiting for the lower-priority pod to terminate, the scheduler can schedule the higher-priority pod on that node. As a result, the <code>nominatedNodeName</code> field and <code>nodeName</code> field of the pod specification might be different.</p>
</div>
<div class="paragraph">
<p>Also, if the scheduler preempts pods on a node and is waiting for termination, and a pod with a higher-priority pod than the pending pod needs to be scheduled, the scheduler can schedule the higher-priority pod instead. In such a case, the scheduler clears the <code>nominatedNodeName</code> of the pending pod, making the pod eligible for another node.</p>
</div>
<div class="paragraph">
<p>Preemption does not necessarily remove all lower-priority pods from a node. The scheduler can schedule a pending pod by removing a portion of the lower-priority pods.</p>
</div>
<div class="paragraph">
<p>The scheduler considers a node for pod preemption only if the pending pod can be scheduled on the node.</p>
</div>
<div class="sect4">
<h5 id="priority-preemption-other-nodes-pods-priority">Pod preemption and other scheduler settings</h5>
<div class="paragraph">
<p>If you enable pod priority and preemption, consider your other scheduler settings:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Pod priority and pod disruption budget</dt>
<dd>
<p>A pod disruption budget specifies the minimum number or percentage of replicas that must be up at a time. If you specify pod disruption budgets, OpenShift Enterprise respects them when preempting pods at a best effort level. The scheduler attempts to preempt pods without violating the pod disruption budget. If no such pods are found, lower-priority pods might be preempted despite their pod disruption budget requirements.</p>
</dd>
<dt class="hdlist1">Pod priority and pod affinity</dt>
<dd>
<p>Pod affinity requires a new pod to be scheduled on the same node as other pods with the same label.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>If a pending pod has inter-pod affinity with one or more of the lower-priority pods on a node, the scheduler cannot preempt the lower-priority pods without violating the affinity requirements.  In this case, the scheduler looks for another node to schedule the pending pod. However, there is no guarantee that the scheduler can find an appropriate node and pending pod might not be scheduled.</p>
</div>
<div class="paragraph">
<p>To prevent this situation, carefully configure pod affinity with equal-priority pods.</p>
</div>
</div>
<div class="sect4">
<h5 id="priority-preemption-graceful-nodes-pods-priority">Graceful termination of preempted pods</h5>
<div class="paragraph">
<p>When preempting a pod, the scheduler waits for the pod graceful termination period to expire, allowing the pod to finish working and exit. If the pod does not exit after the period, the scheduler kills the pod. This graceful termination period creates a time gap between the point that the scheduler preempts the pod and the time when the pending pod can be scheduled on the node.</p>
</div>
<div class="paragraph">
<p>To minimize this gap, configure a small graceful termination period for lower-priority pods.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-priority-configuring-nodes-pods-priority">Configuring priority and preemption</h4>
<div class="paragraph">
<p>You apply pod priority and preemption by creating a priority class object and associating pods to the priority using the
<code>priorityClassName</code> in your pod specifications.</p>
</div>
<div class="listingblock">
<div class="title">Sample priority class object</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: high-priority <b class="conum">(1)</b>
value: 1000000 <b class="conum">(2)</b>
globalDefault: false <b class="conum">(3)</b>
description: "This priority class should be used for XYZ service pods only." <b class="conum">(4)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The name of the priority class object.</p>
</li>
<li>
<p>The priority value of the object.</p>
</li>
<li>
<p>Optional field that indicates whether this priority class should be used for pods without a priority class name specified. This field is <code>false</code> by default. Only one priority class with <code>globalDefault</code> set to <code>true</code> can exist in the cluster. If there is no priority class with <code>globalDefault:true</code>, the priority of pods with no priority class name is zero. Adding a priority class with <code>globalDefault:true</code> affects only pods created after the priority class is added and does not change the priorities of existing pods.</p>
</li>
<li>
<p>Optional arbitrary text string that describes which pods developers should use with this priority class.</p>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To configure your cluster to use priority and preemption:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create one or more priority classes:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Specify a name and value for the priority.</p>
</li>
<li>
<p>Optionally specify the <code>globalDefault</code> field in the priority class and a description.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create a pod specification or edit existing pods to include the name of a priority class, similar to the following:</p>
<div class="listingblock">
<div class="title">Sample pod specification with priority class name</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify the priority class to use with this pod.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>You can add the priority name directly to the pod configuration or to a pod template.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-pods-priority-disabling-nodes-pods-priority">Disabling priority and preemption</h4>
<div class="paragraph">
<p>You can disable the pod priority and preemption feature.</p>
</div>
<div class="paragraph">
<p>After the feature is disabled, the existing pods keep their priority fields, but preemption is disabled, and priority fields are ignored. If the feature is disabled, you cannot set a priority class name in new pods.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>Critical pods rely on scheduler preemption to be scheduled when a cluster is under resource pressure. For this reason, Red Hat recommends not disabling preemption.
DaemonSet pods are scheduled by the DaemonSet controller and not affected by disabling preemption.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To disable the preemption for the cluster:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Edit the Scheduler Operator Custom Resource to add the <code>disablePreemption: true</code> parameter:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  creationTimestamp: '2019-03-12T01:45:02Z'
  generation: 1
  name: example
  resourceVersion: '1882034'
  selfLink: /apis/config.openshift.io/v1/schedulers/example
  uid: 743701e9-4468-11e9-bd34-02a7fe1bf828
spec:
  disablePreemption: true</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_controlling-pod-placement-onto-nodes-scheduling">Controlling pod placement onto nodes (scheduling)</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="about-pod-placement-using-the-scheduler">Controlling pod placement using the OpenShift Enterprise scheduler</h3>
<div class="paragraph">
<p>Pod scheduling is an internal process that determines placement of new
pods onto nodes within the cluster.</p>
</div>
<div class="paragraph">
<p>The scheduler code has a clean separation that watches new pods
as they get created and identifies the most suitable node to host them. It then
creates bindings (pod to node bindings) for the pods using the master API.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Default pod scheduling</dt>
<dd>
<p>OpenShift Container Platform comes with a default scheduler that serves the needs of most users. The default scheduler uses both inherent and customization tools to determine the best fit for a pod.</p>
</dd>
<dt class="hdlist1">Advanced pod scheduling</dt>
<dd>
<p>In situations where you might want more control over where new pods are placed, the OpenShift Container Platform advanced scheduling features allow you to configure a pod so that the pod is required or has a preference to run on a particular node or alongside a specific pod by.</p>
<div class="ulist">
<ul>
<li>
<p>Using pod affinity and anti-affinity rules</p>
</li>
<li>
<p>Placing a pod on a node by name</p>
</li>
<li>
<p>Placing a pod in a specific project</p>
</li>
<li>
<p>Placing pods on overcomitted nodes</p>
</li>
<li>
<p>Controlling pod placement with node affinity</p>
</li>
<li>
<p>Controlling pod placement with taints and tolerations</p>
</li>
<li>
<p>Controlling pod placement with node selectors</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Removing pods from nodes based priority</dt>
<dd>
<p>You can configure the pod scheduler to evict, or preempt, lower-priority pods in order to schedule higher-priority pods.</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="placing-pods-onto-nodes-with-the-default-scheduler">Configuring the default scheduler to control pod placement in OpenShift Enterprise</h3>
<div class="paragraph">
<p>The default OpenShift Enterprise pod scheduler is responsible for determining placement of new
pods onto nodes within the cluster. It reads data from the pod and tries to find
a node that is a good fit based on configured policies. It is completely
independent and exists as a standalone/pluggable solution. It does not modify
the pod and just creates a binding for the pod that ties the pod to the
particular node.</p>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-default-about-nodes-scheduler-default">Understanding default scheduling in OpenShift Enterprise</h4>
<div class="paragraph">
<p>The existing generic scheduler is the default platform-provided scheduler
<em>engine</em> that selects a node to host the pod in a three-step operation:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Filters the Nodes</dt>
<dd>
<p>The available nodes are filtered based on the constraints or requirements
specified. This is done by running each node through the list of filter
functions called <em>predicates</em>.</p>
</dd>
<dt class="hdlist1">Prioritize the Filtered List of Nodes</dt>
<dd>
<p>This is achieved by passing each node through a series of priority_ functions
that assign it a score between 0 - 10, with 0 indicating a bad fit and 10
indicating a good fit to host the pod. The scheduler configuration can also take
in a simple <em>weight</em> (positive numeric value) for each priority function. The
node score provided by each priority function is multiplied by the weight
(default weight for most priorities is 1) and then combined by adding the scores for each node
provided by all the priorities. This weight attribute can be used by
administrators to give higher importance to some priorities.</p>
</dd>
<dt class="hdlist1">Select the Best Fit Node</dt>
<dd>
<p>The nodes are sorted based on their scores and the node with the highest score
is selected to host the pod. If multiple nodes have the same high score, then
one of them is selected at random.</p>
</dd>
</dl>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-default-about-understanding-nodes-scheduler-default">Understanding Scheduler Policy</h5>
<div class="paragraph">
<p>The selection of the predicate and priorities defines the policy for the scheduler.</p>
</div>
<div class="paragraph">
<p>The scheduler configuration file is a JSON file that specifies the predicates and priorities the scheduler
will consider.</p>
</div>
<div class="paragraph">
<p>In the absence of the scheduler policy file, the default scheduler behavior is used.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>The predicates and priorities defined in
the scheduler configuration file completely override the default scheduler
policy. If any of the default predicates and priorities are required,
you must explicitly specify the functions in the policy configuration.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="title">Sample scheduler configuration file</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">{
"kind" : "Policy",
"apiVersion" : "v1",
"predicates" : [
	{"name" : "PodFitsHostPorts"},
	{"name" : "PodFitsResources"},
	{"name" : "NoDiskConflict"},
	{"name" : "NoVolumeZoneConflict"},
	{"name" : "MatchNodeSelector"},
	{"name" : "HostName"}
	],
"priorities" : [
	{"name" : "LeastRequestedPriority", "weight" : 1},
	{"name" : "BalancedResourceAllocation", "weight" : 1},
	{"name" : "ServiceSpreadingPriority", "weight" : 1},
	{"name" : "EqualPriority", "weight" : 1}
	]
}</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-default-about-use-cases-nodes-scheduler-default">Scheduler Use Cases</h5>
<div class="paragraph">
<p>One of the important use cases for scheduling within OpenShift Enterprise is to
support flexible affinity and anti-affinity policies.</p>
</div>
<div class="sect5">
<h6 id="infrastructure-topological-levels-nodes-scheduler-default">Infrastructure Topological Levels</h6>
<div class="paragraph">
<p>Administrators can define multiple topological levels for their infrastructure
(nodes) by specifying labels on nodes. For example: <code>region=r1</code>, <code>zone=z1</code>, <code>rack=s1</code>.</p>
</div>
<div class="paragraph">
<p>These label names have no particular meaning and
administrators are free to name their infrastructure levels anything, such as
city/building/room. Also, administrators can define any number of levels
for their infrastructure topology, with three levels usually being adequate
(such as: <code>regions</code> &#8594; <code>zones</code> &#8594; <code>racks</code>).  Administrators can specify affinity
and anti-affinity rules at each of these levels in any combination.</p>
</div>
</div>
<div class="sect5">
<h6 id="affinity-nodes-scheduler-default">Affinity</h6>
<div class="paragraph">
<p>Administrators should be able to configure the scheduler to specify affinity at
any topological level, or even at multiple levels. Affinity at a particular
level indicates that all pods that belong to the same service are scheduled
onto nodes that belong to the same level. This handles any latency requirements
of applications by allowing administrators to ensure that peer pods do not end
up being too geographically separated. If no node is available within the same
affinity group to host the pod, then the pod is not scheduled.</p>
</div>
<div class="paragraph">
<p>If you need greater control over where the pods are scheduled, see Using Node Affinity and
Using Pod Affinity and Anti-affinity.</p>
</div>
<div class="paragraph">
<p>These advanced scheduling features allow administrators
to specify which node a pod can be scheduled on and to force or reject scheduling relative to other pods.</p>
</div>
</div>
<div class="sect5">
<h6 id="anti-affinity-nodes-scheduler-default">Anti-Affinity</h6>
<div class="paragraph">
<p>Administrators should be able to configure the scheduler to specify
anti-affinity at any topological level, or even at multiple levels.
Anti-affinity (or 'spread') at a particular level indicates that all pods that
belong to the same service are spread across nodes that belong to that
level. This ensures that the application is well spread for high availability
purposes. The scheduler tries to balance the service pods across all
applicable nodes as evenly as possible.</p>
</div>
<div class="paragraph">
<p>If you need greater control over where the pods are scheduled, see Using Node Affinity and
Using Pod Affinity and Anti-affinity.</p>
</div>
<div class="paragraph">
<p>These advanced scheduling features allow administrators
to specify which node a pod can be scheduled on and to force or reject scheduling relative to other pods.</p>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-default-creating_nodes-scheduler-default">Creating a scheduler policy file</h4>
<div class="paragraph">
<p>You can control change the default scheduling behavior using a ConfigMap in the <code>openshift-config</code> project.
Add and remove predicates and priorities to the ConfigMap to create a <em>scheduler policy</em>.</p>
</div>
<div class="listingblock">
<div class="title">Sample scheduler configuration map</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: scheduler-policy
  namespace: openshift-config
  selfLink: /api/v1/namespaces/openshift-config/configmaps/mypolicy
  uid: 83917dfb-4422-11e9-b2c9-0a5e37b2b12e
  resourceVersion: '1049773'
  creationTimestamp: '2019-03-11T17:24:23Z'
data:
  policy.cfg: "{\n\"kind\" : \"Policy\",\n\"apiVersion\" : \"v1\",\n\"predicates\" : [\n\t{\"name\" : \"PodFitsHostPorts\"},\n\t{\"name\" : \"PodFitsResources\"},\n\t{\"name\" : \"NoDiskConflict\"},\n\t{\"name\" : \"NoVolumeZoneConflict\"},\n\t{\"name\" : \"MatchNodeSelector\"},\n\t{\"name\" : \"HostName\"}\n\t],\n\"priorities\" : [\n\t{\"name\" : \"LeastRequestedPriority\", \"weight\" : 10},\n\t{\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1},\n\t{\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1},\n\t{\"name\" : \"EqualPriority\", \"weight\" : 1}\n\t]\n}\n"</code></pre>
</div>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To create the scheduler policy:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the a JSON file with the desired predicates and priorities.</p>
<div class="listingblock">
<div class="title">Sample scheduler JSON file</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">{
"kind" : "Policy",
"apiVersion" : "v1",
"predicates" : [      <b class="conum">(1)</b>
	{"name" : "PodFitsHostPorts"},
	{"name" : "PodFitsResources"},
	{"name" : "NoDiskConflict"},
	{"name" : "NoVolumeZoneConflict"},
	{"name" : "MatchNodeSelector"},
	{"name" : "HostName"}
	],
"priorities" : [     <b class="conum">(2)</b>
	{"name" : "LeastRequestedPriority", "weight" : 1},
	{"name" : "BalancedResourceAllocation", "weight" : 1},
	{"name" : "ServiceSpreadingPriority", "weight" : 1},
	{"name" : "EqualPriority", "weight" : 1}
	]
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Add the predicates as needed.</p>
</li>
<li>
<p>Add the priorities as needed.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create a ConfigMap based on the JSON file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create configmap -n openshift-config --from-file=policy.cfg &lt;configmap-name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create configmap -n openshift-config --from-file=policy.cfg scheduler-policy

configmap/scheduler-policy created</pre>
</div>
</div>
</li>
<li>
<p>Edit the <strong><em>scheduler.yaml</em></strong> file, installed by default, to add the ConfigMap.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
spec: {}
  policy:
    name: scheduler-policy</pre>
</div>
</div>
</li>
<li>
<p>Create the <code>scheduler</code> object:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f scheduler.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-default-modifying-nodes-scheduler-default">Modifying scheduler policies</h4>
<div class="paragraph">
<p>You change scheduling behavior by creating or editing your scheduler policy ConfigMap in the <code>openshift-config</code> project.
Add and remove predicates and priorities to the ConfigMap to create a <em>scheduler policy</em>.</p>
</div>
<div class="listingblock">
<div class="title">Typical predicate string</div>
<div class="content">
<pre class="nowrap">\n\t{\"name\" : \"&lt;PredicateName&gt;\", \"label\" : \"&lt;label&gt;\",  \"&lt;condition&gt;\" : \"&lt;state&gt;\"},</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>name</code> is the name of the predicate, such as <code>labelsPresence</code>.</p>
</li>
<li>
<p><code>label</code> and <code>&lt;label&gt;</code> is the node label:value pair to match to apply the predicate, such <code>label:rack</code>.</p>
</li>
<li>
<p><code>&lt;condition&gt;</code> and <code>&lt;state&gt;</code> is when the predicate should be applied, such as <code>presence:true</code>.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Typical priority string</div>
<div class="content">
<pre class="nowrap">\n\t{\"name\" : \"&lt;PredicateName&gt;\", \"label\" : \"&lt;label&gt;\",  \"&lt;condition&gt;\" : \"&lt;state&gt;\", \"weight\" : &lt;weight&gt;},</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>name</code> is the name of the priority, such as <code>labelsPresence</code>.</p>
</li>
<li>
<p><code>label</code> and <code>&lt;label&gt;</code> is the node <code>label:value</code> pair to match to apply the priority, such <code>label:rack</code>.</p>
</li>
<li>
<p><code>&lt;condition&gt;</code> and <code>&lt;state&gt;</code> is when the priority should be applied, such as <code>presence:true</code>.</p>
</li>
<li>
<p><code>weight</code> and `&lt;weight&gt; is the numerical weight to apply to the priority.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To modify the scheduler policy:</p>
</div>
<div class="paragraph">
<p>Edit the scheduler configuration file to configure the desired
predicates and priorities.</p>
</div>
<div class="listingblock">
<div class="title">Sample modified scheduler configuration map</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: scheduler-policy
  namespace: openshift-config
  selfLink: /api/v1/namespaces/openshift-config/configmaps/mypolicy
  uid: 83917dfb-4422-11e9-b2c9-0a5e37b2b12e
  resourceVersion: '1049773'
  creationTimestamp: '2019-03-11T17:24:23Z'
data:
  policy.cfg: "{\n\"kind\" : \"Policy\",\n\"apiVersion\" : \"v1\",\n\"predicates\" : [\n\t{\"name\" : \"PodFitsHostPorts\"},\n\t{\"name\" : \"PodFitsResources\"},\n\t{\"name\" : \"NoDiskConflict\"},\n\t{\"name\" : \"NoVolumeZoneConflict\"},\n\t{\"name\" : \"MatchNodeSelector\"},\n\t{\"name\" : \"HostName\"}\n\t],\n\"priorities\" : [\n\t{\"name\" : \"LeastRequestedPriority\", \"weight\" : 10},\n\t{\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1},\n\t{\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1},\n\t{\"name\" : \"EqualPriority\", \"weight\" : 1}\n\t]\n}\n"</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, the following strings add the <code>labelpresence</code> predicate requiring the <code>rack</code> label on the nodes and the <code>labelPreference</code> priority giving a weight of 2 to the <code>rack</code> label:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">\n\t{\"name\" : \"labelPresence\", \"label\" : \"rack\",  \"presence\" : \"true\"},
\n\t{\"name\" : \"labelPreference\", \"label\" : \"rack\", \"presence\" : \"true\", \"weight\" : 2},\n\t</code></pre>
</div>
</div>
<div class="paragraph">
<p>The ConfigMap appears as following with the new priority:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">policy.cfg: "{\n\"kind\" : \"Policy\",\n\"apiVersion\" : \"v1\",\n\"predicates\" : [\n\t{\"name\" : \"PodFitsHostPorts\"},\n\t{\"name\" : \"PodFitsResources\"},\n\t{\"name\" : \"NoDiskConflict\"},\n\t{\"name\" : \"NoVolumeZoneConflict\"},\n\t{\"name\" : \"MatchNodeSelector\"},\n\t{\"name\" : \"HostName\"},\n\t{\"name\" : \"labelPresence\", \"label\" : \"rack\",  \"presence\" : \"true\"}\n\t],\n\"priorities\" : [\n\t{\"name\" : \"LeastRequestedPriority\", \"weight\" : 10},\n\t{\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1},\n\t{\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1},\n\t{\"name\" : \"EqualPriority\", \"weight\" : 1},\n\t{\"name\" : \"labelPreference\", \"label\" : \"rack\", \"presence\" : \"true\", \"weight\" : 2},\n\t]\n}\n "</code></pre>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-default-predicates-nodes-scheduler-default">Understanding the scheduler predicates</h5>
<div class="paragraph">
<p>Predicates are rules that filter out unqualified nodes.</p>
</div>
<div class="paragraph">
<p>There are several predicates provided by default in OpenShift Enterprise. Some of
these predicates can be customized by providing certain parameters. Multiple
predicates can be combined to provide additional filtering of nodes.</p>
</div>
<div class="sect5">
<h6 id="static-predicates-nodes-scheduler-default">Static Predicates</h6>
<div class="paragraph">
<p>These predicates do not take any configuration parameters or inputs from the
user. These are specified in the scheduler configuration using their exact
name.</p>
</div>
<div class="sect6">
<h7 id="default-predicates-nodes-scheduler-default">Default Predicates</h7>
<div class="paragraph">
<p>The default scheduler policy includes the following predicates:</p>
</div>
<div class="paragraph">
<p><strong><em>NoVolumeZoneConflict</em></strong> checks that the volumes a pod requests
are available in the zone.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "NoVolumeZoneConflict"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>MaxEBSVolumeCount</em></strong> checks the maximum number of volumes that can be attached to an AWS instance.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "MaxEBSVolumeCount"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>MaxGCEPDVolumeCount</em></strong> checks the maximum number of Google Compute Engine (GCE) Persistent Disks (PD).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "MaxGCEPDVolumeCount"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>MatchInterPodAffinity</em></strong> checks if the pod affinity/anti-affinity rules permit the pod.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "MatchInterPodAffinity"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>NoDiskConflict</em></strong> checks if the volume requested by a pod is available.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "NoDiskConflict"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>PodToleratesNodeTaints</em></strong> checks if a pod can tolerate the node taints.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "PodToleratesNodeTaints"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>CheckNodeMemoryPressure</em></strong> checks if a pod can be scheduled on a node with a memory pressure condition.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "CheckNodeMemoryPressure"}</pre>
</div>
</div>
</div>
<div class="sect6">
<h7 id="other-predicates-nodes-scheduler-default">Other Static Predicates</h7>
<div class="paragraph">
<p>OpenShift Enterprise also supports the following predicates:</p>
</div>
<div class="paragraph">
<p><strong><em>CheckNodeDiskPressure</em></strong> checks if a pod can be scheduled on a node with a disk pressure condition.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "CheckNodeDiskPressure"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>CheckVolumeBinding</em></strong> evaluates if a pod can fit based on the volumes, it requests, for both bound and unbound PVCs.
* For PVCs that are bound, the predicate checks that the corresponding PV&#8217;s node affinity is satisfied by the given node.
* For PVCs that are unbound, the predicate searched for available PVs that can satisfy the PVC requirements and that
the PV node affinity is satisfied by the given node.</p>
</div>
<div class="paragraph">
<p>The predicate returns true if all bound PVCs have compatible PVs with the node, and if all unbound PVCs can be matched with an available and node-compatible PV.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "CheckVolumeBinding"}</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>CheckVolumeBinding</code> predicate must be enabled in non-default schedulers.</p>
</div>
<div class="paragraph">
<p><strong><em>CheckNodeCondition</em></strong> checks if a pod can be scheduled on a node reporting <strong>out of disk</strong>, <strong>network unavailable</strong>, or <strong>not ready</strong> conditions.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "CheckNodeCondition"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>PodToleratesNodeNoExecuteTaints</em></strong> checks if a pod tolerations can tolerate a node <strong>NoExecute</strong> taints.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "PodToleratesNodeNoExecuteTaints"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>CheckNodeLabelPresence</em></strong> checks if all of the specified labels exist on a node, regardless of their value.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "CheckNodeLabelPresence"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>checkServiceAffinity</em></strong> checks that ServiceAffinity labels are homogeneous for pods that are scheduled on a node.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "checkServiceAffinity"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>MaxAzureDiskVolumeCount</em></strong>  checks the maximum number of Azure Disk Volumes.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "MaxAzureDiskVolumeCount"}</pre>
</div>
</div>
</div>
</div>
<div class="sect5">
<h6 id="admin-guide-scheduler-general-predicates-nodes-scheduler-default">General Predicates</h6>
<div class="paragraph">
<p>The following general predicates check whether non-critical predicates and essential predicates pass. Non-critical predicates are the predicates
that only non-critical pods need to pass and essential predicates are the predicates that all pods need to pass.</p>
</div>
<div class="paragraph">
<p><em>The default scheduler policy includes the general predicates.</em></p>
</div>
<h7 id="_non-critical-general-predicates" class="discrete">Non-critical general predicates</h7>
<div class="paragraph">
<p><strong><em>PodFitsResources</em></strong> determines a fit based on resource availability
(CPU, memory, GPU, and so forth). The
nodes can declare their resource capacities and then pods can specify what
resources they require. Fit is based on requested, rather than used
resources.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "PodFitsResources"}</pre>
</div>
</div>
<h8 id="_essential-general-predicates" class="discrete">Essential general predicates</h8>
<div class="paragraph">
<p><strong><em>PodFitsHostPorts</em></strong> determines if a node has free ports for the requested pod ports (absence
of port conflicts).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "PodFitsHostPorts"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>HostName</em></strong> determines fit based on the presence of the Host parameter
and a string match with the name of the host.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "HostName"}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>MatchNodeSelector</em></strong> determines fit based on node selector (nodeSelector) queries
defined in the pod.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "MatchNodeSelector"}</pre>
</div>
</div>
</div>
<div class="sect5">
<h6 id="configurable-predicates-nodes-scheduler-default">Configurable Predicates</h6>
<div class="paragraph">
<p>You can configure these predicates in the scheduler policy Configmap,
<code>policy-configmap</code> in the <code>openshift-config</code> project, to add labels to affect
how the predicate functions.</p>
</div>
<div class="paragraph">
<p>Since these are configurable, multiple predicates
of the same type (but different configuration parameters) can be combined as
long as their user-defined names are different.</p>
</div>
<div class="paragraph">
<p>For information on using these priorities, see Modifying Scheduler Policy.</p>
</div>
<div class="paragraph">
<p><strong><em>ServiceAffinity</em></strong> places pods on nodes based on the service running on that pod.
Placing pods of the same service on the same or co-located nodes can lead to higher efficiency.</p>
</div>
<div class="paragraph">
<p>This predicate attempts to place pods with specific labels
in its node selector on nodes that have the same label.</p>
</div>
<div class="paragraph">
<p>If the pod does not specify the labels in its
node selector, then the first pod is placed on any node based on availability
and all subsequent pods of the service are scheduled on nodes that have the
same label values as that node.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">"predicates":[
      {
         "name":"&lt;name&gt;", <b class="conum">(1)</b>
         "argument":{
            "serviceAffinity":{
               "labels":[
                  "&lt;label&gt;" <b class="conum">(2)</b>
               ]
            }
         }
      }
   ],</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify a name for the predicate.</p>
</li>
<li>
<p>Specify a label to match.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">        "name":"ZoneAffinity",
        "argument":{
            "serviceAffinity":{
                "labels":[
                    "rack"
                ]
            }
        }</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example. if the first pod of a service had a node selector <code>rack</code> was scheduled to a node with label <code>region=rack</code>,
all the other subsequent pods belonging to the same service will be scheduled on nodes
with the same <code>region=rack</code> label.</p>
</div>
<div class="paragraph">
<p>Multiple-level labels are also supported. Users can also specify all pods for a service to
be scheduled on nodes within the same region and within the same zone (under the region).</p>
</div>
<div class="paragraph">
<p>The <code>labelsPresence</code> parameter checks whether a particular node has a specific label. The labels create node <em>groups</em> that the
<code>LabelPreference</code> priority uses. Matching by label can be useful, for example, where nodes have their physical location or status defined by labels.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">"predicates":[
      {
         "name":"&lt;name&gt;", <b class="conum">(1)</b>
         "argument":{
            "labelsPresence":{
               "labels":[
                  "&lt;label&gt;" <b class="conum">(2)</b>
                ],
                "presence": true <b class="conum">(3)</b>
            }
         }
      }
   ],</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify a name for the predicate.</p>
</li>
<li>
<p>Specify a label to match.</p>
</li>
<li>
<p>Specify whether the labels are required, either <code>true</code> or <code>false</code>.</p>
<div class="ulist">
<ul>
<li>
<p>For <code>presence:false</code>, if any of the requested labels are present in the node labels,
the pod cannot be scheduled. If the labels are not present, the pod can be scheduled.</p>
</li>
<li>
<p>For <code>presence:true</code>, if all of the requested labels are present in the node labels,
the pod can be scheduled. If all of the labels are not present, the pod is not scheduled.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">        "name":"RackPreferred",
        "argument":{
            "labelsPresence":{
                "labels":[
                    "rack",
                    "region"
                ],
                "presence": true
            }
        }</code></pre>
</div>
</div>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-default-priorities-nodes-scheduler-default">Understanding the scheduler priorities</h5>
<div class="paragraph">
<p>Priorities are rules that rank nodes according to preferences.</p>
</div>
<div class="paragraph">
<p>A custom set of priorities can be specified to configure the scheduler.
There are several priorities provided by default in OpenShift Enterprise.
Other priorities can be customized by providing certain
parameters. Multiple priorities can be combined and different weights
can be given to each in order to impact the prioritization.</p>
</div>
<div class="sect5">
<h6 id="static-priority-functions-nodes-scheduler-default">Static Priorities</h6>
<div class="paragraph">
<p>Static priorities do not take any configuration parameters from
the user, except weight. A weight is required to be specified and cannot be 0 or negative.</p>
</div>
<div class="paragraph">
<p>These are specified in the scheduler policy Configmap,
<code>policy-configmap</code> in the <code>openshift-config</code> project.</p>
</div>
<div class="sect6">
<h7 id="default-priorities-nodes-scheduler-default">Default Priorities</h7>
<div class="paragraph">
<p>The default scheduler policy includes the following priorities. Each of
the priority function has a weight of <code>1</code> except <code><strong>NodePreferAvoidPodsPriority</strong></code>,
which has a weight of <code>10000</code>.</p>
</div>
<div class="paragraph">
<p><strong><em>SelectorSpreadPriority</em></strong> looks for services, replication controllers (RC),
replication sets (RS), and stateful sets that match the pod,
then finds existing pods that match those selectors.
The scheduler favors nodes that have fewer existing matching pods. Then, it schedules the pod on a node with the smallest number of
pods that match those selectors as the pod being scheduled.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "SelectorSpreadPriority", "weight" : 1}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>InterPodAffinityPriority</em></strong> computes a sum by iterating through the elements of <code>weightedPodAffinityTerm</code> and adding
<em>weight</em> to the sum if the corresponding PodAffinityTerm is satisfied for that node. The node(s) with the highest sum are the most preferred.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "InterPodAffinityPriority", "weight" : 1}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>LeastRequestedPriority</em></strong> favors nodes with fewer requested resources. It
calculates the percentage of memory and CPU requested by pods scheduled on the
node, and prioritizes nodes that have the highest available/remaining capacity.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "LeastRequestedPriority", "weight" : 1}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>BalancedResourceAllocation</em></strong> favors nodes with balanced resource usage rate.
It calculates the difference between the consumed CPU and memory as a fraction
of capacity, and prioritizes the nodes based on how close the two metrics are to
each other. This should always be used together with <code>LeastRequestedPriority</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "BalancedResourceAllocation", "weight" : 1}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>NodePreferAvoidPodsPriority</em></strong> ignores pods that are owned by a controller other than a replication controller.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "NodePreferAvoidPodsPriority", "weight" : 10000}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>NodeAffinityPriority</em></strong> prioritizes nodes according to node affinity scheduling preferences</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "NodeAffinityPriority", "weight" : 1}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>TaintTolerationPriority</em></strong> prioritizes nodes that have a fewer number of <em>intolerable</em> taints on them for a pod. An intolerable taint is one which has key <code>PreferNoSchedule</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "TaintTolerationPriority", "weight" : 1}</pre>
</div>
</div>
</div>
<div class="sect6">
<h7 id="other-priorities-nodes-scheduler-default">Other Static Priorities</h7>
<div class="paragraph">
<p>OpenShift Enterprise also supports the following priorities:</p>
</div>
<div class="paragraph">
<p><strong><em>EqualPriority</em></strong> gives an equal weight of <code>1</code> to all nodes, if no priority
configurations are provided. We recommend using this priority only for testing environments.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "EqualPriority", "weight" : 1}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>MostRequestedPriority</em></strong> prioritizes nodes with most requested resources. It calculates the percentage of memory and CPU
requested by pods scheduled on the node, and prioritizes based on the maximum of the average of the fraction of requested to capacity.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "MostRequestedPriority", "weight" : 1}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>ImageLocalityPriority</em></strong> prioritizes nodes that already have requested pod container&#8217;s images.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "ImageLocalityPriority", "weight" : 1}</pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>ServiceSpreadingPriority</em></strong> spreads pods by minimizing the number of pods
belonging to the same service onto the same machine.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">{"name" : "ServiceSpreadingPriority", "weight" : 1}</pre>
</div>
</div>
</div>
</div>
<div class="sect5">
<h6 id="configurable-priority-functions-nodes-scheduler-default">Configurable Priorities</h6>
<div class="paragraph">
<p>You can configure these priorities in the scheduler policy Configmap,
<code>policy-configmap</code> in the <code>openshift-config</code> project, to add labels to affect
how the priorities.</p>
</div>
<div class="paragraph">
<p>The type of the priority
function is identified by the argument that they take. Since these are
configurable, multiple priorities of the same type (but different
configuration parameters) can be combined as long as their user-defined names
are different.</p>
</div>
<div class="paragraph">
<p>For information on using these priorities, see Modifying Scheduler Policy.</p>
</div>
<div class="paragraph">
<p><strong><em>ServiceAntiAffinity</em></strong> takes a label and ensures a good spread of the pods
belonging to the same service across the group of nodes based on the label
values. It gives the same score to all nodes that have the same value for the
specified label. It gives a higher score to nodes within a group with the least
concentration of pods.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">"priorities":[
    {
        "name":"&lt;name&gt;", <b class="conum">(1)</b>
        "weight" : 1 <b class="conum">(2)</b>
        "argument":{
            "serviceAntiAffinity":{
                "label":[
                    "&lt;label&gt;" <b class="conum">(3)</b>
                ]
            }
        }
    }
]</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify a name for the priority.</p>
</li>
<li>
<p>Specify a weight. Enter a non-zero positive value.</p>
</li>
<li>
<p>Specify a label to match.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">        "name":"RackSpread", <b class="conum">(1)</b>
        "weight" : 1 <b class="conum">(2)</b>
        "argument":{
            "serviceAntiAffinity":{
                "label": "rack" <b class="conum">(3)</b>
            }
        }</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify a name for the priority.</p>
</li>
<li>
<p>Specify a weight. Enter a non-zero positive value.</p>
</li>
<li>
<p>Specify a label to match.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>In some situations using <code>ServiceAntiAffinity</code> based on custom labels does not spread pod as expected.
See <a href="https://access.redhat.com/solutions/3432401">this Red Hat Solution</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>*The <code>labelPreference</code> parameter gives priority based on the specified label.
If the label is present on a node, that node is given priority.
If no label is specified, priority is given to nodes that do not have a label.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-json" data-lang="json">"priorities":[
    {
        "name":"&lt;name&gt;", <b class="conum">(1)</b>
        "weight" : 1 <b class="conum">(2)</b>
        "argument":{
            "labelPreference":{
                "label": "&lt;label&gt;", <b class="conum">(3)</b>
                "presence": true <b class="conum">(4)</b>
            }
        }
    }
]</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify a name for the priority.</p>
</li>
<li>
<p>Specify a weight. Enter a non-zero positive value.</p>
</li>
<li>
<p>Specify a label to match.</p>
</li>
<li>
<p>Specify whether the label is required, either <code>true</code> or <code>false</code>.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-default-sample-nodes-scheduler-default">Sample Policy Configurations</h4>
<div class="paragraph">
<p>The configuration below specifies the default scheduler configuration, if it
were to be specified using the scheduler policy file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: mypolicy
  namespace: openshift-config
  selfLink: /api/v1/namespaces/openshift-config/configmaps/mypolicy
  uid: 83917dfb-4422-11e9-b2c9-0a5e37b2b12e
  resourceVersion: '1100851'
  creationTimestamp: '2019-03-11T17:24:23Z'
data:
  policy.cfg: "{\n\"kind\" : \"Policy\",\n\"apiVersion\" : \"v1\",\n\"predicates\" : [\n\t{\"name\" : \"PodFitsHostPorts\"},\n\t{\"name\" : \"PodFitsResources\"},\n\t{\"name\" : \"NoDiskConflict\"},\n\t{\"name\" : \"NoVolumeZoneConflict\"},\n\t{\"name\" : \"MatchNodeSelector\"},\n\t{\"name\" : \"HostName\"}\n\t],\n\"priorities\" : [\n\t{\"name\" : \"LeastRequestedPriority\", \"weight\" : 10},\n\t{\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1},\n\t{\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1},\n\t{\"name\" : \"EqualPriority\", \"weight\" : 1},\n\t{\"name\" : \"labelPreference\", \"label\" : \"rack\", \"presence\" : \"true\", \"weight\" : 2},\n\t]\n}\n "</code></pre>
</div>
</div>
<div class="paragraph">
<p>In all of the sample configurations below, the list of predicates and priority
functions is truncated to include only the ones that pertain to the use case
specified.  In practice, a complete/meaningful scheduler policy should include
most, if not all, of the default predicates and priorities listed above.</p>
</div>
<div class="paragraph">
<p>The following example defines three topological levels, region (affinity) &#8594; zone (affinity) &#8594; rack (anti-affinity):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">"{\n\"kind\" : \"Policy\",\n\"apiVersion\" : \"v1\",\n\"predicates\" : [\n\t{\"name\" : \"RegionZoneAffinity\", \"label\" : \"region\", \"label\" : \"zone\"}\n\t],\n\"priorities\" : [\n\t{\"name\" : \"serviceAntiAffinity\", \"label\" : \"rack\", \"weight\" : 1},\n\t]\n}\n"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following example defines three topological levels, city (affinity) &#8594; building
(anti-affinity) &#8594; room (anti-affinity):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">"{\n\"kind\" : \"Policy\",\n\"apiVersion\" : \"v1\",\n\"predicates\" : [\n\t{\"name\" : \"serviceAffinityy\", \"label\" : \"city\"},\n\t],\n\"priorities\" : [\n\t{\"name\" : \"serviceAntiAffinity\", \"label\" : \"building\" \"weight\" : 1}, \n\t{\"name\" : \"serviceAntiAffinity\", \"label\" : \"room\" \"weight\" : 1},\n\t]\n}\n"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following example defines a policy to only use nodes with the 'region' label defined and prefer nodes with the 'zone'
label defined:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">"{\n\"kind\" : \"Policy\",\n\"apiVersion\" : \"v1\",\n\"predicates\" : [\n\t{\"name\" : \"labelsPresence\", \"label\" : \"region\", \"presence\" : \"true\"},\n\t],\n\"priorities\" : [\n\t{\"name\" : \"ZonePreferred\", \"label\" : \"zone\", \"presence\" : \"true\", \"weight\" : 1},\n\t]\n}\n"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following example combines both static and configurable predicates and
also priorities:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">"{\n\"kind\" : \"Policy\",\n\"apiVersion\" : \"v1\",\n\"predicates\" : [\n\t{\"name\" : \"labelsPresence\", \"label\" : \"building\", \"presence\" : \"true\"}, \n\t{\"name\" : \"PodFitsHostPorts\"},\n\t{\"name\" : \"MatchNodeSelector\"},\n\t],\n\"priorities\" : [\n\t{\"name\" : \"ZonePreferred\", \"label\" : \"zone\", \"presence\" : \"true\", \"weight\" : 1},\n\t]\n}\n \"</code></pre>
</div>
</div>
<div id="nodes-scheduler-pod-affinity-nodes-scheduler-pod-affinity" class="paragraph">
<p>[[placing-pods-relative-to-other-pods-using-pod-affinity/anti-affinity-rules]]
= Placing pods relative to other pods using affinity/anti-affinity rules in OpenShift Enterprise
:experimental:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:asb-name: OpenShift Ansible Broker
:tsb-name: Template Service Broker
:relfileprefix: ../</p>
</div>
<div class="paragraph">
<p>Affinity is a property of pods that controls the nodes on which they prefer to be scheduled. Anti-affinity is a property of pods
that prevents a pod from being scheduled on a node.</p>
</div>
<div class="paragraph">
<p>In OpenShift Enterprise <em>pod affinity</em> and <em>pod anti-affinity</em> allow you to constrain which nodes your pod is eligible to be scheduled on based on the key/value labels on other pods.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-pod-affinity-about-nodes-scheduler-pod-affinity">Understanding pod affinity in OpenShift Enterprise</h4>
<div class="paragraph">
<p><em>Pod affinity</em> and <em>pod anti-affinity</em> allow you to constrain which nodes your pod is eligible to be scheduled on based on the key/value labels on other pods.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Pod affinity can tell the scheduler to locate a new pod on the same node as other pods if the label selector on the new pod matches the label on the current pod.</p>
</li>
<li>
<p>Pod anti-affinity can prevent the scheduler from locating a new pod on the same node as pods with the same labels if the label selector on the new pod matches the label on the current pod.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, using affinity rules, you could spread or pack pods within a service or relative to pods in other services. Anti-affinity rules allow you to prevent pods of a particular service  from scheduling  on the same nodes as pods of another service that are known to interfere with the performance of the pods of the first service. Or, you could spread the pods of a service across nodes or availability zones to reduce correlated failures.</p>
</div>
<div class="paragraph">
<p>There are two types of pod affinity rules: <em>required</em> and <em>preferred</em>.</p>
</div>
<div class="paragraph">
<p>Required rules <strong>must</strong> be met before a pod can be scheduled on a node. Preferred rules specify that, if the rule is met, the scheduler tries to enforce the rules, but does not guarantee enforcement.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Depending on your pod priority and preemption settings, the scheduler might not be able to find an appropriate node for a pod without violating affinity
requirements. If so, a pod might not be scheduled.</p>
</div>
<div class="paragraph">
<p>To prevent this situation, carefully configure pod affinity with equal-priority pods.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You configure pod affinity/anti-affinity through the pod specification files. You can specify a required rule, a preferred rule, or both. If you specify both, the node must first meet the required rule, then attempts to meet the preferred rule.</p>
</div>
<div class="paragraph">
<p>The following example shows a pod specification configured for pod affinity and anti-affinity.</p>
</div>
<div class="paragraph">
<p>In this example, the pod affinity rule indicates that the pod can schedule onto a node only if that node has at least one already-running pod with a label that has the key <code>security</code> and value <code>S1</code>. The pod anti-affinity rule says that the pod prefers to not schedule onto a node if that node is already running a pod with label having key <code>security</code> and value <code>S2</code>.</p>
</div>
<div class="listingblock">
<div class="title">Sample pod config file with pod affinity</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity: <b class="conum">(1)</b>
      requiredDuringSchedulingIgnoredDuringExecution: <b class="conum">(2)</b>
      - labelSelector:
          matchExpressions:
          - key: security <b class="conum">(3)</b>
            operator: In <b class="conum">(4)</b>
            values:
            - S1 <b class="conum">(3)</b>
        topologyKey: failure-domain.beta.kubernetes.io/zone
  containers:
  - name: with-pod-affinity
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Stanza to configure pod affinity.</p>
</li>
<li>
<p>Defines a required rule.</p>
</li>
<li>
<p>The key and value (label) that must be matched to apply the rule.</p>
</li>
<li>
<p>The operator represents the relationship between the label on the existing pod and the set of values in the <code>matchExpression</code> parameters in the specification for the new pod.  Can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, or <code>DoesNotExist</code>.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">Sample pod config file with pod anti-affinity</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: with-pod-antiaffinity
spec:
  affinity:
    podAntiAffinity: <b class="conum">(1)</b>
      preferredDuringSchedulingIgnoredDuringExecution: <b class="conum">(2)</b>
      - weight: 100  <b class="conum">(3)</b>
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security <b class="conum">(4)</b>
              operator: In <b class="conum">(5)</b>
              values:
              - S2
          topologyKey: kubernetes.io/hostname
  containers:
  - name: with-pod-affinity
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Stanza to configure pod anti-affinity.</p>
</li>
<li>
<p>Defines a preferred rule.</p>
</li>
<li>
<p>Specifies a weight for a preferred rule. The node with the highest weight is preferred.</p>
</li>
<li>
<p>Description of the pod label that determines when the anti-affinity rule applies. Specify a key and value for the label.</p>
</li>
<li>
<p>The operator represents the relationship between the label on the existing pod and the set of values in the <code>matchExpression</code> parameters in the specification for the new pod. Can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, or <code>DoesNotExist</code>.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>If labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod continues to run on the node.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-pod-affinity-configuring-nodes-scheduler-pod-affinity">Configuring a pod affinity rule</h4>
<div class="paragraph">
<p>The following steps demonstrate a simple two-pod configuration that creates pod with a label and a pod that uses affinity to allow scheduling with that pod.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a pod with a specific label in the pod specification:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ cat team4.yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-s1
  labels:
    security: S1
spec:
  containers:
  - name: security-s1
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
</li>
<li>
<p>When creating other pods, edit the pod specification as follows:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Use the <code>podAffinity</code> stanza to configure the <code>requiredDuringSchedulingIgnoredDuringExecution</code> parameter or <code>preferredDuringSchedulingIgnoredDuringExecution</code> parameter:</p>
</li>
<li>
<p>Specify the key and value that must be met. If you want the new pod to be scheduled with the other pod, use the same <code>key</code> and <code>value</code> parameters as the label on the first pod.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone</code></pre>
</div>
</div>
</li>
<li>
<p>Specify an <code>operator</code>. The operator can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, or <code>DoesNotExist</code>. For example, use the operator <code>In</code> to require the label to be in the node.</p>
</li>
<li>
<p>Specify a <code>topologyKey</code>, which is a prepopulated <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels">Kubernetes label</a> that the system uses to denote such a topology domain.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the pod.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;pod-spec&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-pod-anti-affinity-configuring-nodes-scheduler-pod-affinity">Configuring a pod anti-affinity rule</h4>
<div class="paragraph">
<p>The following steps demonstrate a simple two-pod configuration that creates pod with a label and a pod that uses an anti-affinity preferred rule to attempt to prevent scheduling with that pod.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a pod with a specific label in the pod specification:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ cat team4.yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-s2
  labels:
    security: S2
spec:
  containers:
  - name: security-s2
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
</li>
<li>
<p>When creating other pods, edit the pod specification to set the following parameters:</p>
</li>
<li>
<p>Use the <code>podAffinity</code> stanza to configure the <code>requiredDuringSchedulingIgnoredDuringExecution</code> parameter or <code>preferredDuringSchedulingIgnoredDuringExecution</code> parameter:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Specify a weight for the node, 1-100. The node that with highest weight is preferred.</p>
</li>
<li>
<p>Specify the key and values that must be met. If you want the new pod to not be scheduled with the other pod, use the same <code>key</code> and <code>value</code> parameters as the label on the first pod.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: kubernetes.io/hostname</code></pre>
</div>
</div>
</li>
<li>
<p>For a preferred rule, specify a weight, 1-100.</p>
</li>
<li>
<p>Specify an <code>operator</code>. The operator can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, or <code>DoesNotExist</code>. For example, use the operator <code>In</code> to require the label to be in the node.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Specify a <code>topologyKey</code>, which is a prepopulated <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels">Kubernetes label</a> that the system uses to denote such a topology domain.</p>
</li>
<li>
<p>Create the pod.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;pod-spec&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-pod-affinity-example-nodes-scheduler-pod-affinity">Sample pod affinity and anti-affinity rules</h4>
<div class="paragraph">
<p>The following examples demonstrate pod affinity and pod anti-affinity.</p>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-pod-affinity-example-affinity-nodes-scheduler-pod-affinity">Pod Affinity</h5>
<div class="paragraph">
<p>The following example demonstrates pod affinity for pods with matching labels and label selectors.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The pod <strong>team4</strong> has the label <code>team:4</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ cat team4.yaml
apiVersion: v1
kind: Pod
metadata:
  name: team4
  labels:
     team: "4"
spec:
  containers:
  - name: ocp
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>team4a</strong> has the label selector <code>team:4</code> under <code>podAffinity</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ cat pod-team4a.yaml
apiVersion: v1
kind: Pod
metadata:
  name: team4a
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: team
            operator: In
            values:
            - "4"
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-affinity
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
</li>
<li>
<p>The <strong>team4a</strong> pod is scheduled on the same node as the <strong>team4</strong> pod.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-pod-affinity-example-antiaffinity-nodes-scheduler-pod-affinity">Pod Anti-affinity</h5>
<div class="paragraph">
<p>The following example demonstrates pod anti-affinity for pods with matching labels and label selectors.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The pod <strong>pod-s1</strong> has the label <code>security:s1</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">cat pod-s1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: s1
  labels:
    security: s1
spec:
  containers:
  - name: ocp
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>pod-s2</strong> has the label selector <code>security:s1</code> under <code>podAntiAffinity</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">cat pod-s2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-s2
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - s1
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-antiaffinity
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>pod-s2</strong> is not scheduled unless there is a node with a pod that has the <code>security:s2</code> label. If there is no other pod with that label, the new pod remains in a pending state:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">NAME      READY     STATUS    RESTARTS   AGE       IP        NODE
pod-s2    0/1       Pending   0          32s       &lt;none&gt;</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-pod-affinity-example-no-labels-nodes-scheduler-pod-affinity">Pod Affinity with no Matching Labels</h5>
<div class="paragraph">
<p>The following example demonstrates pod affinity for pods without matching labels and label selectors.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The pod <strong>pod-s1</strong> has the label <code>security:s1</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ cat pod-s1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-s1
  labels:
    security: s1
spec:
  containers:
  - name: ocp
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>pod-s2</strong> has the label selector <code>security:s2</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ cat pod-s2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-s2
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - s2
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-affinity
    image: docker.io/ocpqe/hello-pod</code></pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>pod-s2</strong> cannot be scheduled on the same node as <code>pod-s1</code>.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="controlling-pod-placement-on-nodes-using-node-affinity-rules">Controlling pod placement on nodes using node affinity rules in OpenShift Enterprise</h3>
<div class="paragraph">
<p>Affinity is a property of pods that controls the nodes on which they prefer to be scheduled.</p>
</div>
<div class="paragraph">
<p>In OpenShift Enterprisenode affinity is a set of rules used by the scheduler to determine where a pod can be placed.
The rules are defined using custom labels on the nodes and label selectors specified in pods.</p>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-affinity-about-nodes-scheduler-node-affinity">Understanding node affinity</h4>
<div class="paragraph">
<p>Node affinity allows a pod to specify an affinity towards a group of nodes it can be placed on. The node does not have control over the placement.</p>
</div>
<div class="paragraph">
<p>For example, you could configure a pod to only run on a node with a specific CPU or in a specific availability zone.</p>
</div>
<div class="paragraph">
<p>There are two types of node affinity rules: <em>required</em> and <em>preferred</em>.</p>
</div>
<div class="paragraph">
<p>Required rules <strong>must</strong> be met before a pod can be scheduled on a node. Preferred rules specify that, if the rule is met, the scheduler tries to enforce the rules, but does not guarantee enforcement.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>If labels on a node change at runtime that results in an node affinity rule on a pod no longer being met, the pod continues to run on the node.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You configure node affinity through the pod specification file. You can specify a required rule, a preferred rule, or both. If you specify both, the node must first meet the required rule, then attempts to meet the preferred rule.</p>
</div>
<div class="paragraph">
<p>The following example is a pod specification with a rule that requires the pod be placed on a node with a label whose key is <code>e2e-az-NorthSouth</code> and whose value is either <code>e2e-az-North</code> or <code>e2e-az-South</code>:</p>
</div>
<div class="listingblock">
<div class="title">Sample pod configuration file with a node affinity required rule</div>
<div class="content">
<pre class="nowrap">apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity: <b class="conum">(1)</b>
      requiredDuringSchedulingIgnoredDuringExecution: <b class="conum">(2)</b>
        nodeSelectorTerms:
        - matchExpressions:
          - key: e2e-az-NorthSouth <b class="conum">(3)</b>
            operator: In <b class="conum">(4)</b>
            values:
            - e2e-az-North <b class="conum">(3)</b>
            - e2e-az-South <b class="conum">(3)</b>
  containers:
  - name: with-node-affinity
    image: docker.io/ocpqe/hello-pod</pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The stanza to configure node affinity.</p>
</li>
<li>
<p>Defines a required rule.</p>
</li>
<li>
<p>The key/value pair (label) that must be matched to apply the rule.</p>
</li>
<li>
<p>The operator represents the relationship between the label on the node and the set of values in the <code>matchExpression</code> parameters in the pod specification. This value can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, or <code>DoesNotExist</code>, <code>Lt</code>, or <code>Gt</code>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The following example is a node specification with a preferred rule that a node with a label whose key is <code>e2e-az-EastWest</code> and whose value is either <code>e2e-az-East</code> or <code>e2e-az-West</code> is preferred for the pod:</p>
</div>
<div class="listingblock">
<div class="title">Sample pod configuration file with a node affinity preferred rule</div>
<div class="content">
<pre class="nowrap">apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity: <b class="conum">(1)</b>
      preferredDuringSchedulingIgnoredDuringExecution: <b class="conum">(2)</b>
      - weight: 1 <b class="conum">(3)</b>
        preference:
          matchExpressions:
          - key: e2e-az-EastWest <b class="conum">(4)</b>
            operator: In <b class="conum">(5)</b>
            values:
            - e2e-az-East <b class="conum">(4)</b>
            - e2e-az-West <b class="conum">(4)</b>
  containers:
  - name: with-node-affinity
    image: docker.io/ocpqe/hello-pod</pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The stanza to configure node affinity.</p>
</li>
<li>
<p>Defines a preferred rule.</p>
</li>
<li>
<p>Specifies a weight for a preferred rule. The node with highest weight is preferred.</p>
</li>
<li>
<p>The key/value pair (label) that must be matched to apply the rule.</p>
</li>
<li>
<p>The operator represents the relationship between the label on the node and
the set of values in the <code>matchExpression</code> parameters in the pod specification.
This value can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, or <code>DoesNotExist</code>, <code>Lt</code>, or <code>Gt</code>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>There is no explicit <em>node anti-affinity</em> concept, but using the <code>NotIn</code> or <code>DoesNotExist</code> operator replicates that behavior.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>If you are using node affinity and node selectors in the same pod configuration, note the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If you configure both <code>nodeSelector</code> and <code>nodeAffinity</code>, both conditions must be satisfied for the pod to be scheduled onto a candidate node.</p>
</li>
<li>
<p>If you specify multiple <code>nodeSelectorTerms</code> associated with <code>nodeAffinity</code> types, then the pod can be scheduled onto a node if one of the <code>nodeSelectorTerms</code> is satisfied.</p>
</li>
<li>
<p>If you specify multiple <code>matchExpressions</code> associated with <code>nodeSelectorTerms</code>, then the pod can be scheduled onto a node only if all <code>matchExpressions</code> are satisfied.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-affinity-configuring-required-nodes-scheduler-node-affinity">Configuring a required node affinity rule</h4>
<div class="paragraph">
<p>Required rules <strong>must</strong> be met before a pod can be scheduled on a node.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>The following steps demonstrate a simple configuration that creates a node and a pod that the scheduler is required to place on the node.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add a label to a node using the <code>oc label node</code> command:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label node node1 e2e-az-name=e2e-az1</pre>
</div>
</div>
</li>
<li>
<p>In the pod specification, use the <code>nodeAffinity</code> stanza to configure the <code>requiredDuringSchedulingIgnoredDuringExecution</code> parameter:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Specify the key and values that must be met. If you want the new pod to be scheduled on the node you edited, use the same <code>key</code> and <code>value</code> parameters as the label in the node.</p>
</li>
<li>
<p>Specify an <code>operator</code>. The operator can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>, <code>Lt</code>, or <code>Gt</code>. For example, use the operator <code>In</code> to require the label to be in the node:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f e2e-az2.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-affinity-configuring-preferred-nodes-scheduler-node-affinity">Configuring a Preferred Node Affinity Rule</h4>
<div class="paragraph">
<p>Preferred rules specify that, if the rule is met, the scheduler tries to enforce the rules, but does not guarantee enforcement.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>The following steps demonstrate a simple configuration that creates a node and a pod that the scheduler tries to place on the node.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add a label to a node using the <code>oc label node</code> command:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label node node1 e2e-az-name=e2e-az3</pre>
</div>
</div>
</li>
<li>
<p>In the pod specification, use the <code>nodeAffinity</code> stanza to configure the <code>preferredDuringSchedulingIgnoredDuringExecution</code> parameter:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Specify a weight for the node, as a number 1-100. The node with highest weight is preferred.</p>
</li>
<li>
<p>Specify the key and values that must be met. If you want the new pod to be scheduled on the node you edited, use the same <code>key</code> and <code>value</code> parameters as the label in the node:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: e2e-az-name
            operator: In
            values:
            - e2e-az3</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Specify an <code>operator</code>. The operator can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>, <code>Lt</code>, or <code>Gt</code>. For example, use the operator <code>In</code> to require the label to be in the node.</p>
</li>
<li>
<p>Create the pod.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f e2e-az3.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-affinity-examples-nodes-scheduler-node-affinity">Sample node affinity rules</h4>
<div class="paragraph">
<p>The following examples demonstrate node affinity.</p>
</div>
<div class="sect4">
<h5 id="admin-guide-sched-affinity-examples1-nodes-scheduler-node-affinity">Node Affinity with Matching Labels</h5>
<div class="paragraph">
<p>The following example demonstrates node affinity for a node and pod with matching labels:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <strong>Node1</strong> node has the label <code>zone:us</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label node node1 zone=us</pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>pod-s1</strong> has the <code>zone</code> and <code>us</code> key/value pair under a required node affinity rule:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ cat pod-s1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-s1
spec:
  containers:
    - image: "docker.io/ocpqe/hello-pod"
      name: hello-pod
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: "zone"
              operator: In
              values:
              - us</pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>pod-s1</strong> can be scheduled on <strong>Node1</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pod -o wide
NAME     READY     STATUS       RESTARTS   AGE      IP      NODE
pod-s1   1/1       Running      0          4m       IP1     node1</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="admin-guide-sched-affinity-examples2-nodes-scheduler-node-affinity">Node Affinity with No Matching Labels</h5>
<div class="paragraph">
<p>The following example demonstrates node affinity for a node and pod without matching labels:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <strong>Node1</strong> node has the label <code>zone:emea</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label node node1 zone=emea</pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>pod-s1</strong> has the <code>zone</code> and <code>us</code> key/value pair under a required node affinity rule:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ cat pod-s1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-s1
spec:
  containers:
    - image: "docker.io/ocpqe/hello-pod"
      name: hello-pod
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: "zone"
              operator: In
              values:
              - us</pre>
</div>
</div>
</li>
<li>
<p>The pod <strong>pod-s1</strong> cannot be scheduled on <strong>Node1</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe pod pod-s1
&lt;---snip---&gt;
Events:
 FirstSeen LastSeen Count From              SubObjectPath  Type                Reason
 --------- -------- ----- ----              -------------  --------            ------
 1m        33s      8     default-scheduler Warning        FailedScheduling    No nodes are available that match all of the following predicates:: MatchNodeSelector (1).</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-affinity-addtl-resources-nodes-scheduler-node-affinity">Additional resources</h4>
<div class="paragraph">
<p>For information about changing node labels, see <a href="#nodes-nodes-working-updating-nodes-nodes-working">Understanding how to update labels on nodes</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="placing-a-pod-on-a-specific-node-by-name">Placing a pod on a specific node by name in OpenShift Enterprise</h3>
<div class="paragraph">
<p>Use the Pod Node Constraints admission controller to ensure a pod
is deployed onto only a specified node host by assigning it a label
and specifying this in the <code>nodeName</code> setting in a pod configuration.</p>
</div>
<div class="paragraph">
<p>The Pod Node Constraints admission controller ensures that pods
are deployed onto only specified node hosts using labels
and prevents users without a specific role from using the
<code>nodeSelector</code> field to schedule pods.</p>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-name-configuring-nodes-scheduler-node-names">Configuring the Pod Node Constraints admission controller to use names in OpenShift Enterprise</h4>
<div class="paragraph">
<p>You can configure the Pod Node Constraints admission controller to ensure that pods are only placed onto nodes with a specific name.</p>
</div>
<div class="paragraph">
<div class="title">Prerequisites</div>
<p>Ensure you have the desired labels
and node selector set up in your environment.</p>
</div>
<div class="paragraph">
<p>For example, make sure that your pod configuration features the <code>nodeName</code>
value indicating the desired label:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
spec:
  nodeName: &lt;value&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To configure the Pod Node Constraints admission controller:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a file containing the admission controller information:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">podNodeSelectorPluginConfig:
 clusterDefaultNodeSelector: name-of-node-selector
 namespace1: name-of-node-selector
 namespace2: name-of-node-selector</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">podNodeConstraintsPluginConfig:
 clusterDefaultNodeSelector: ns1
 ns1: region=west,env=test,infra=fedora,os=fedora</code></pre>
</div>
</div>
</li>
<li>
<p>Create an <strong>AdmissionConfiguration</strong> object that references the file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: AdmissionConfiguration
apiVersion: apiserver.k8s.io/v1alpha1
plugins:
- name: PodNodeConstraints
  path: podnodeconstraints.yaml</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-names-addtl-resources-nodes-scheduler-node-names">Additional resources</h4>
<div class="paragraph">
<p>For information about changing node labels, see <a href="#nodes-nodes-working-updating-nodes-nodes-working">Understanding how to update labels on nodes</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="placing-a-pod-in-a-specific-project">Placing a pod in a specific project in OpenShift Enterprise</h3>
<div class="paragraph">
<p>The Pod Node Selector admission controller allows you to force pods onto nodes associated with a specific project and prevent pods from being scheduled in those nodes.</p>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-projects-about-nodes-scheduler-node-project">Understanding how to constrain pods by project name in OpenShift Enterprise</h4>
<div class="paragraph">
<p>The Pod Node Selector admission controller determines where a pod can be placed using labels on projects and node selectors specified in pods. A new pod will be placed on a node associated with a project only if the node selectors in the pod match the labels in the project.</p>
</div>
<div class="paragraph">
<p>After the pod is created, the node selectors are merged into the pod so that the pod specification includes the labels originally included in the specification and any new labels from the node selectors. The example below illustrates the merging effect.</p>
</div>
<div class="paragraph">
<p>The Pod Node Selector admission controller also allows you to create a list of labels that are permitted in a specific project. This list acts as a whitelist that lets developers know what labels are acceptable to use in a project and gives administrators greater control over labeling in a cluster.</p>
</div>
<div class="paragraph">
<p>The Pod Node Selector uses the annotation key <code>scheduler.alpha.kubernetes.io/node-selector</code> to assign node selectors to namespaces.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Namespace
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/node-selector: name-of-node-selector
  name: namespace3</code></pre>
</div>
</div>
<div class="paragraph">
<p>This admission controller has the following behavior:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If the Namespace has an annotation with a key scheduler.alpha.kubernetes.io/node-selector, use its value as the node selector.</p>
</li>
<li>
<p>If the namespace lacks such an annotation, use the clusterDefaultNodeSelector defined in the PodNodeSelector plugin configuration file as the node selector.</p>
</li>
<li>
<p>Evaluate the pods node selector against the namespace node selector for conflicts. Conflicts result in rejection.</p>
</li>
<li>
<p>Evaluate the pods node selector against the namespace-specific whitelist defined the plugin configuration file. Conflicts result in rejection.</p>
</li>
</ol>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-node-projects-configuring-nodes-scheduler-node-project">Configuring the Pod Node Selector admission controller to use projects in OpenShift Enterprise</h5>
<div class="paragraph">
<p>You can configure the Pod Node Selector admission controller to ensure that pods are only placed onto nodes in specific projects.
The Pod Node Selector admission controller uses a configuration file to set options for the behavior of the backend.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a file containing the admission controller information:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">podNodeSelectorPluginConfig:
 clusterDefaultNodeSelector: name-of-node-selector
 namespace1: name-of-node-selector
 namespace2: name-of-node-selector</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">podNodeSelectorPluginConfig:
 clusterDefaultNodeSelector: ns1
 ns1: region=west,env=test,infra=fedora,os=fedora</code></pre>
</div>
</div>
</li>
<li>
<p>Create an <strong>AdmissionConfiguration</strong> object that references the file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: AdmissionConfiguration
apiVersion: apiserver.k8s.io/v1alpha1
plugins:
- name: PodNodeSelector
  path: podnodeselector.yaml</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="placing-pods-onto-overcommited-nodes">Placing pods onto overcommited nodes in OpenShift Enterprise</h3>
<div class="paragraph">
<p>In an <em>overcommited</em> state, the sum of the container compute resource requests and limits exceeds the resources available on the system.
Overcommitment might be desirable in development environments where a trade-off of guaranteed performance for capacity is acceptable.</p>
</div>
<div class="paragraph">
<p>Requests and limits enable administrators to allow and manage the overcommitment of resources on a node.
The scheduler uses requests for scheduling your container and providing a minimum service guarantee.
Limits constrain the amount of compute resource that may be consumed on your node.</p>
</div>
<div class="sect3">
<h4 id="nodes-cluster-overcommit-about-nodes-sceduler-overcommit">Understanding overcommitment in OpenShift Enterprise</h4>
<div class="paragraph">
<p>Requests and limits enable administrators to allow and manage the overcommitment of resources on a node. The scheduler uses requests for scheduling your container and providing a minimum service guarantee. Limits constrain the amount of compute resource that may be consumed on your node.</p>
</div>
<div class="paragraph">
<p>OpenShift Enterprise administrators can control the level of overcommit and manage container density on nodes by configuring masters to override the ratio between request and limit set on developer containers. In conjunction with a per-project LimitRange specifying limits and defaults, this adjusts the container limit and request to achieve the desired level of overcommit.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>That these overrides have no effect if no limits have been set on containers. Create a LimitRange object with default limits (per individual project, or in the project template) in order to ensure that the overrides apply.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After these overrides, the container limits and requests must still be validated by any LimitRange objects in the project. It is possible, for example, for developers to specify a limit close to the minimum limit, and have the request then be overridden below the minimum limit, causing the pod to be forbidden. This unfortunate user experience should be addressed with future work, but for now, configure this capability and LimitRanges with caution.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-overcommit-configure-nodes-nodes-sceduler-overcommit">Understanding nodes overcommitment</h4>
<div class="paragraph">
<p>In an overcommitted environment, it is important to properly configure your node to provide best system behavior.</p>
</div>
<div class="paragraph">
<p>When the node starts, it ensures that the kernel tunable flags for memory
management are set properly. The kernel should never fail memory allocations
unless it runs out of physical memory.</p>
</div>
<div class="paragraph">
<p>In an overcommitted environment, it is important to properly configure your node to provide best system behavior.</p>
</div>
<div class="paragraph">
<p>When the node starts, it ensures that the kernel tunable flags for memory
management are set properly. The kernel should never fail memory allocations
unless it runs out of physical memory.</p>
</div>
<div class="paragraph">
<p>To ensure this behavior, OpenShift Enterprise configures the kernel to always overcommit
memory by setting the <code>vm.overcommit_memory</code> parameter to <code>1</code>, overriding the
default operating system setting.</p>
</div>
<div class="paragraph">
<p>OpenShift Enterprise also configures the kernel not to panic when it runs out of memory
by setting the <code>vm.panic_on_oom</code> parameter to <code>0</code>. A setting of 0 instructs the
kernel to call oom_killer in an Out of Memory (OOM) condition, which kills
processes based on priority</p>
</div>
<div class="paragraph">
<p>You can view the current setting by running the following commands on your node:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ sysctl -a |grep commit

vm.overcommit_memory = 0</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ sysctl -a |grep panic
vm.panic_on_oom = 0</pre>
</div>
</div>
<div class="paragraph">
<p>You can change these settings using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ sysctl -w vm.overcommit_memory=1
$ sysctl -w vm.panic_on_oom=0</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The above flags should already be set on nodes, and no further action is
required.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can also perform the following configurations for each node:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Disable or enforce CPU limits using CPU CFS quotas</p>
</li>
<li>
<p>Reserve resources for system processes</p>
</li>
<li>
<p>Reserve memory across quality of service tiers</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="controlling-pod-placement-using-node-taints">Controlling pod placement using node taints in OpenShift Enterprise</h3>
<div class="paragraph">
<p>Taints and tolerations allow the Node to control which Pods should (or should not) be scheduled on them.</p>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-taints-tolerations-about-nodes-scheduler-taints-tolerations">Understanding taints and tolerations in OpenShift Enterprise</h4>
<div class="paragraph">
<p>A <em>taint</em> allows a node to refuse pod to be scheduled unless that pod has a matching <em>toleration</em>.</p>
</div>
<div class="paragraph">
<p>You apply taints to a node through the node specification (<code>NodeSpec</code>) and apply tolerations to a pod through the pod specification (<code>PodSpec</code>). A taint on a node instructs the node to repel all pods that do not tolerate the taint.</p>
</div>
<div class="paragraph">
<p>Taints and tolerations consist of a key, value, and effect. An operator allows you to leave one of these parameters empty.</p>
</div>
<table id="taint-components-table-nodes-scheduler-taints-tolerations" class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Taint and toleration components</caption>
<colgroup>
<col style="width: 27.2727%;">
<col style="width: 72.7273%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Parameter</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>key</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>The <code>key</code> is any string, up to 253 characters. The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>value</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>The <code>value</code> is any string, up to 63 characters. The value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>effect</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>The effect is one of the following:</p>
</div>
<table class="tableblock frame-none grid-all stretch">
<colgroup>
<col style="width: 40%;">
<col style="width: 60%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>NoSchedule</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="ulist">
<ul>
<li>
<p>New pods that do not match the taint are not scheduled onto that node.</p>
</li>
<li>
<p>Existing pods on the node remain.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>PreferNoSchedule</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="ulist">
<ul>
<li>
<p>New pods that do not match the taint might be scheduled onto that node, but the scheduler tries not to.</p>
</li>
<li>
<p>Existing pods on the node remain.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>NoExecute</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="ulist">
<ul>
<li>
<p>New pods that do not match the taint cannot be scheduled onto that node.</p>
</li>
<li>
<p>Existing pods on the node that do not have a matching toleration  are removed.</p>
</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>operator</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><table class="tableblock frame-none grid-all stretch">
<colgroup>
<col style="width: 40%;">
<col style="width: 60%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Equal</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The <code>key</code>/<code>value</code>/<code>effect</code> parameters must match. This is the default.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Exists</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The <code>key</code>/<code>effect</code> parameters must match. You must leave a blank <code>value</code> parameter, which matches any.</p></td>
</tr>
</tbody>
</table></div></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>A toleration matches a taint:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the <code>operator</code> parameter is set to <code>Equal</code>:</p>
<div class="ulist">
<ul>
<li>
<p>the <code>key</code> parameters are the same;</p>
</li>
<li>
<p>the <code>value</code> parameters are the same;</p>
</li>
<li>
<p>the <code>effect</code> parameters are the same.</p>
</li>
</ul>
</div>
</li>
<li>
<p>If the <code>operator</code> parameter is set to <code>Exists</code>:</p>
<div class="ulist">
<ul>
<li>
<p>the <code>key</code> parameters are the same;</p>
</li>
<li>
<p>the <code>effect</code> parameters are the same.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-taints-tolerations-about-seconds-nodes-scheduler-taints-tolerations">Understanding how to use toleration seconds to delay pod evictions</h5>
<div class="paragraph">
<p>You can specify how long a pod can remain bound to a node before being evicted by specifying the <code>tolerationSeconds</code> parameter in the pod specification. If a taint with the <code>NoExecute</code> effect is added to a node, any pods that do not tolerate the taint are evicted immediately (pods that do tolerate the taint are not evicted). However, if a pod that to be evicted has the <code>tolerationSeconds</code> parameter, the pod is not evicted until that time period expires.</p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here, if this pod is running but does not have a matching taint, the pod stays bound to the node for 3,600 seconds and then be evicted. If the taint is removed before that time, the pod is not evicted.</p>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-taints-tolerations-about-multiple-nodes-scheduler-taints-tolerations">Understanding how to use multiple taints</h5>
<div class="paragraph">
<p>You can put multiple taints on the same node and multiple tolerations on the same pod. OpenShift Enterprise processes multiple taints and tolerations as follows:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Process the taints for which the pod has a matching toleration.</p>
</li>
<li>
<p>The remaining unmatched taints have the indicated effects on the pod:</p>
<div class="ulist">
<ul>
<li>
<p>If there is at least one unmatched taint with effect <code>NoSchedule</code>, OpenShift Enterprise cannot schedule a pod onto that node.</p>
</li>
<li>
<p>If there is no unmatched taint with effect <code>NoSchedule</code> but there is at least one unmatched taint with effect <code>PreferNoSchedule</code>, OpenShift Enterprise tries to not schedule the pod onto the node.</p>
</li>
<li>
<p>If there is at least one unmatched taint with effect <code>NoExecute</code>, OpenShift Enterprise evicts the pod from the node (if it is already running on the node), or the pod is not scheduled onto the node (if it is not yet running on the node).</p>
<div class="ulist">
<ul>
<li>
<p>Pods that do not tolerate the taint are evicted immediately.</p>
</li>
<li>
<p>Pods that tolerate the taint without specifying <code>tolerationSeconds</code> in their toleration specification remain bound forever.</p>
</li>
<li>
<p>Pods that tolerate the taint with a specified <code>tolerationSeconds</code> remain bound for the specified amount of time.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The node has the following taints:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm taint nodes node1 key1=value1:NoSchedule
$ oc adm taint nodes node1 key1=value1:NoExecute
$ oc adm taint nodes node1 key2=value2:NoSchedule</pre>
</div>
</div>
</li>
<li>
<p>The pod has the following tolerations:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>In this case, the pod cannot be scheduled onto the node, because there is no toleration matching the third taint. The pod continues running if it is already running on the node when the taint is added, because the third taint is the only
one of the three that is not tolerated by the pod.</p>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-taints-tolerations-about-prevent-nodes-scheduler-taints-tolerations">Preventing pod eviction for node problems</h5>
<div class="paragraph">
<p>OpenShift Enterprise can be configured to represent <strong>node unreachable</strong> and <strong>node not ready</strong> conditions as taints.  This allows per-pod specification of how long to remain bound to a node that becomes unreachable or not ready, rather than using the default of five minutes.</p>
</div>
<div class="paragraph">
<p>The Taint-Based Evictions feature is enabled by default. The taints are automatically added by the node controller and the normal logic for evicting pods from <code>Ready</code> nodes is disabled.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If a node enters a not ready state, the <code>node.kubernetes.io/not-ready:NoExecute</code>  taint is added and pods cannot be scheduled on the node. Existing pods remain for the toleration seconds period.</p>
</li>
<li>
<p>If a node enters a not reachable state, the <code>node.kubernetes.io/unreachable:NoExecute</code> taint is added and pods cannot be scheduled on the node. Existing pods remain for the toleration seconds period.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This feature, in combination with <code>tolerationSeconds</code>, allows a pod to specify how long it should stay bound to a node that has one or both of these problems.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>DaemonSet pods are created with <code>NoExecute</code> tolerations for <code>node.kubernetes.io/unreachable</code> and <code>node.kubernetes.io/not-ready</code>
with no <code>tolerationSeconds</code> to ensure that DaemonSet pods are never evicted due to these problems, even when the Default Toleration Seconds feature is disabled.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-taints-tolerations-adding-nodes-scheduler-taints-tolerations">Adding taints and tolerations in OpenShift Enterprise</h4>
<div class="paragraph">
<p>You add taints to nodes and tolerations to pods allow the node to control which pods should (or should not) be scheduled on them.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Use the following command using the parameters described in the taint and toleration components table:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm taint nodes &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm taint nodes node1 key1=value1:NoSchedule</pre>
</div>
</div>
<div class="paragraph">
<p>This example places a taint on <code>node1</code> that has key <code>key1</code>, value <code>value1</code>, and taint effect <code>NoSchedule</code>.</p>
</div>
</li>
<li>
<p>Add a toleration to a pod by editing the pod specification to include a <code>tolerations</code> section:</p>
<div class="listingblock">
<div class="title">Sample pod configuration file with <code>Equal</code> operator</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">tolerations:
- key: "key1" <b class="conum">(1)</b>
  operator: "Equal" <b class="conum">(1)</b>
  value: "value1" <b class="conum">(1)</b>
  effect: "NoExecute" <b class="conum">(1)</b>
  tolerationSeconds: 3600 <b class="conum">(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The toleration parameters, as described in the taint and toleration components table.</p>
</li>
<li>
<p>The <code>tolerationSeconds</code> parameter specifies how long a pod can remain bound to a node before being evicted.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="title">Sample pod configuration file with <code>Exists</code> operator</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 3600</code></pre>
</div>
</div>
<div class="paragraph">
<p>Both of these tolerations match the taint created by the <code>oc adm taint</code> command above. A pod with either toleration would be able to schedule onto <code>node1</code>.</p>
</div>
</li>
</ol>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-taints-tolerations_dedicating-nodes-scheduler-taints-tolerations">Dedicating a Node for a User using taints and tolerations</h5>
<div class="paragraph">
<p>You can specify a set of nodes for exclusive use by a particular set of users.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To specify dedicated nodes:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add a taint to those nodes:</p>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm taint nodes node1 dedicated=groupName:NoSchedule</pre>
</div>
</div>
</li>
<li>
<p>Add a corresponding toleration to the pods by writing a custom admission controller.</p>
<div class="paragraph">
<p>Only the pods with the tolerations are allowed to use the dedicated nodes.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-taints-tolerations-bindings-nodes-scheduler-taints-tolerations">Binding a user to a Node using taints and tolerations</h5>
<div class="paragraph">
<p>You can configure a node so that particular users can use only the dedicated nodes.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To configure a node so that users can use only that node:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add a taint to those nodes:</p>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm taint nodes node1 dedicated=groupName:NoSchedule</pre>
</div>
</div>
</li>
<li>
<p>Add a corresponding toleration to the pods by writing a custom admission controller.</p>
<div class="paragraph">
<p>The admission controller should add a node affinity to require that the pods can only schedule onto nodes labeled with the <code>key:value</code> label (<code>dedicated=groupName</code>).</p>
</div>
</li>
<li>
<p>Add a label similar to the taint (such as the <code>key:value</code> label) to the dedicated nodes.</p>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="nodes-scheduler-taints-tolerations-special-nodes-scheduler-taints-tolerations">Controlling Nodes with special hardware using taints and tolerations</h5>
<div class="paragraph">
<p>In a cluster where a small subset of nodes have specialized hardware (for example GPUs), you can use taints and tolerations to keep pods that do not need the specialized hardware off of those nodes, leaving the nodes for pods that do need the specialized hardware. You can also require pods that need specialized hardware to use specific nodes.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To ensure pods are blocked from the specialized hardware:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Taint the nodes that have the specialized hardware using one of the following commands:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm taint nodes &lt;node-name&gt; disktype=ssd:NoSchedule
$ oc adm taint nodes &lt;node-name&gt; disktype=ssd:PreferNoSchedule</pre>
</div>
</div>
</li>
<li>
<p>Adding a corresponding toleration to pods that use the special hardware using an admission controller.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example, the admission controller could use some characteristic(s) of the pod to determine that the pod should be allowed to use the special nodes by adding a toleration.</p>
</div>
<div class="paragraph">
<p>To ensure pods can only use the specialized hardware, you need some additional mechanism. For example, you could label the nodes that have the special hardware and use node affinity on the pods that need the hardware.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="constraining-pod-placement-using-node-selectors">Constraining pod placement using node selectors in OpenShift Enterprise</h3>
<div class="paragraph">
<p>A <em>node selector</em> specifies a map of key-value pairs. The rules are defined using custom labels on nodes and selectors specified in pods.</p>
</div>
<div class="paragraph">
<p>For the pod to be eligible to run on a node, the pod must have the indicated key-value pairs as the label on the node.</p>
</div>
<div class="paragraph">
<p>If you are using node affinity and node selectors in the same pod configuration, see the important considerations below.</p>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-selector-about-nodes-scheduler-node-selectors">Understanding node selectors in OpenShift Enterprise</h4>
<div class="paragraph">
<p>Using <em>node selectors</em>, you can ensure that pods are only placed onto nodes with specific labels. As a cluster administrator, you can
use the Pod Node Constraints admission controller to set a policy that prevents users without the <strong>pods/binding</strong> permission
from using node selectors to schedule pods.</p>
</div>
<div class="paragraph">
<p>The <code>nodeSelectorLabelBlacklist</code> admission controller field gives you control over the labels that certain roles can specify in a pod configuration&#8217;s
<code>nodeSelector</code> field. Users, service accounts, and groups that have the
<strong>pods/binding</strong> permission role can specify any node selector. Those without the
<strong>pods/binding</strong> permission are prohibited from setting a <code>nodeSelector</code> for any
label that appears in <code>nodeSelectorLabelBlacklist</code>.</p>
</div>
<div class="paragraph">
<p>For example, an OpenShift Enterprise cluster might consist of five data
centers spread across two regions. In the U.S., <strong>us-east</strong>, <strong>us-central</strong>, and
<strong>us-west</strong>; and in the Asia-Pacific region (APAC), <strong>apac-east</strong> and <strong>apac-west</strong>.
Each node in each geographical region is labeled accordingly. For example,
<code>region: us-east</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>See Updating Labels on Nodes for details on assigning labels.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>As a cluster administrator, you can create an infrastructure where application
developers should be deploying pods only onto the nodes closest to their
geographical location. You can create a node selector, grouping the U.S. data centers into <code>superregion: us</code> and the APAC
data centers into <code>superregion: apac</code>.</p>
</div>
<div class="paragraph">
<p>To maintain an even loading of resources per data center, you can add the
desired <code>region</code> to the <code>nodeSelectorLabelBlacklist</code> section of a master
configuration. Then, whenever a developer located in the U.S. creates a pod, it
is deployed onto a node in one of the regions with the <code>superregion: us</code> label.
If the developer tries to target a specific region for their pod (for example,
<code>region: us-east</code>), they receive an error. If they try again, without the
node selector on their pod, it can still be deployed onto the region they tried
to target, because <code>superregion: us</code> is set as the project-level node selector,
and nodes labeled <code>region: us-east</code> are also labeled <code>superregion: us</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-node-selectors-configuring-nodes-scheduler-node-selectors">Configuring the Pod Node Constraints admission controller to use node selectors in OpenShift Enterprise</h4>
<div class="paragraph">
<p>You can configure the Pod Node Constraints admission controller to ensure that pods are only placed onto nodes with specific labels.</p>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>Ensure you have the desired labels
labels on your nodes.
and node selector set up in your environment.</p>
<div class="paragraph">
<p>For example, make sure that your pod configuration features the <code>nodeSelector</code>
value indicating the desired label:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
spec:
  nodeSelector:
    &lt;key&gt;: &lt;value&gt;
...</code></pre>
</div>
</div>
</li>
<li>
<p>Create a file containing the admission controller information:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">podNodeSelectorPluginConfig:
 clusterDefaultNodeSelector: name-of-node-selector
 namespace1: name-of-node-selector
 namespace2: name-of-node-selector</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">podNodeConstraintsPluginConfig:
 clusterDefaultNodeSelector: ns1
 ns1: region=west,env=test,infra=fedora,os=fedora</code></pre>
</div>
</div>
</li>
<li>
<p>Create an <strong>AdmissionConfiguration</strong> object that references the file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: AdmissionConfiguration
apiVersion: apiserver.k8s.io/v1alpha1
plugins:
- name: PodNodeConstraints
  path: podnodeconstraints.yaml
  nodeSelectorLabelBlacklist:
  kubernetes.io/hostname
  - &lt;label&gt;</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>If you are using node selectors and node affinity in the same pod configuration, note the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If you configure both <code>nodeSelector</code> and <code>nodeAffinity</code>, both conditions must be satisfied for the pod to be scheduled onto a candidate node.</p>
</li>
<li>
<p>If you specify multiple <code>nodeSelectorTerms</code> associated with <code>nodeAffinity</code> types, then the pod can be scheduled onto a node if one of the <code>nodeSelectorTerms</code> is satisfied.</p>
</li>
<li>
<p>If you specify multiple <code>matchExpressions</code> associated with <code>nodeSelectorTerms</code>, then the pod can be scheduled onto a node only if all <code>matchExpressions</code> are satisfied.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="keeping-your-cluster-balanced-using-the-descheduler">Keeping your cluster balanced using the descheduler</h3>
<div class="paragraph">
<p><em>Descheduling</em> involves evicting pods based on specific policies so that the pods can be rescheduled onto more appropriate nodes.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>Descheduler is a Technology Preview release only. Technology Preview releases
are not supported with Red Hat production service-level agreements (SLAs) and
might not be functionally complete, and Red Hat does not recommend using them
for production. These features provide early access to upcoming product
features, enabling customers to test functionality and provide feedback during
the development process. For more information see
<a href="https://access.redhat.com/support/offerings/techpreview/">Red Hat Technology
Preview Features Support Scope</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-descheduling-about-nodes-scheduler-descheduler">Understanding how to evict pods with the descheduler</h4>
<div class="paragraph">
<p>Because OpenShift Enterprise clusters are highly dynamic and their state changes over time
your cluster can benefit from installing and using the Descheduler Operator to manage the descheduler which can evict from one node and rescheduling onto a more appropriate node for various reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Nodes are under- or over-utilized.</p>
</li>
<li>
<p>Pod and node affinity requirements, such as taints or labels, have changed and the original scheduling decisions are no longer appropriate for certain nodes.</p>
</li>
<li>
<p>Node failure requires pods to be moved.</p>
</li>
<li>
<p>New nodes are added to clusters.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The descheduler does not schedule replacement of evicted pods. The scheduler automatically performs this task for the evicted pods.</p>
</div>
<div class="paragraph">
<p>It is important to note that there are a number of core components, such as Heapster and DNS, that are critical to a fully functional cluster,
but, run on a regular cluster node rather than the master. A cluster may stop working properly if the component is evicted. To prevent the
descheduler from removing these pods, configure the pod as a critical pod by adding a priority class to the pod specification.</p>
</div>
<div class="paragraph">
<p>The descheduler does not evict the following types of pods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Critical pods (with an appropriate priority class).</p>
</li>
<li>
<p>Pods (<a href="https://kubernetes.io/docs/tasks/administer-cluster/static-pod/">static and mirror pods</a> or pods in standalone mode) not associated with a ReplicaSet, Replication Controller, Deployment, or Job (because these pods are not recreated).</p>
</li>
<li>
<p>Pods associated with DaemonSets.</p>
</li>
<li>
<p>Pods with local storage.</p>
</li>
<li>
<p>Pods subject to Pod Disruption Budget (PDB) are not evicted if descheduling violates the PDB. The pods can be evicted using
an eviction policy.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Best efforts pods are evicted before Burstable and Guaranteed pods.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The following is a high-level process to configure and run the descheduler:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create a role with the needed permissions.</p>
</li>
<li>
<p>Define the descheduling behavior in a policy file.</p>
</li>
<li>
<p>Install the descheduler through the web console.</p>
</li>
<li>
<p>Edit the descheduler custom resource to specify the descheduler policy to use.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-descheduler-create-role-nodes-scheduler-descheduler">Creating a cluster role for descheduling in OpenShift Enterprise</h4>
<div class="paragraph">
<p>In order for the Descheduler Operator to remove pods from nodes and replace them on other nodes, you first need a specific cluster role
with the necessary permissions for the descheduler to work in a pod.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To configure the necessary cluster role and associated service account:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a cluster role with the following rules:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: descheduler-cluster-role
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list"] <b class="conum">(1)</b>
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list", "delete"] <b class="conum">(2)</b>
- apiGroups: [""]
  resources: ["pods/eviction"] <b class="conum">(3)</b>
  verbs: ["create"]</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Configures the role to allow viewing nodes.</p>
</li>
<li>
<p>Configures the role to allow viewing and deleting pods.</p>
</li>
<li>
<p>Allows a node to evict pods bound to itself.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the service account that runs the job:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create sa &lt;file-name&gt;.yaml -n openshift-operators</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create sa descheduler-sa.yaml -n openshift-operators</pre>
</div>
</div>
</li>
<li>
<p>Bind the cluster role to the schedluer service account:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create clusterrolebinding descheduler-cluster-role-binding \
    --clusterrole=descheduler-cluster-role \
    --serviceaccount=openshift-operators:descheduler-sa</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-descheduler-install-nodes-scheduler-descheduler">Installing the Descheduler Operator</h4>
<div class="paragraph">
<p>You can use the OpenShift Enterprise web console to install the Descheduler Operator.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>The process to install the descheduler involves installing the Descheduler Operator and creating a descheduler instance.</p>
</div>
<div class="paragraph">
<p>To install the descheduler:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the OpenShift Enterprise console, click <strong>Catalog</strong> &#8594; <strong>OperatorHub</strong>.</p>
</li>
<li>
<p>Choose  <strong>Elasticsearch Operator</strong> from the list of available Operators, and click <strong>Install</strong>.</p>
</li>
<li>
<p>On the <strong>Create Operator Subscription</strong> page, select <strong>All namespaces on the cluster</strong> under <strong>Installation Mode</strong>.
Then, click <strong>Subscribe</strong>.</p>
<div class="paragraph">
<p>This makes the Operator available to all users and projects that use this OpenShift Enterprise cluster.</p>
</div>
</li>
<li>
<p>Verify the operator installations:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Switch to the <strong>Catalog</strong>  <strong>Installed Operators</strong> page.</p>
</li>
<li>
<p>Ensure that <strong>Descheduler</strong> is listed on the <strong>InstallSucceeded</strong> tab with a <strong>Status</strong> of <strong>InstallSucceeded</strong>. Change to the <strong>openshift-operators</strong> project or
<strong>all projects</strong> if necessary.</p>
<div class="paragraph">
<p>If the operator does not appear as installed, to troubleshoot further:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>On the <strong>Copied</strong> tab of the <strong>Installed Operators</strong> page, if an operator show a <strong>Status</strong> of
<strong>Copied</strong>, this indicates the installation is in process and is expected behavior.</p>
</li>
<li>
<p>Switch to the <strong>Catalog</strong>  <strong>Operator Management</strong> page and inspect
the <strong>Operator Subscriptions</strong> and <strong>Install Plans</strong> tabs for any failure or errors
under <strong>Status</strong>.</p>
</li>
<li>
<p>Switch to the <strong>Workloads</strong>  <strong>Pods</strong> page and check the logs in any Pods in the
<code>openshift-operators</code> project that are reporting issues.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create the descheduler instance:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Click <strong>Administration</strong> &#8594; <strong>CRD</strong>.</p>
</li>
<li>
<p>On the <strong>Custom Resource Definitions</strong> page, click <strong>Descheduler</strong>.</p>
</li>
<li>
<p>On the <strong>Deschedulers</strong> page, click <strong>Create Descheduler</strong>.</p>
</li>
<li>
<p>Specify a name and enter the <strong>openshift-operators</strong> namespace.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: descheduler.io/v1alpha1
kind: Descheduler
metadata:
  name: example <b class="conum">(1)</b>
  namespace: default <b class="conum">(2)</b>
spec: {}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify a name for the descheduler.</p>
</li>
<li>
<p>Specify <code>openshift-operators</code> as the namespace.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: descheduler.io/v1alpha1
kind: Descheduler
metadata:
  name: descheduler
  namespace: openshift-operators
spec: {}</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Click <strong>Create</strong>. This creates the descheduler Custom Resource, which you
can edit to make changes to your cluster logging cluster.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-descheduling-policy-nodes-scheduler-descheduler">Understanding descheduler policies</h4>
<div class="paragraph">
<p>The Descheduler Operator creates a descheduler custom resource which you can use to specify the descheduling policy to implement.</p>
</div>
<div class="paragraph">
<p>There are four default policies that you can use.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Move pods to underutilized nodes</dt>
<dd>
<p>The <code>lownodeutilization</code> strategy finds nodes that are underutilized and evicts pods from other nodes so that the evicted pods can be scheduled on these underutilized nodes.</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="title">Sample <code>lownodeutilizaton</code> policy</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  strategies:
    - name: lownodeutilization
      params:
       - name: cputhreshold <b class="conum">(1)</b>
         value: 10
       - name: memorythreshold
         value: 20
       - name: podsthreshold
         value: 30
       - name: memorytargetthreshold <b class="conum">(2)</b>
         value: 40
       - name: cputargetthreshold
         value: 50
       - name: podstargetthreshold
         value: 60
       - name: nodes
         value: 3  <b class="conum">(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Set the low-end thresholds for the <code>LowNodeUtilization</code> strategy. If the node is below all three values, the descheduler considers the node underutilized.</p>
</li>
<li>
<p>Set the high-end thresholds for the <code>LowNodeUtilization</code> strategy. If the node is below these values and above the <code>threshold</code> values, the descheduler considers the node  properly utilized.</p>
</li>
<li>
<p>Set the number of nodes that can be underutilized before the descheduler will evict pods from underutilized nodes.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The underutilization of nodes is determined by a configurable threshold, <code>thresholds</code>, for CPU, memory, or number of pods (based on percentage). If a node usage is below all these thresholds, the node is considered underutilized and the descheduler can evict pods from other nodes. Pods request resource requirements are considered when computing node resource utilization.</p>
</div>
<div class="paragraph">
<p>A high threshold value, <code>targetThresholds</code> is used to determine properly utilized nodes. Any node that is between the <em>thresholds</em> and <em>targetThresholds</em> is considered properly utilized and is not considered for eviction. The threshold, <code>targetThresholds</code>, can be configured for CPU, memory, and number of pods (based on percentage).</p>
</div>
<div class="paragraph">
<p>These thresholds could be tuned for your cluster requirements.</p>
</div>
<div class="paragraph">
<p>The <code>numberOfNodes</code> parameter can be configured to activate the strategy only when number of underutilized nodes is above the configured value. Set this parameter if it is acceptable for a few nodes to go underutilized. By default, <code>numberOfNodes</code> is set to zero.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Remove Duplicate Pods</dt>
<dd>
<p>The <code>duplicates</code> strategy ensures that there is only one pod associated with a ReplicaSet, Replication Controller, DeploymentConfig, or Job running on same node.
If there are other pods associated with those objects, the duplicate pods are evicted. Removing duplicate pods results in better spreading of pods in a cluster.</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="title">Sample <code>duplicates</code> policy</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  strategies:
    - name: duplicates
      params: null</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, duplicate pods could happen if a node fails and the pods on the node are moved to another node, leading to more than one pod associated with an ReplicaSet or Replication Controller, running on same node. After the failed node is ready again, this strategy could be used to evict those duplicate pods.</p>
</div>
<div class="paragraph">
<p>There are no parameters associated with this strategy.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Remove Pods Violating Inter-Pod Anti-Affinity</dt>
<dd>
<p>The <code>interpodantiaffinity</code> strategy ensures that pods violating inter-pod anti-affinity are removed from nodes.</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="title">Sample <code>interpodantiaffinity</code> policy</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  strategies:
    - name: interpodantiaffinity
      params: null</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, <strong>Node1</strong> has <strong>podA</strong>, <strong>podB</strong>, and <strong>podC</strong>. <strong>podB</strong> and <strong>podC</strong> have anti-affinity rules that prohibit them from running on the same node as <strong>podA</strong>. <strong>podA</strong> will be evicted from the node so that <strong>podB</strong> and <strong>podC</strong> can run on that node. This situation could happen if the anti-affinity rule was applied when <strong>podB</strong> and <strong>podC</strong> were running on the node.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Remove Pods Violating Node Affinity</dt>
<dd>
<p>The <code>removepodsviolatingnodeaffinity</code> strategy ensures that pods violating node affinity are removed from nodes.</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="title">Sample <code>removepodsviolatingnodeaffinity</code> policy</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  strategies:
    - name: removepodsviolatingnodeaffinity
      params: null</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, <strong>nodeA</strong> has <strong>podA</strong> which satisfied the node affinity rules at the time of scheduling. If over time <strong>nodeA</strong> no longer satisfies the rule and <strong>nodeB</strong> is available that satisfies the node affinity rule, <strong>podA</strong> is be evicted from <strong>nodeA</strong>.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-scheduler-descheduler-m-nodes-scheduler-descheduler">Modifying the Descheduler policy</h4>
<div class="paragraph">
<p>You can change the descheduler policy by editing the descheduler custom resource (CR).</p>
</div>
<div class="ulist">
<div class="title">Prerequisite</div>
<ul>
<li>
<p>The Descheduler Operator must be installed.</p>
</li>
<li>
<p>Have the name of the descheduler CR:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get descheduler

NAME          AGE
descheduler   6m6s</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To modify the descheduler policy:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open the descheduler CR for editing.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc edit Descheduler &lt;cr-name&gt; -n openshift-operators</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc edit Descheduler descheduler -n openshift-operators</pre>
</div>
</div>
</li>
<li>
<p>Change the parameters and values as needed:</p>
<div class="listingblock">
<div class="title">Sample <code>lownodeutilizaton</code> policy</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  strategies:
    - name: lownodeutilization <b class="conum">(1)</b>
      params:
       - name: cputhreshold <b class="conum">(2)</b>
         value: 10
       - name: memorythreshold
         value: 20
       - name: podsthreshold
         value: 30
       - name: memorytargetthreshold <b class="conum">(3)</b>
         value: 40
       - name: cputargetthreshold
         value: 50
       - name: podstargetthreshold
         value: 60
       - name: nodes
         value: 3  <b class="conum">(4)</b>
  - name: duplicates <b class="conum">(1)</b>
  - name: interpodantiaffinity <b class="conum">(1)</b>
  - name: removepodsviolatingnodeaffinity <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify the strategy as described in <strong>Understanding descheduler policies</strong>.</p>
<div class="ulist">
<ul>
<li>
<p><code>lownodeutilization</code></p>
</li>
<li>
<p><code>duplicates</code></p>
</li>
<li>
<p><code>removepodsviolatingnodeaffinity</code></p>
</li>
<li>
<p><code>interpodantiaffinity</code></p>
</li>
</ul>
</div>
</li>
<li>
<p>For the <code>lownodeutilization</code> policy, optionally, set the low-end thresholds for the <code>LowNodeUtilization</code> strategy. If the node is below all three values, the descheduler considers the node underutilized.</p>
</li>
<li>
<p>For the <code>lownodeutilization</code> policy, optionally, set the high-end thresholds for the <code>LowNodeUtilization</code> strategy. If the node is below these values and above the <code>threshold</code> values, the descheduler considers the node properly utilized.</p>
</li>
<li>
<p>For the <code>lownodeutilization</code> policy, optionally, set the number of nodes that can be underutilized before the descheduler will evict pods from underutilized nodes.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using-jobs-and-daemonsets">Using Jobs and DaemonSets</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="running-background-tasks-on-nodes-automatically-with-daemonsets">Running background tasks on nodes automatically with daemonsets in OpenShift Enterprise</h3>
<div class="paragraph">
<p>As an administrator, you can create and use DaemonSets to run replicas of a pod on specific or all nodes in an OpenShift Enterprise cluster.</p>
</div>
<div class="paragraph">
<p>A DaemonSet ensures that all (or some) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to the cluster.
As nodes are removed from the cluster, those pods are removed through garbage collection. Deleting a DaemonSet will clean up the Pods it created.</p>
</div>
<div class="paragraph">
<p>You can use daemonsets to create shared storage, run a logging pod on every node in
your cluster, or deploy a monitoring agent on every node.</p>
</div>
<div class="paragraph">
<p>For security reasons, only cluster administrators can create daemonsets.</p>
</div>
<div class="paragraph">
<p>For more information on daemonsets, see the <a href="http://kubernetes.io/docs/admin/daemons/">Kubernetes documentation</a>.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>Daemonset scheduling is incompatible with project&#8217;s default node selector.
If you fail to disable it, the daemonset gets restricted by merging with the
default node selector. This results in frequent pod recreates on the nodes that
got unselected by the merged node selector, which in turn puts unwanted load on
the cluster.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="nodes-pods-daemonsets-creating-nodes-pods-daemonsets">Creating daemonsets</h4>
<div class="paragraph">
<p>When creating daemonsets, the <code><strong>nodeSelector</strong></code> field is used to indicate the
nodes on which the daemonset should deploy replicas.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Before you start using daemonsets, disable the default project-wide node selector
in your namespace, by setting the namespace annotation <code>openshift.io/node-selector</code> to an empty string:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc patch namespace myproject -p \
    '{"metadata": {"annotations": {"openshift.io/node-selector": ""}}}'</pre>
</div>
</div>
</li>
<li>
<p>If you are creating a new project, overwrite the default node selector using
<code>oc adm new-project &lt;name&gt; --node-selector=""</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To create a daemonset:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Define the daemonset yaml file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: hello-daemonset
spec:
  selector:
      matchLabels:
        name: hello-daemonset <b class="conum">(1)</b>
  template:
    metadata:
      labels:
        name: hello-daemonset <b class="conum">(2)</b>
    spec:
      nodeSelector: <b class="conum">(3)</b>
        role: worker
      containers:
      - image: openshift/hello-openshift
        imagePullPolicy: Always
        name: registry
        ports:
        - containerPort: 80
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
      serviceAccount: default
      terminationGracePeriodSeconds: 10</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The label selector that determines which pods belong to the daemonset.</p>
</li>
<li>
<p>The pod template&#8217;s label selector. Must match the label selector above.</p>
</li>
<li>
<p>The node selector that determines on which nodes pod replicas should be deployed.
A matching label must be present on the node.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the daemonset object:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f daemonset.yaml</pre>
</div>
</div>
</li>
<li>
<p>To verify that the pods were created, and that each node has a pod replica:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Find the daemonset pods:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pods
hello-daemonset-cx6md   1/1       Running   0          2m
hello-daemonset-e3md9   1/1       Running   0          2m</pre>
</div>
</div>
</li>
<li>
<p>View the pods to verify the pod has been placed onto the node:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe pod/hello-daemonset-cx6md|grep Node
Node:        openshift-node01.hostname.com/10.14.20.134
$ oc describe pod/hello-daemonset-e3md9|grep Node
Node:        openshift-node02.hostname.com/10.14.20.137</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>If you update a daemonset&#8217;s pod template, the existing pod
replicas are not affected.</p>
</li>
<li>
<p>If you delete a daemonSet and then create a new daemonset
with a different template but the same label selector, it recognizes any
existing pod replicas as having matching labels and thus does not update them or
create new replicas despite a mismatch in the pod template.</p>
</li>
<li>
<p>If you change node labels, the daemonset adds pods to nodes that match the new labels and deletes pods
from nodes that do not match the new labels.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To update a daemonset, force new pod replicas to be created by deleting the old
replicas or nodes.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="running-tasks-in-pods-using-jobs">Running tasks in pods using jobs in OpenShift Enterprise</h3>
<div class="paragraph">
<p>A <em>job</em> executes a task in your OpenShift Enterprise cluster.</p>
</div>
<div class="paragraph">
<p>A job tracks the overall progress of a task and updates its status with information
about active, succeeded, and failed pods. Deleting a job will clean up any pod
replicas it created. Jobs are part of the Kubernetes API, which can be managed
with <code>oc</code> commands like other object types.</p>
</div>
<div class="paragraph">
<p>See the <a href="http://kubernetes.io/docs/user-guide/jobs/">Kubernetes documentation</a> for
more information about jobs.</p>
</div>
<div class="sect3">
<h4 id="nodes-nodes-jobs-about-nodes-nodes-jobs">Understanding jobs and CronJobs in OpenShift Enterprise</h4>
<div class="paragraph">
<p>A job tracks the overall progress of a task and updates its status with information
about active, succeeded, and failed pods. Deleting a job will clean up any pods it created.
Jobs are part of the Kubernetes API, which can be managed
with <code>oc</code> commands like other object types.</p>
</div>
<div class="paragraph">
<p>There are two possible resource types that allow creating run-once objects in OpenShift Enterprise:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Job</dt>
<dd>
<p>A regular job is a run-once object that creates a task and ensures the job finishes.</p>
</dd>
<dt class="hdlist1">CronJob</dt>
<dd>
<p>A CronJob can be scheduled to run multiple times, use a CronJob.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>A <em>CronJob</em> builds on a regular job by allowing you to specify
how the job should be run. CronJobs are part of the
<a href="http://kubernetes.io/docs/user-guide/cron-jobs">Kubernetes</a> API, which
can be managed with <code>oc</code> commands like other object types.</p>
</div>
<div class="paragraph">
<p>CronJobs are useful for creating periodic and recurring tasks, like running backups or sending emails.
CronJobs can also schedule individual tasks for a specific time, such as if you want to schedule a job for a low activity period.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>A CronJob creates a job object approximately once per execution time of its
schedule, but there are circumstances in which it fails to create a job or
two jobs might be created.  Therefore, jobs must be idempotent and you must
configure history limits.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="jobs-create-nodes-nodes-jobs">Understanding how to create jobs</h4>
<div class="paragraph">
<p>Both resource types require a job configuration that consists of the following key parts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A pod template, which describes the pod that OpenShift Enterprise creates.</p>
</li>
<li>
<p>An optional <code>parallelism</code> parameter, which specifies how many pods running in parallel at any point in time should execute a job. If not specified, this defaults to
the value in the <code>completions</code> parameter.</p>
</li>
<li>
<p>An optional <code>completions</code> parameter, specifying how many successful pod completions are needed to finish a job. If not specified, this value defaults to one.</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="jobs-set-max-nodes-nodes-jobs">Understanding how to set a maximum duration for jobs</h5>
<div class="paragraph">
<p>When defining a job, you can define its maximum duration by setting
the <code>activeDeadlineSeconds</code> field. It is specified in seconds and is not
set by default. When not set, there is no maximum duration enforced.</p>
</div>
<div class="paragraph">
<p>The maximum duration is counted from the time when a first pod gets scheduled in
the system, and defines how long a job can be active. It tracks overall time of
an execution. After reaching the specified timeout, the job is terminated by OpenShift Enterprise.</p>
</div>
</div>
<div class="sect4">
<h5 id="jobs-set-backoff-nodes-nodes-jobs">Understanding how to set a job back off policy for pod failure</h5>
<div class="paragraph">
<p>A Job can be considered failed, after a set amount of retries due to a
logical error in configuration or other similar reasons. Failed Pods associated with the Job are recreated by the controller with
an exponential back off delay (<code>10s</code>, <code>20s</code>, <code>40s</code> ) capped at six minutes. The
limit is reset if no new failed pods appear between controller checks.</p>
</div>
<div class="paragraph">
<p>Use the <code>spec.backoffLimit</code> parameter to set the number of retries for a job.</p>
</div>
</div>
<div class="sect4">
<h5 id="jobs-artifacts-nodes-nodes-jobs">Understanding how to configure a CronJob to remove artifacts</h5>
<div class="paragraph">
<p>CronJobs can leave behind artifact resources such as jobs or pods.  As a user it is important
to configure history limits so that old jobs and their pods are properly cleaned.  There are two fields within CronJob&#8217;s spec responsible for that:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>.spec.successfulJobsHistoryLimit</code>. The number of successful finished jobs to retain (defaults to 3).</p>
</li>
<li>
<p><code>.spec.failedJobsHistoryLimit</code>. The number of failed finished jobs to retain (defaults to 1).</p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>Delete CronJobs that you no longer need:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc delete cronjob/&lt;cron_job_name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Doing this prevents them from generating unnecessary artifacts.</p>
</div>
</li>
<li>
<p>You can suspend further executions by setting the <code>spec.suspend</code> to true.  All subsequent executions are suspended until you reset to <code>false</code>.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="jobs-limits-nodes-nodes-jobs">Known limitations</h4>
<div class="paragraph">
<p>The job specification restart policy only applies to the <em>pods</em>, and not the <em>job controller</em>. However, the job controller is hard-coded to keep retrying jobs to completion.</p>
</div>
<div class="paragraph">
<p>As such, <code>restartPolicy: Never</code> or <code>--restart=Never</code> results in the same behavior as <code>restartPolicy: OnFailure</code> or <code>--restart=OnFailure</code>. That is, when a job fails it is restarted automatically until it succeeds (or is manually discarded). The policy only sets which subsystem performs the restart.</p>
</div>
<div class="paragraph">
<p>With the <code>Never</code> policy, the <em>job controller</em> performs the restart. With each attempt, the job controller increments the number of failures in the job status and create new pods. This means that with each failed attempt, the number of pods increases.</p>
</div>
<div class="paragraph">
<p>With the <code>OnFailure</code> policy, <em>kubelet</em> performs the restart. Each attempt does not increment the number of failures in the job status. In addition, kubelet will retry failed jobs starting pods on the same nodes.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-jobs-creating-nodes-nodes-jobs">Creating jobs</h4>
<div class="paragraph">
<p>You create a job in OpenShift Enterprise by creating a job object.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To create a job:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a YAML file similar to the following:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  parallelism: 1    <b class="conum">(1)</b>
  completions: 1    <b class="conum">(2)</b>
  activeDeadlineSeconds: 1800 <b class="conum">(3)</b>
  backoffLimit: 6   <b class="conum">(4)</b>
  template:         <b class="conum">(5)</b>
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: OnFailure    <b class="conum">(6)</b></code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Optional value for how many pod replicas a job should run in parallel; defaults to <code>completions</code>.</p>
</li>
<li>
<p>Optional value for how many successful pod completions are needed to mark a job completed; defaults to one.</p>
</li>
<li>
<p>Optional value for the maximum duration the job can run.</p>
</li>
<li>
<p>Option value to set the number of retries for a job. This field defaults to six.</p>
</li>
<li>
<p>Template for the pod the controller creates.</p>
</li>
<li>
<p>The restart policy of the pod. This does not apply to the job controller. See <strong>Known Issues</strong> for details.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the job:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>You can also create and launch a job from a single command using <code>oc run</code>. The following command creates and launches the same job as specified in the previous example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc run pi --image=perl --replicas=1  --restart=OnFailure \
    --command -- perl -Mbignum=bpi -wle 'print bpi(2000)'</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-jobs-creating-cron-nodes-nodes-jobs">Creating CronJobs</h4>
<div class="paragraph">
<p>You create a CronJob in OpenShift Enterprise by creating a job object.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To create a CronJob:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a YAML file similar to the following:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: pi
spec:
  schedule: "*/1 * * * *"  <b class="conum">(1)</b>
  concurrencyPolicy: "Replace" <b class="conum">(2)</b>
  startingDeadlineSeconds: 200 <b class="conum">(3)</b>
  suspend: true            <b class="conum">(4)</b>
  successfulJobsHistoryLimit: 3 <b class="conum">(5)</b>
  failedJobsHistoryLimit: 1     <b class="conum">(6)</b>
  jobTemplate:             <b class="conum">(7)</b>
    spec:
      template:
        metadata:
          labels:          <b class="conum">(8)</b>
            parent: "cronjobpi"
        spec:
          containers:
          - name: pi
            image: perl
            command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
          restartPolicy: OnFailure <b class="conum">(9)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Schedule for the job specified in <a href="https://en.wikipedia.org/wiki/Cron">cron format</a>. In this example, the job will run every minute.</p>
</li>
<li>
<p>An optional concurrency policy, specifying how to treat concurrent jobs within a CronJob. Only one of the following concurrent policies may be specified. If not specified, this defaults to allowing concurrent executions.</p>
<div class="ulist">
<ul>
<li>
<p><code>Allow</code> allows CronJobs to run concurrently.</p>
</li>
<li>
<p><code>Forbid</code> forbids concurrent runs, skipping the next run if the previous has not
finished yet.</p>
</li>
<li>
<p><code>Replace</code> cancels the currently running job and replaces
it with a new one.</p>
</li>
</ul>
</div>
</li>
<li>
<p>An optional deadline (in seconds) for starting the job if it misses its
scheduled time for any reason. Missed jobs executions will be counted as failed
ones. If not specified, there is no deadline.</p>
</li>
<li>
<p>An optional flag allowing the suspension of a CronJob. If set to <code>true</code>,
all subsequent executions will be suspended.</p>
</li>
<li>
<p>The number of successful finished jobs to retain (defaults to 3).</p>
</li>
<li>
<p>The number of failed finished jobs to retain (defaults to 1).</p>
</li>
<li>
<p>Job template. This is similar to the job example.</p>
</li>
<li>
<p>Sets a label for jobs spawned by this CronJob.</p>
</li>
<li>
<p>The restart policy of the pod. This does not apply to the job controller. See Known Issues and Limitations for details.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>.spec.successfulJobsHistoryLimit</code> and <code>.spec.failedJobsHistoryLimit</code> fields are optional.
These fields specify how many completed and failed jobs should be kept.  By default, they are
set to <code>3</code> and <code>1</code> respectively.  Setting a limit to <code>0</code> corresponds to keeping none of the corresponding
kind of jobs after they finish.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create the CronJob:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>You can also create and launch a CronJob from a single command using <code>oc run</code>. The following command creates and launches the same CronJob as specified in the previous example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc run pi --image=perl --schedule='*/1 * * * *' \
    --restart=OnFailure --labels parent="cronjobpi" \
    --command -- perl -Mbignum=bpi -wle 'print bpi(2000)'</pre>
</div>
</div>
<div class="paragraph">
<p>With <code>oc run</code>, the <code>--schedule</code> option accepts schedules in <a href="https://en.wikipedia.org/wiki/Cron">cron format</a>.</p>
</div>
<div class="paragraph">
<p>When creating a CronJob,  <code>oc run</code> only supports the <code>Never</code> or <code>OnFailure</code> restart policies (<code>--restart</code>).</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_working-with-nodes">Working with nodes</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="viewing-and-listing-the-nodes-in-your-cluster">Viewing and listing the nodes in your OpenShift Enterprise cluster</h3>
<div class="paragraph">
<p>You can list all the nodes in your cluster to obtain information such as status, age, memory usage, and details about the nodes.</p>
</div>
<div class="paragraph">
<p>When you perform node management operations, the CLI interacts with node objects that are representations of actual node hosts.
The master uses the information from node objects to validate nodes with health checks.</p>
</div>
<div class="sect3">
<h4 id="nodes-nodes-viewing-listing-nodes-nodes-viewing">About listing all the nodes in a cluster</h4>
<div class="paragraph">
<p>You can get detailed information on the nodes in the cluster.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The following command lists all nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get nodes

NAME                   STATUS    ROLES     AGE       VERSION
master.example.com     Ready     master    7h        v1.12.4+91d94b671b
node1.example.com      Ready     worker    7h        v1.12.4+91d94b671b
node2.example.com      Ready     worker    7h        v1.12.4+91d94b671b</pre>
</div>
</div>
</li>
<li>
<p>The <code>-wide</code> option provides provides additional information on all nodes.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get nodes -o wide</pre>
</div>
</div>
</li>
<li>
<p>The following command lists information about a single node:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get node &lt;node&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>STATUS</code> column in the output of these commands can show nodes with the
following conditions:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Node Conditions <a id="node-conditions"></a></caption>
<colgroup>
<col style="width: 27.2727%;">
<col style="width: 72.7273%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Condition</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>Ready</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>The node reports its own readiness to the apiserver by returning <code>True</code>.</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>NotReady</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>One of the underlying components, such as the container runtime or network, is experiencing issues or is not yet configured.</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>SchedulingDisabled</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Pods cannot be scheduled for placement on the node.</p>
</div></div></td>
</tr>
</tbody>
</table>
</li>
<li>
<p>The following command provides more detailed information about a specific node, including the reason for
the current condition:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe node &lt;node&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe node node1.example.com

Name:               node1.example.com <b class="conum">(1)</b>
Roles:              worker <b class="conum">(2)</b>
Labels:             beta.kubernetes.io/arch=amd64   <b class="conum">(3)</b>
                    beta.kubernetes.io/instance-type=m4.large
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=us-east-2
                    failure-domain.beta.kubernetes.io/zone=us-east-2a
                    kubernetes.io/hostname=ip-10-0-140-16
                    node-role.kubernetes.io/worker=
Annotations:        cluster.k8s.io/machine: openshift-machine-api/ahardin-worker-us-east-2a-q5dzc  <b class="conum">(4)</b>
                    machineconfiguration.openshift.io/currentConfig: worker-309c228e8b3a92e2235edd544c62fea8
                    machineconfiguration.openshift.io/desiredConfig: worker-309c228e8b3a92e2235edd544c62fea8
                    machineconfiguration.openshift.io/state: Done
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 13 Feb 2019 11:05:57 -0500
Taints:             &lt;none&gt;  <b class="conum">(5)</b>
Unschedulable:      false
Conditions:                 <b class="conum">(6)</b>
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:05:57 -0500   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:05:57 -0500   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:05:57 -0500   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:05:57 -0500   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:07:09 -0500   KubeletReady                 kubelet is posting ready status
Addresses:   <b class="conum">(7)</b>
  InternalIP:   10.0.140.16
  InternalDNS:  ip-10-0-140-16.us-east-2.compute.internal
  Hostname:     ip-10-0-140-16.us-east-2.compute.internal
Capacity:    <b class="conum">(8)</b>
 attachable-volumes-aws-ebs:  39
 cpu:                         2
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      8172516Ki
 pods:                        250
Allocatable:
 attachable-volumes-aws-ebs:  39
 cpu:                         1500m
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      7558116Ki
 pods:                        250
System Info:    <b class="conum">(9)</b>
 Machine ID:                              63787c9534c24fde9a0cde35c13f1f66
 System UUID:                             EC22BF97-A006-4A58-6AF8-0A38DEEA122A
 Boot ID:                                 f24ad37d-2594-46b4-8830-7f7555918325
 Kernel Version:                          3.10.0-957.5.1.el7.x86_64
 OS Image:                                Red Hat Enterprise Linux CoreOS 4.0
 Operating System:                        linux
 Architecture:                            amd64
 Container Runtime Version:               cri-o://1.12.5-2.rhaos4.0.gitd4191df.el7-dev
 Kubelet Version:                         v1.12.4+91d94b671b
 Kube-Proxy Version:                      v1.12.4+91d94b671b
PodCIDR:                                  10.128.4.0/24
ProviderID:                               aws:///us-east-2a/i-04e87b31dc6b3e171
Non-terminated Pods:                      (13 in total)  <b class="conum">(10)</b>
  Namespace                               Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                               ----                                   ------------  ----------  ---------------  -------------
  openshift-cluster-node-tuning-operator  tuned-hdl5q                            0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-dns                           dns-default-l69zr                      0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-image-registry                node-ca-9hmcg                          0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-ingress                       router-default-76455c45c-c5ptv         0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-machine-config-operator       machine-config-daemon-cvqw9            20m (1%)      0 (0%)      50Mi (0%)        0 (0%)
  openshift-marketplace                   community-operators-f67fh              0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-monitoring                    alertmanager-main-0                    50m (3%)      50m (3%)    210Mi (2%)       10Mi (0%)
  openshift-monitoring                    grafana-78765ddcc7-hnjmm               100m (6%)     200m (13%)  100Mi (1%)       200Mi (2%)
  openshift-monitoring                    node-exporter-l7q8d                    10m (0%)      20m (1%)    20Mi (0%)        40Mi (0%)
  openshift-monitoring                    prometheus-adapter-75d769c874-hvb85    0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-multus                        multus-kw8w5                           0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-sdn                           ovs-t4dsn                              100m (6%)     0 (0%)      300Mi (4%)       0 (0%)
  openshift-sdn                           sdn-g79hg                              100m (6%)     0 (0%)      200Mi (2%)       0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                    Requests     Limits
  --------                    --------     ------
  cpu                         380m (25%)   270m (18%)
  memory                      880Mi (11%)  250Mi (3%)
  attachable-volumes-aws-ebs  0            0
Events:     <b class="conum">(11)</b>
  Type     Reason                   Age                From                      Message
  ----     ------                   ----               ----                      -------
  Normal   NodeHasSufficientPID     6d (x5 over 6d)    kubelet, m01.example.com  Node m01.example.com status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  6d                 kubelet, m01.example.com  Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  6d (x6 over 6d)    kubelet, m01.example.com  Node m01.example.com status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    6d (x6 over 6d)    kubelet, m01.example.com  Node m01.example.com status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientDisk    6d (x6 over 6d)    kubelet, m01.example.com  Node m01.example.com status is now: NodeHasSufficientDisk
  Normal   NodeHasSufficientPID     6d                 kubelet, m01.example.com  Node m01.example.com status is now: NodeHasSufficientPID
  Normal   Starting                 6d                 kubelet, m01.example.com  Starting kubelet.
 ...</pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The name of the node.</p>
</li>
<li>
<p>The role of the node, either <code>master</code> or <code>compute</code>.</p>
</li>
<li>
<p>The labels applied to the node.</p>
</li>
<li>
<p>The annotations applied to the node.</p>
</li>
<li>
<p>The taints applied to the node.</p>
</li>
<li>
<p>Node conditions.</p>
</li>
<li>
<p>The IP address and host name of the node.</p>
</li>
<li>
<p>The pod resources and allocatable resources.</p>
</li>
<li>
<p>Information about the node host.</p>
</li>
<li>
<p>The pods on the node.</p>
</li>
<li>
<p>The events reported by the node.</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-viewing-listing-pods-nodes-nodes-viewing">Listing pods on a node in your cluster</h4>
<div class="paragraph">
<p>You can list all the pods on a specific node.</p>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>To list all or selected pods on one or more nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe node &lt;node1&gt; &lt;node2&gt; --list-pods [--pod-selector=&lt;pod_selector&gt;] [-o json|yaml]</pre>
</div>
</div>
</li>
<li>
<p>To list all or selected pods on selected nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe --selector=&lt;node_selector&gt; --list-pods [--pod-selector=&lt;pod_selector&gt;] [-o json|yaml]</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-viewing-memory-nodes-nodes-viewing">Viewing memory and CPU usage statistics on your nodes</h4>
<div class="paragraph">
<p>You can display usage statistics about nodes, which provide the runtime
environments for containers. These usage statistics include CPU, memory, and
storage consumption.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You must have <code>cluster-reader</code> permission to view the usage statistics.</p>
</li>
<li>
<p>Metrics must be installed to view the usage statistics.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>To view the usage statistics:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm top nodes

NAME                                   CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
ip-10-0-12-143.ec2.compute.internal    1503m        100%      4533Mi          61%
ip-10-0-132-16.ec2.compute.internal    76m          5%        1391Mi          18%
ip-10-0-140-137.ec2.compute.internal   398m         26%       2473Mi          33%
ip-10-0-142-44.ec2.compute.internal    656m         43%       6119Mi          82%
ip-10-0-146-165.ec2.compute.internal   188m         12%       3367Mi          45%
ip-10-0-19-62.ec2.compute.internal     896m         59%       5754Mi          77%
ip-10-0-44-193.ec2.compute.internal    632m         42%       5349Mi          72%</pre>
</div>
</div>
</li>
<li>
<p>To view the usage statistics for nodes with labels:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm top node --selector=''</pre>
</div>
</div>
<div class="paragraph">
<p>You must choose the selector (label query) to filter on. Supports <code>=</code>, <code>==</code>, and <code>!=</code>.</p>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="working-with-nodes">Working with nodes in OpenShift Enterprise</h3>
<div class="paragraph">
<p>As an administrator, you can perform a number of tasks to make your clusters more efficient.</p>
</div>
<div class="sect3">
<h4 id="nodes-nodes-working-evacuating-nodes-nodes-working">Understanding how to evacuate pods on nodes</h4>
<div class="paragraph">
<p>Evacuating pods allows you to migrate all or selected pods from a given node or
nodes.</p>
</div>
<div class="paragraph">
<p>You can only evacuate pods backed by a replication controller. The replication controllers create new pods on
other nodes and removes the existing pods from the specified node(s).</p>
</div>
<div class="paragraph">
<p>Bare pods, meaning those not backed by a replication controller, are unaffected by default.
You can evacuate a subset of pods by specifying a pod-selector. Pod selectors are
based on labels, so all the pods with the specified label will be evacuated.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Nodes must first be marked unschedulable to perform pod evacuation.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc cordon &lt;node1&gt;
NAME        STATUS                        ROLES     AGE       VERSION
&lt;node1&gt;     NotReady,SchedulingDisabled   worker   1d        v1.12.0+d4cacc0</pre>
</div>
</div>
<div class="paragraph">
<p>Use <code>oc uncordon</code> to mark the node as schedulable when done.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc cordon &lt;node1&gt;</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>The following command evacuates all or selected pods on one or more nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm drain &lt;node1&gt; &lt;node2&gt; [--pod-selector=&lt;pod_selector&gt;]</pre>
</div>
</div>
</li>
<li>
<p>The following command forces deletion of bare pods using the <code>--force</code> option. When set to
<code>true</code>, deletion continues even if there are pods not managed by a replication
controller, ReplicaSet, job, daemonset, or StatefulSet:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --force=true</pre>
</div>
</div>
</li>
<li>
<p>The following command sets a period  of time in seconds for each pod to
terminate gracefully, use <code>--grace-period</code>. If negative, the default value specified in the pod will
be used:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --grace-period=-1</pre>
</div>
</div>
</li>
<li>
<p>The following command ignores DaemonSet-managed pods using the <code>--ignore-daemonsets</code> flag set to <code>true</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --ignore-daemonsets=true</pre>
</div>
</div>
</li>
<li>
<p>The following command sets the length of time to wait before giving up using the <code>--timeout</code> flag. A
value of <code>0</code> sets an infinite length of time:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --timeout=5s</pre>
</div>
</div>
</li>
<li>
<p>The following command deletes pods even if there are pods using emptyDir using the <code>--delete-local-data</code> flag set to <code>true</code>. Local data is deleted when the node
is drained:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --delete-local-data=true</pre>
</div>
</div>
</li>
<li>
<p>The following command lists objects that will be migrated without actually performing the evacuation,
using the <code>--dry-run</code> option set to <code>true</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm drain &lt;node1&gt; &lt;node2&gt;  --dry-run=true</pre>
</div>
</div>
<div class="paragraph">
<p>Instead of specifying specific node names (for example, <code>&lt;node1&gt; &lt;node2&gt;</code>), you
can use the <code>--selector=&lt;node_selector&gt;</code> option to evacuate pods on selected
nodes.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-working-updating-nodes-nodes-working">Understanding how to update labels on nodes</h4>
<div class="paragraph">
<p>You can update any label on a node.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The following command adds or updates labels on a node:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label node &lt;node&gt; &lt;key_1&gt;=&lt;value_1&gt; ... &lt;key_n&gt;=&lt;value_n&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label nodes webconsole-7f7f6 unhealthy=true</pre>
</div>
</div>
</li>
<li>
<p>The following command updates all pods in the namespace:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label pods --all &lt;key_1&gt;=&lt;value_1&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label pods --all status=unhealthy</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-working-marking-nodes-nodes-working">Understanding how to marking nodes as unschedulable or schedulable</h4>
<div class="paragraph">
<p>By default, healthy nodes with a <code>Ready</code> status are
marked as schedulable, meaning that new pods are allowed for placement on the
node. Manually marking a node as unschedulable blocks any new pods from being
scheduled on the node. Existing pods on the node are not affected.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The following command marks a node or nodes as unschedulable:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm cordon &lt;node&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc cordon node1.example.com
node/node1.example.com cordoned

NAME                 LABELS                                        STATUS
node1.example.com    kubernetes.io/hostname=node1.example.com      Ready,SchedulingDisabled</pre>
</div>
</div>
</li>
<li>
<p>The following command marks a currently unschedulable node or nodes as schedulable:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc uncordon &lt;node1&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Alternatively, instead of specifying specific node names (for example, <code>&lt;node&gt;</code>), you can use the <code>--selector=&lt;node_selector&gt;</code> option to mark selected
nodes as schedulable or unschedulable.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-working-deleting-nodes-nodes-working">Deleting nodes from a cluster</h4>
<div class="paragraph">
<p>When you delete a node using the CLI, the node object is deleted in Kubernetes,
but the pods that exist on the node are not deleted. Any bare pods not
backed by a replication controller become inaccessible to OpenShift Enterprise.
Pods backed by replication controllers are rescheduled to other available
nodes. You need to delete local manifest pods.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To delete a node from the OpenShift Enterprise cluster edit the appropriate MachineSet:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>View the MachineSets that are in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get machinesets -n openshift-machine-api</pre>
</div>
</div>
<div class="paragraph">
<p>The MachineSets are listed in the form of &lt;clusterid&gt;-worker-&lt;aws-region-az&gt;.</p>
</div>
</li>
<li>
<p>Scale the MachineSet:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>For more information on scaling your cluster using a MachineSet, see Manually scaling a MachineSet.</p>
</div>
</div>
<div class="sect3">
<h4 id="_additional-resources">Additional resources</h4>
<div class="paragraph">
<p>For more information on scaling your cluster using a MachineSet,
see <a href="https://access.redhat.com/documentation/en-us/openshift_enterprise/4.0/html-single/machine_management/#machineset-manually-scaling-manually-scaling-machineset">Manually scaling a MachineSet</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="managing-nodes">Managing Nodes</h3>
<div class="paragraph">
<p>OpenShift Enterprise uses a KubeletConfig Custom Resource to manage the
configuration of nodes. By creating an instance of a KubeletConfig, a managed
MachineConfig is created to override setting on the node.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Logging into remote machines for the purpose of changing their configuration is not supported.</strong></p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="nodes-nodes-managing-about-nodes-nodes-jobs">Modifying Nodes</h4>
<div class="paragraph">
<p>To make configuration changes to a cluster, or MachinePool, you need to create a Custom Resource Definition, or KubeletConfig instance. OpenShift Enterprise uses the Machine Config Controller to watch for changes introduced through the CRD applies the changes to the cluster.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Obtain the label associated with the static CRD, Machine Config Pool, for the type of node you want to configure.
Perform one of the following steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Edit the machineconfigpool master, add label"custom-kubelet: small-pods"</p>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc edit machineconfigpool worker

metadata:
  creationTimestamp: 2019-01-31T07:10:04Z
  generation: 3
  labels:
    custom-kubelet: small-pods <b class="conum">(1)</b>
    operator.machineconfiguration.openshift.io/required-for-upgrade: ""</pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>If a label has been added it appears under <code>labels</code>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>If the label is not present, add a key/value pair under <code>labels</code>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create a Custom Resource (CR) for your configuration change.</p>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="title">Sample configuration for a <strong>max-pods</strong> KubeletConfig</div>
<div class="content">
<pre class="nowrap">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: set-max-pods <b class="conum">(1)</b>
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: small-pods <b class="conum">(2)</b>
  kubeletConfig: <b class="conum">(3)</b>
    maxPods: 100</pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Assign a name to CR.</p>
</li>
<li>
<p>Specify the label to apply the configuration change.</p>
</li>
<li>
<p>Specify the new value(s) you want to change.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the CR object.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f master-kube-config.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Most <a href="https://github.com/kubernetes/kubernetes/blob/release-1.11/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go#L45">KubeletConfig Options</a>  may be set by the user. The following options are not allowed to be overwritten:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>CgroupDriver</p>
</li>
<li>
<p>ClusterDNS</p>
</li>
<li>
<p>ClusterDomain</p>
</li>
<li>
<p>RuntimeRequestTimeout</p>
</li>
<li>
<p>StaticPodPath</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="managing-the-maximum-number-of-pods-per-node">Managing the maximum number of Pods per Node</h3>
<div class="paragraph">
<p>In OpenShift Enterprise, you can configure the number of pods that can run on a node based on the number of
processor cores on the node, a hard limit or both. If you use both options,
the lower of the two limits the number of pods on a node.</p>
</div>
<div class="paragraph">
<p>Exceeding these values can result in:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Increased CPU utilization by OpenShift Enterprise.</p>
</li>
<li>
<p>Slow pod scheduling.</p>
</li>
<li>
<p>Potential out-of-memory scenarios, depending on the amount of memory in the node.</p>
</li>
<li>
<p>Exhausting the IP address pool.</p>
</li>
<li>
<p>Resource overcommitting, leading to poor user application performance.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>A pod that is holding a single container actually uses two
containers. The second container sets up networking prior to the
actual container starting. As a result, a node running 10 pods actually
has 20 containers running.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>podsPerCore</code> parameter limits the number of pods the node can run based on the number of
processor cores on the node. For example, if <code>podsPerCore</code> is set to <code>10</code> on
a node with 4 processor cores, the maximum number of pods allowed on the node is 40.</p>
</div>
<div class="paragraph">
<p>The <code>maxPods</code> parameter limits the number of pods the node can run to a fixed value, regardless
of the properties of the node.</p>
</div>
<div class="sect3">
<h4 id="nodes-nodes-managing-max-pods-about-nodes-nodes-jobs">Configuring the maximum number of Pods per Node</h4>
<div class="paragraph">
<p>Two parameters control the maximum number of pods that can be scheduled to a node: <code>podsPerCore</code> and <code>maxPods</code>. If you use both options,
the lower of the two limits the number of pods on a node.</p>
</div>
<div class="paragraph">
<p>For example, if <code>podsPerCore</code> is set to <code>10</code> on a node with 4 processor cores, the maximum number of pods allowed on the node will be 40.</p>
</div>
<div class="olist arabic">
<div class="title">Prerequisite</div>
<ol class="arabic">
<li>
<p>Obtain the label associated with the static Machine Config Pool CRD for the type of node you want to configure.
Perform one of the following steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>View the Machine Config Pool:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe machineconfigpool &lt;name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe machineconfigpool worker

apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: 2019-02-08T14:52:39Z
  generation: 1
  labels:
    custom-kubelet: small-pods <b class="conum">(1)</b></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>If a label has been added it appears under <code>labels</code>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>If the label is not present, add a key/value pair:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a Custom Resource (CR) for your configuration change.</p>
<div class="listingblock">
<div class="title">Sample configuration for a <strong>max-pods</strong> CR</div>
<div class="content">
<pre class="nowrap">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods <b class="conum">(1)</b>
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: small-pods <b class="conum">(2)</b>
  kubeletConfig:
    podsPerCore: 100 <b class="conum">(3)</b>
    maxPods: 250 <b class="conum">(4)</b></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Assign a name to CR.</p>
</li>
<li>
<p>Specify the label to apply the configuration change.</p>
</li>
<li>
<p>Specify the number of pods the node can run based on the number of
processor cores on the node.</p>
</li>
<li>
<p>Specify the number of pods the node can run to a fixed value, regardless
of the properties of the node.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Setting <code>podsPerCore</code> to 0 disables this limit.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In the above example, the default value for <code>podsPerCore</code> is <code>10</code> and the
default value for <code>maxPods</code> is <code>250</code>. This means that unless the node has 25
cores or more, by default, <code>podsPerCore</code> will be the limiting factor.</p>
</div>
</li>
<li>
<p>List the Machine Config Pool CRDs to see if the change is applied. The <code>UPDATING</code> column reports <code>True</code> if the change is picked up by the Machine Config Controller:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get machineconfigpools
NAME     CONFIG                        UPDATED   UPDATING   DEGRADED
master   master-9cc2c72f205e103bb534   False     False      False
worker   worker-8cecd1236b33ee3f8a5e   False     True       False</pre>
</div>
</div>
<div class="paragraph">
<p>Once the change is complete, the <code>UPDATED</code> column reports <code>True</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get machineconfigpools
NAME     CONFIG                        UPDATED   UPDATING   DEGRADED
master   master-9cc2c72f205e103bb534   False     True       False
worker   worker-8cecd1236b33ee3f8a5e   True      False      False</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="using-the-node-tuning-operator">Using the Node Tuning Operator</h3>
<div class="paragraph">
<p>Learn about the Node Tuning Operator and how you can use it to manage node-level
tuning by orchestrating the tuned daemon.</p>
</div>
<div class="sect3">
<h4 id="about-node-tuning-operator-nodes-node-tuning-operator">About the Node Tuning Operator</h4>
<div class="paragraph">
<p>The Node Tuning Operator helps you manage node-level tuning by orchestrating the
tuned daemon. The majority of high-performance applications require some level of
kernel tuning. The Node Tuning Operator provides a unified management interface
to users of node-level sysctls and more flexibility to add custom tuning, which
is currently a Technology Preview feature, specified by user needs. The Operator
manages the containerized tuned daemon for OpenShift Enterprise as a Kubernetes
DaemonSet. It ensures the custom tuning specification is passed to all
containerized tuned daemons running in the cluster in the format that the
daemons understand. The daemons run on all nodes in the cluster, one per node.</p>
</div>
<div class="paragraph">
<p>The Node Tuning Operator is part of a standard OpenShift Enterprise installation in
version 4.0 and later.</p>
</div>
</div>
<div class="sect3">
<h4 id="accessing-an-example-node-tuning-operator-specification-nodes-node-tuning-operator">Accessing an example Node Tuning Operator specification</h4>
<div class="paragraph">
<p>Use this process to access an example Node Tuning Operator specification.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Run:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get Tuned/default -o yaml -n openshift-cluster-node-tuning-operator</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="custom-tuning-specification-nodes-node-tuning-operator">Custom tuning specification</h4>
<div class="paragraph">
<p>The custom resource (CR) for the operator has two major sections. The first
section, <code>profile:</code>, is a list of tuned profiles and their names. The second,
<code>recommend:</code>, defines the profile selection logic.</p>
</div>
<div class="paragraph">
<p>Multiple custom tuning specifications can co-exist as multiple CRs in the
operator&#8217;s namespace. The existence of new CRs or the deletion of old CRs is
detected by the Operator. All existing custom tuning specifications are merged
and appropriate objects for the containerized tuned daemons are updated.</p>
</div>
<div class="paragraph">
<p><strong>Profile data</strong></p>
</div>
<div class="paragraph">
<p>The <code>profile:</code> section lists tuned profiles and their names.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">profile:
- name: tuned_profile_1
  data: |
    # Tuned profile specification
    [main]
    summary=Description of tuned_profile_1 profile

    [sysctl]
    net.ipv4.ip_forward=1
    # ... other sysctl's or other tuned daemon plugins supported by the containerized tuned

# ...

- name: tuned_profile_n
  data: |
    # Tuned profile specification
    [main]
    summary=Description of tuned_profile_n profile

    # tuned_profile_n profile settings</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Recommended profiles</strong></p>
</div>
<div class="paragraph">
<p>The <code>profile:</code> selection logic is defined by the <code>recommend:</code> section of the CR:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">recommend:
- match:                              # optional; if omitted, profile match is assumed unless a profile with a higher matches first
  &lt;match&gt;                             # an optional array
  priority: &lt;priority&gt;                # profile ordering priority, lower numbers mean higher priority (0 is the highest priority)
  profile: &lt;tuned_profile_name&gt;       # e.g. tuned_profile_1

# ...

- match:
  &lt;match&gt;
  priority: &lt;priority&gt;
  profile: &lt;tuned_profile_name&gt;       # e.g. tuned_profile_n</pre>
</div>
</div>
<div class="paragraph">
<p>If <code>&lt;match&gt;</code> is omitted, a profile match (for example, <code>true</code>) is assumed.</p>
</div>
<div class="paragraph">
<p><code>&lt;match&gt;</code> is an optional array recursively defined as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">- label: &lt;label_name&gt;     # node or pod label name
  value: &lt;label_value&gt;    # optional node or pod label value; if omitted, the presence of &lt;label_name&gt; is enough to match
  type: &lt;label_type&gt;      # optional node or pod type ("node" or "pod"); if omitted, "node" is assumed
  &lt;match&gt;                 # an optional &lt;match&gt; array</pre>
</div>
</div>
<div class="paragraph">
<p>If <code>&lt;match&gt;</code> is not omitted, all nested <code>&lt;match&gt;</code> sections must also evaluate to
<code>true</code>. Otherwise, <code>false</code> is assumed and the profile with the respective
<code>&lt;match&gt;</code> section will not be applied or recommended. Therefore, the nesting
(child <code>&lt;match&gt;</code> sections) works as logical AND operator. Conversely, if any
item of the <code>&lt;match&gt;</code> array matches, the entire <code>&lt;match&gt;</code> array evaluates to
<code>true</code>. Therefore, the array acts as logical OR operator.</p>
</div>
<div class="listingblock">
<div class="title">Example</div>
<div class="content">
<pre class="nowrap">- match:
  - label: tuned.openshift.io/elasticsearch
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
    type: pod
  priority: 10
  profile: openshift-control-plane-es
- match:
  - label: node-role.kubernetes.io/master
  - label: node-role.kubernetes.io/infra
  priority: 20
  profile: openshift-control-plane
- priority: 30
  profile: openshift-node</pre>
</div>
</div>
<div class="paragraph">
<p>The CR above is translated for the containerized tuned daemon into its
<code>recommend.conf</code> file based on the profile priorities. The profile with the
highest priority (<code>10</code>) is <code>openshift-control-plane-es</code> and, therefore, it is
considered first. The containerized tuned daemon running on a given node looks
to see if there is a pod running on the same node with the
<code>tuned.openshift.io/elasticsearch</code> label set. If not, the entire <code>&lt;match&gt;</code>
section evaluates as <code>false</code>. If there is such a pod with the label, in order for
the <code>&lt;match&gt;</code> section to evaluate to <code>true</code>, the node label also needs to be
<code>node-role.kubernetes.io/master</code> or <code>node-role.kubernetes.io/infra</code>.</p>
</div>
<div class="paragraph">
<p>If the labels for the profile with priority <code>10</code> matched,
<code>openshift-control-plane-es</code> profile is applied and no other profile is
considered. If the node/pod label combination did not match, the second highest
priority profile (<code>openshift-control-plane</code>) is considered. This profile is
applied if the containerized tuned pod runs on a node with labels
<code>node-role.kubernetes.io/master</code> or <code>node-role.kubernetes.io/infra</code>.</p>
</div>
<div class="paragraph">
<p>Finally, the profile <code>openshift-node</code> has the lowest priority of <code>30</code>. It lacks
the <code>&lt;match&gt;</code> section and, therefore, will always match. It acts as a profile
catch-all to set <code>openshift-node</code> profile, if no other profile with higher
priority matches on a given node.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/node-tuning-operator-workflow-revised.png" alt="Decision workflow">
</div>
</div>
</div>
<div class="sect3">
<h4 id="custom-tuning-default-profiles-set-nodes-node-tuning-operator">Default profiles set on a cluster</h4>
<div class="paragraph">
<p>The following are the default profiles set on a cluster.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">apiVersion: tuned.openshift.io/v1alpha1
kind: Tuned
metadata:
  name: default
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - name: "openshift"
    data: |
      [main]
      summary=Optimize systems running OpenShift (parent profile)
      include=${f:virt_check:virtual-guest:throughput-performance}
      [selinux]
      avc_cache_threshold=8192
      [net]
      nf_conntrack_hashsize=131072
      [sysctl]
      net.ipv4.ip_forward=1
      kernel.pid_max=&gt;131072
      net.netfilter.nf_conntrack_max=1048576
      net.ipv4.neigh.default.gc_thresh1=8192
      net.ipv4.neigh.default.gc_thresh2=32768
      net.ipv4.neigh.default.gc_thresh3=65536
      net.ipv6.neigh.default.gc_thresh1=8192
      net.ipv6.neigh.default.gc_thresh2=32768
      net.ipv6.neigh.default.gc_thresh3=65536
      [sysfs]
      /sys/module/nvme_core/parameters/io_timeout=4294967295
      /sys/module/nvme_core/parameters/max_retries=10
  - name: "openshift-control-plane"
    data: |
      [main]
      summary=Optimize systems running OpenShift control plane
      include=openshift
      [sysctl]
      # ktune sysctl settings, maximizing i/o throughput
      #
      # Minimal preemption granularity for CPU-bound tasks:
      # (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
      kernel.sched_min_granularity_ns=10000000
      # The total time the scheduler will consider a migrated process
      # "cache hot" and thus less likely to be re-migrated
      # (system default is 500000, i.e. 0.5 ms)
      kernel.sched_migration_cost_ns=5000000
      # SCHED_OTHER wake-up granularity.
      #
      # Preemption granularity when tasks wake up.  Lower the value to
      # improve wake-up latency and throughput for latency critical tasks.
      kernel.sched_wakeup_granularity_ns=4000000
  - name: "openshift-node"
    data: |
      [main]
      summary=Optimize systems running OpenShift nodes
      include=openshift
      [sysctl]
      net.ipv4.tcp_fastopen=3
      fs.inotify.max_user_watches=65536
  - name: "openshift-control-plane-es"
    data: |
      [main]
      summary=Optimize systems running ES on OpenShift control-plane
      include=openshift-control-plane
      [sysctl]
      vm.max_map_count=262144
  - name: "openshift-node-es"
    data: |
      [main]
      summary=Optimize systems running ES on OpenShift nodes
      include=openshift-node
      [sysctl]
      vm.max_map_count=262144
  recommend:
  - profile: "openshift-control-plane-es"
    priority: 10
    match:
    - label: "tuned.openshift.io/elasticsearch"
      type: "pod"
      match:
      - label: "node-role.kubernetes.io/master"
      - label: "node-role.kubernetes.io/infra"

  - profile: "openshift-node-es"
    priority: 20
    match:
    - label: "tuned.openshift.io/elasticsearch"
      type: "pod"

  - profile: "openshift-control-plane"
    priority: 30
    match:
    - label: "node-role.kubernetes.io/master"
    - label: "node-role.kubernetes.io/infra"

  - profile: "openshift-node"
priority: 40</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="supported-tuned-daemon-plug-ins-nodes-node-tuning-operator">Supported Tuned daemon plug-ins</h4>
<div class="paragraph">
<p>Excluding the <code>[main]</code> section, the following Tuned plug-ins are supported when
using custom profiles defined in the <code>profile:</code> section of the Tuned CR:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>audio</p>
</li>
<li>
<p>cpu</p>
</li>
<li>
<p>disk</p>
</li>
<li>
<p>eeepc_she</p>
</li>
<li>
<p>modules</p>
</li>
<li>
<p>mounts</p>
</li>
<li>
<p>net</p>
</li>
<li>
<p>scheduler</p>
</li>
<li>
<p>scsi_host</p>
</li>
<li>
<p>selinux</p>
</li>
<li>
<p>sysctl</p>
</li>
<li>
<p>sysfs</p>
</li>
<li>
<p>usb</p>
</li>
<li>
<p>video</p>
</li>
<li>
<p>vm</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There is some dynamic tuning functionality provided by some of these plug-ins
that is not supported. The following Tuned plug-ins are currently not supported:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>bootloader</p>
</li>
<li>
<p>script</p>
</li>
<li>
<p>systemd</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="understanding-node-rebooting">Understanding node rebooting in OpenShift Enterprise</h3>
<div class="paragraph">
<p>To reboot a node without causing an outage for applications running on the
platform, it is important to first evacuate the pods. For pods that are
made highly available by the routing tier, nothing
else needs to be done. For other pods needing storage, typically databases, it
is critical to ensure that they can remain in operation with one pod
temporarily going offline. While implementing resiliency for stateful pods
is different for each application, in all cases it is important to configure
the scheduler to use node anti-affinity to
ensure that the pods are properly spread across available nodes.</p>
</div>
<div class="paragraph">
<p>Another challenge is how to handle nodes that are running critical
infrastructure such as the router or the registry. The same node evacuation
process applies, though it is important to understand certain edge cases.</p>
</div>
<div class="sect3">
<h4 id="nodes-nodes-rebooting-infrastructure-nodes-nodes-rebooting">Understanding infrastructure node rebooting in OpenShift Enterprise</h4>
<div class="paragraph">
<p>Infrastructure nodes are nodes that are labeled to run pieces of the
OpenShift Enterprise environment. Currently, the easiest way to manage node reboots
is to ensure that there are at least three nodes available to run
infrastructure. The nodes to run the infrastructure are called <strong>master</strong> nodes.</p>
</div>
<div class="paragraph">
<p>The scenario below demonstrates a common mistake that can lead
to service interruptions for the applications running on OpenShift Enterprise when
only two nodes are available.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Node A is marked unschedulable and all pods are evacuated.</p>
</li>
<li>
<p>The registry pod running on that node is now redeployed on node B. This means
node B is now running both registry pods.</p>
</li>
<li>
<p>Node B is now marked unschedulable and is evacuated.</p>
</li>
<li>
<p>The service exposing the two pod endpoints on node B, for a brief period of
time, loses all endpoints until they are redeployed to node A.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The same process using three master nodes for infrastructure does not result in a service
disruption. However, due to pod scheduling, the last node that is evacuated and
brought back in to rotation is left running zero registries. The other two nodes
will run two and one registries respectively. The best solution is to rely on
pod anti-affinity.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-rebooting-affinity-nodes-nodes-rebooting">Rebooting a node using pod anti-affinity</h4>
<div class="paragraph">
<p>Pod anti-affinity is slightly different than node anti-affinity. Node anti-affinity can be
violated if there are no other suitable locations to deploy a pod. Pod
anti-affinity can be set to either required or preferred.</p>
</div>
<div class="paragraph">
<p>With this in place, if only two infrastructure nodes are available and one is rebooted, the container image registry
pod is prevented from running on the other node. <code><strong>oc get pods</strong></code> reports the pod as unready until a suitable node is available.
Once a node is available and all pods are back in ready state, the next node can be restarted.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To reboot a node using pod anti-affinity:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Edit the node specification to configure pod anti-affinity:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: with-pod-antiaffinity
spec:
  affinity:
    podAntiAffinity: <b class="conum">(1)</b>
      preferredDuringSchedulingIgnoredDuringExecution: <b class="conum">(2)</b>
      - weight: 100 <b class="conum">(3)</b>
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: registry <b class="conum">(4)</b>
              operator: In <b class="conum">(5)</b>
              values:
              - default
          topologyKey: kubernetes.io/hostname</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Stanza to configure pod anti-affinity.</p>
</li>
<li>
<p>Defines a preferred rule.</p>
</li>
<li>
<p>Specifies a weight for a preferred rule. The node with the highest weight is preferred.</p>
</li>
<li>
<p>Description of the pod label that determines when the anti-affinity rule applies. Specify a key and value for the label.</p>
</li>
<li>
<p>The operator represents the relationship between the label on the existing pod and the set of values in the <code>matchExpression</code> parameters in the specification for the new pod. Can be <code>In</code>, <code>NotIn</code>, <code>Exists</code>, or <code>DoesNotExist</code>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This example assumes the container image registry pod has a label of
<code><strong>registry=default</strong></code>. Pod anti-affinity can use any Kubernetes match
expression.</p>
</div>
</li>
<li>
<p>Enable the <code><strong>MatchInterPodAffinity</strong></code> scheduler predicate in the scheduling policy file. See <em>Default Scheduling</em>.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-rebooting-router-nodes-nodes-rebooting">Understanding how to reboot nodes running routers</h4>
<div class="paragraph">
<p>In most cases, a pod running an OpenShift Enterprise router exposes a host port.</p>
</div>
<div class="paragraph">
<p>The <code><strong>PodFitsPorts</strong></code> scheduler predicate ensures that no router pods using the
same port can run on the same node, and pod anti-affinity is achieved. If the
routers are relying on IP failover for high availability, there is nothing else that is needed.</p>
</div>
<div class="paragraph">
<p>For router pods relying on an external service such as AWS Elastic Load Balancing for high
availability, it is that service&#8217;s responsibility to react to router pod restarts.</p>
</div>
<div class="paragraph">
<p>In rare cases, a router pod may not have a host port configured. In those cases,
it is important to follow the recommended restart process for infrastructure nodes.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="freeing-node-resources-using-garbage-collection">Freeing node resources using garbage collection</h3>
<div class="paragraph">
<p>As an administrator, you can use OpenShift Enterprise to ensure that your nodes are running efficiently
by freeing up resources through garbage collection.</p>
</div>
<div class="paragraph">
<p>The OpenShift Enterprise node performs two types of garbage collection:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Container garbage collection: Removes terminated containers.</p>
</li>
<li>
<p>Image garbage collection: Removes images not referenced by any running pods.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="nodes-nodes-garbage-collection-containers-nodes-nodes-configuring">Understanding how terminated containers are removed though garbage collection</h4>
<div class="paragraph">
<p>Container garbage collection is enabled by default and happens automatically in
response to eviction thresholds being reached. The node tries to keep any
container for any pod accessible from the API. If the pod has been deleted, the
containers will be as well. Containers are preserved as long the pod is not
deleted and the eviction threshold is not reached. If the node is under disk
pressure, it will remove containers and their logs will no longer be accessible
via <code>oc logs</code>.</p>
</div>
<div class="paragraph">
<p>The policy for container garbage collection is based on three conditions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The minimum age that a container is eligible for garbage collection. The
default is <strong>0</strong>.</p>
</li>
<li>
<p>The number of instances to retain per pod container. The default is <strong>1</strong>.</p>
</li>
<li>
<p>The maximum number of total dead containers in the node. The default is <strong>-1</strong>, which means unlimited.</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>Garbage collection only removes the containers that do not have any pods.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For container garbage collection, you can modify any of the following variables using
a Custom Resource.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 3. Variables for configuring container garbage collection</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Setting</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>minimum-container-ttl-duration</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The minimum age that a container is eligible for garbage collection. The
default is <strong>0</strong>. Use <strong>0</strong> for no limit. Values for this setting can be
specified using unit suffixes such as <strong>h</strong> for hour, <strong>m</strong> for minutes, <strong>s</strong> for seconds.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>maximum-dead-containers-per-container</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The number of instances to retain per pod container. The default is <strong>1</strong>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>maximum-dead-containers</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The maximum number of total dead containers in the node. The default is <strong>-1</strong>, which means unlimited.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The <code><strong>maximum-dead-containers</strong></code> setting takes precedence over the
<code><strong>maximum-dead-containers-per-container</strong></code> setting when there is a conflict. For
example, if retaining the number of <code><strong>maximum-dead-containers-per-container</strong></code>
would result in a total number of containers that is greater than
<code><strong>maximum-dead-containers</strong></code>, the oldest containers will be removed to satisfy
the <code><strong>maximum-dead-containers</strong></code> limit.</p>
</div>
<div class="paragraph">
<p>When the node removes the dead containers, all files inside those containers are
removed as well. Only containers created by the node are removed.</p>
</div>
<div class="paragraph">
<p>Each spin of the garbage collector loop goes through the following steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Retrieve a list of available containers.</p>
</li>
<li>
<p>Filter out all containers that are running or are not alive longer than
the <code><strong>minimum-container-ttl-duration</strong></code> parameter.</p>
</li>
<li>
<p>Classify all remaining containers into equivalence classes based on pod and image name membership.</p>
</li>
<li>
<p>Remove all unidentified containers (containers that are managed by kubelet but their name is malformed).</p>
</li>
<li>
<p>For each class that contains more containers than the
<code><strong>maximum-dead-containers-per-container</strong></code> parameter, sort containers in the class by
creation time.</p>
</li>
<li>
<p>Start removing containers from the oldest first until the
<code><strong>maximum-dead-containers-per-container</strong></code> parameter is met.</p>
</li>
<li>
<p>If there are still more containers in the list than the
<code><strong>maximum-dead-containers</strong></code> parameter, the collector starts removing containers
from each class so the number of containers in each one is not greater than the
average number of containers per class, or
<code>&lt;all_remaining_containers&gt;/&lt;number_of_classes&gt;</code>.</p>
</li>
<li>
<p>If this is still not enough, sort all containers in the list and start
removing containers from the oldest first until the <code><strong>maximum-dead-containers</strong></code>
criterion is met.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-garbage-collection-images-nodes-nodes-configuring">Understanding how images are removed though garbage collection</h4>
<div class="paragraph">
<p>Image garbage collection relies on disk usage as reported by <strong>cAdvisor</strong> on the
node to decide which images to remove from the node.</p>
</div>
<div class="paragraph">
<p>The policy for container garbage collection is based on two conditions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The percent of disk usage (expressed as an integer) which triggers image
garbage collection. The default is <strong>85</strong>.</p>
</li>
<li>
<p>The percent of disk usage (expressed as an integer) to which image garbage
collection attempts to free. Default is <strong>80</strong>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For image garbage collection, you can modify any of the following variables using
a Custom Resource.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 4. Variables for configuring image garbage collection</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Setting</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>image-gc-high-threshold</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The percent of disk usage (expressed as an integer) which triggers image
garbage collection. The default is <strong>85</strong>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>image-gc-low-threshold</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The percent of disk usage (expressed as an integer) to which image garbage
collection attempts to free. Default is <strong>80</strong>.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Two lists of images are retrieved in each garbage collector run:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A list of images currently running in at least one pod.</p>
</li>
<li>
<p>A list of images available on a host.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>As new containers are run, new images appear. All images are marked with a time
stamp. If the image is running (the first list above) or is newly detected (the
second list above), it is marked with the current time. The remaining images are
already marked from the previous spins. All images are then sorted by the time
stamp.</p>
</div>
<div class="paragraph">
<p>Once the collection starts, the oldest images get deleted first until the
stopping criterion is met.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-garbage-collection-configuring-nodes-nodes-configuring">Configuring garbage collection for containers and images</h4>
<div class="paragraph">
<p>As an administrator, you can configure how OpenShift Enterprise performs garbage collection.</p>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>Obtain the label associated with the static Machine Config Pool CRD for the type of node you want to configure.
Perform one of the following steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>View the Machine Config Pool:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe machineconfigpool &lt;name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ oc describe machineconfigpool worker

apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: 2019-02-08T14:52:39Z
  generation: 1
  labels:
    custom-kubelet: small-pods <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>If a label has been added it appears under <code>labels</code>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>If a label is not present, add a key/value pair:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a Custom Resource (CR) for your configuration change.</p>
<div class="listingblock">
<div class="title">Sample configuration for a <strong>garbage</strong> CR</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-garbage-collection <b class="conum">(1)</b>
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: small-pods <b class="conum">(2)</b>
  kubeletConfig:
    ImageMinimumGCAge: 0 <b class="conum">(3)</b>
    ImageGCHighThresholdPercent: 85 <b class="conum">(4)</b>
    ImageGCLowThresholdPercent: 80 <b class="conum">(5)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Assign a name to CR.</p>
</li>
<li>
<p>Specify the label to apply the configuration change.</p>
</li>
<li>
<p>Specify the minimum age for an unused image before it is garbage collected. A value of <code>0</code> means no limit.</p>
</li>
<li>
<p>Specify the percent of disk usage after which image garbage collection is always run.</p>
</li>
<li>
<p>Specify the percent of disk usage before which image garbage collection is never run.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the garbage collection object:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="allocating-resources-for-nodes">Allocating resources for nodes in an OpenShift Enterprise cluster</h3>
<div class="paragraph">
<p>To provide more reliable scheduling and minimize node resource overcommitment,
each node can reserve a portion of its resources for use by all underlying node
components (such as kubelet, kube-proxy) and the remaining system
components (such as <strong>sshd</strong>, <strong>NetworkManager</strong>) on the host. Once specified, the
scheduler has more information about the resources (e.g., memory, CPU) a node
has allocated for pods.</p>
</div>
<div class="sect3">
<h4 id="nodes-nodes-resources-configuring-about-nodes-nodes-resources-configuring">Understanding how to allocate resources for nodes</h4>
<div class="paragraph">
<p>CPU and memory resources reserved for node components in OpenShift Enterprise are based on two node settings:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 66.6667%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Setting</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>kube-reserved</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Resources reserved for node components. Default is none.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>system-reserved</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Resources reserved for the remaining system components. Default is none.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>If a flag is not set, it defaults to <strong>0</strong>. If none of the flags are set, the
allocated resource is set to the node&#8217;s capacity as it was before the
introduction of allocatable resources.</p>
</div>
<div class="sect4">
<h5 id="computing-allocated-resources-nodes-nodes-resources-configuring">How OpenShift Enterprise computes allocated resources</h5>
<div class="paragraph">
<p>An allocated amount of a resource is computed based on the following formula:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">[Allocatable] = [Node Capacity] - [kube-reserved] - [system-reserved] - [Hard-Eviction-Thresholds]</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The withholding of <code>Hard-Eviction-Thresholds</code> from allocatable is a change in behavior to improve
system reliability now that allocatable is enforced for end-user pods at the node level.
The <code><strong>experimental-allocatable-ignore-eviction</strong></code> setting is available to preserve legacy behavior,
but it will be deprecated in a future release.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If <code>[Allocatable]</code> is negative, it is set to <strong>0</strong>.</p>
</div>
<div class="paragraph">
<p>Each node reports system resources utilized by the container runtime and kubelet.
To better aid your ability to configure <code><strong>--system-reserved</strong></code> and <code><strong>--kube-reserved</strong></code>,
you can introspect corresponding node&#8217;s resource usage using the node summary API,
which is accessible at <strong><em>&lt;master&gt;/api/v1/nodes/&lt;node&gt;/proxy/stats/summary</em></strong>.</p>
</div>
</div>
<div class="sect4">
<h5 id="allocate-node-enforcement-nodes-nodes-resources-configuring">How nodes enforce resource constraints</h5>
<div class="paragraph">
<p>The node is able to limit the total amount of resources that pods
may consume based on the configured allocatable value.  This feature significantly
improves the reliability of the node by preventing pods from starving
system services (for example: container runtime, node agent, etc.) for resources.
It is strongly encouraged that administrators reserve
resources based on the desired node utilization target
in order to improve node reliability.</p>
</div>
<div class="paragraph">
<p>The node enforces resource constraints using a new <strong>cgroup</strong> hierarchy
that enforces quality of service.  All pods are launched in a
dedicated cgroup hierarchy separate from system daemons.</p>
</div>
<div class="paragraph">
<p>Optionally, the node can be made to enforce kube-reserved and system-reserved by
specifying those tokens in the enforce-node-allocatable flag.  If specified, the
corresponding <code>--kube-reserved-cgroup</code> or <code>--system-reserved-cgroup</code> needs to be provided.
In future releases, the node and container runtime will be packaged in a common cgroup
separate from <code>system.slice</code>.  Until that time, we do not recommend users
change the default value of enforce-node-allocatable flag.</p>
</div>
<div class="paragraph">
<p>Administrators should treat system daemons similar to Guaranteed pods.  System daemons
can burst within their bounding control groups and this behavior needs to be managed
as part of cluster deployments.  Enforcing system-reserved limits
can lead to critical system services being CPU starved or OOM killed on the node. The
recommendation is to enforce system-reserved only if operators have profiled their nodes
exhaustively to determine precise estimates and are confident in their ability to
recover if any process in that group is OOM killed.</p>
</div>
<div class="paragraph">
<p>As a result, we strongly recommended that users only enforce node allocatable for
<code>pods</code> by default, and set aside appropriate reservations for system daemons to maintain
overall node reliability.</p>
</div>
</div>
<div class="sect4">
<h5 id="allocate-eviction-thresholds-nodes-nodes-resources-configuring">Understanding Eviction Thresholds</h5>
<div class="paragraph">
<p>If a node is under memory pressure, it can impact the entire node and all pods running on
it.  If a system daemon is using more than its reserved amount of memory, an OOM
event may occur that can impact the entire node and all pods running on it.  To avoid
(or reduce the probability of) system OOMs the node provides out-of-resource handling.</p>
</div>
<div class="paragraph">
<p>You can reserve some memory using the <code>--eviction-hard</code> flag. The node attempts to evict
pods whenever memory availability on the node drops below the absolute value or percentage.
If system daemons do not exist on a node, pods are limited to the memory
<code>capacity - eviction-hard</code>. For this reason, resources set aside as a buffer for eviction
before reaching out of memory conditions are not available for pods.</p>
</div>
<div class="paragraph">
<p>The following is an example to illustrate the impact of node allocatable for memory:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Node capacity is <code>32Gi</code></p>
</li>
<li>
<p>--kube-reserved is <code>2Gi</code></p>
</li>
<li>
<p>--system-reserved is <code>1Gi</code></p>
</li>
<li>
<p>--eviction-hard is set to <code>&lt;100Mi</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For this node, the effective node allocatable value is <code>28.9Gi</code>. If the node
and system components use up all their reservation, the memory available for pods is <code>28.9Gi</code>,
and kubelet will evict pods when it exceeds this usage.</p>
</div>
<div class="paragraph">
<p>If you enforce node allocatable (<code>28.9Gi</code>) via top level cgroups, then pods can never exceed <code>28.9Gi</code>.
Evictions would not be performed unless system daemons are consuming more than <code>3.1Gi</code> of memory.</p>
</div>
<div class="paragraph">
<p>If system daemons do not use up all their reservation, with the above example,
pods would face memcg OOM kills from their bounding cgroup before node evictions kick in.
To better enforce QoS under this situation, the node applies the hard eviction thresholds to
the top-level cgroup for all pods to be <code>Node Allocatable + Eviction Hard Thresholds</code>.</p>
</div>
<div class="paragraph">
<p>If system daemons do not use up all their reservation, the node will evict pods whenever
they consume more than <code>28.9Gi</code> of memory. If eviction does not occur in time, a pod
will be OOM killed if pods consume <code>29Gi</code> of memory.</p>
</div>
</div>
<div class="sect4">
<h5 id="allocate-scheduler-policy-nodes-nodes-resources-configuring">How the scheduler determines resource availability</h5>
<div class="paragraph">
<p>The scheduler uses the value of <code><strong>node.Status.Allocatable</strong></code> instead of
<code><strong>node.Status.Capacity</strong></code> to decide if a node will become a candidate for pod
scheduling.</p>
</div>
<div class="paragraph">
<p>By default, the node will report its machine capacity as fully schedulable by
the cluster.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-resources-configuring-setting-nodes-nodes-resources-configuring">Configuring allocated resources for nodes</h4>
<div class="paragraph">
<p>OpenShift Enterprise supports the CPU and memory resource types for allocation. If
your administrator enabled the ephemeral storage technology preview, the
<code><strong>ephemeral-resource</strong></code> resource type is supported as well. For the <code><strong>cpu</strong></code> type, the
resource quantity is specified in units of cores, such as <code>200m</code>, <code>0.5</code>, or <code>1</code>.
For <code><strong>memory</strong></code> and <code><strong>ephemeral-storage</strong></code>, it is specified in units of bytes,
such as <code>200Ki</code>, <code>50Mi</code>, or <code>5Gi</code>.</p>
</div>
<div class="paragraph">
<p>As an administrator, you can set these using a Custom Resource (CR) through a set of <code>&lt;resource_type&gt;=&lt;resource_quantity&gt;</code> pairs
(e.g., <strong>cpu=200m,memory=512Mi</strong>).</p>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>To help you determine setting for <code><strong>--system-reserved</strong></code> and <code><strong>--kube-reserved</strong></code> you can introspect the corresponding node&#8217;s resource usage
using the node summary API, which is accessible at <strong><em>&lt;master&gt;/api/v1/nodes/&lt;node&gt;/proxy/stats/summary</em></strong>. Run the following command for your node:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ curl &lt;certificate details&gt; https://&lt;master&gt;/api/v1/nodes/&lt;node-name&gt;/proxy/stats/summary</pre>
</div>
</div>
<div class="paragraph">
<p>The <em>REST API Overview</em> has details about certificate details.</p>
</div>
<div class="paragraph">
<p>For example, to access the resources from <strong>cluster.node22</strong> node, you can run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ curl &lt;certificate details&gt; https://&lt;master&gt;/api/v1/nodes/cluster.node22/proxy/stats/summary
{
    "node": {
        "nodeName": "cluster.node22",
        "systemContainers": [
            {
                "cpu": {
                    "usageCoreNanoSeconds": 929684480915,
                    "usageNanoCores": 190998084
                },
                "memory": {
                    "rssBytes": 176726016,
                    "usageBytes": 1397895168,
                    "workingSetBytes": 1050509312
                },
                "name": "kubelet"
            },
            {
                "cpu": {
                    "usageCoreNanoSeconds": 128521955903,
                    "usageNanoCores": 5928600
                },
                "memory": {
                    "rssBytes": 35958784,
                    "usageBytes": 129671168,
                    "workingSetBytes": 102416384
                },
                "name": "runtime"
            }
        ]
    }
}</pre>
</div>
</div>
</li>
<li>
<p>Obtain the label associated with the static Machine Config Pool CRD for the type of node you want to configure.
Perform one of the following steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>View the Machine Config Pool:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe machineconfigpool &lt;name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ oc describe machineconfigpool worker

apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: 2019-02-08T14:52:39Z
  generation: 1
  labels:
    custom-kubelet: small-pods <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>If a label has been added it appears under <code>labels</code>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>If the label is not present, add a key/value pair:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a Custom Resource (CR) for your configuration change.</p>
<div class="listingblock">
<div class="title">Sample configuration for a resource allocation CR</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-allocatable <b class="conum">(1)</b>
spec:
  machineConfigSelector:
    matchLabels:
      custom-kubelet: small-pods <b class="conum">(2)</b>
  kubeletConfig:
    EnforceNodeAllocatable:
      kube-reserved:
        - "cpu=200m,memory=512Mi"
      system-reserved:
        - "cpu=200m,memory=512Mi"</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="advertising-hidden-resources-for-nodes">Advertising hidden resources for nodes in an OpenShift Enterprise cluster</h3>
<div class="paragraph">
<p>Opaque integer resources allow cluster operators to provide new node-level
resources that would be otherwise unknown to the system. Users can consume these
resources in pod specifications, similar to CPU and memory. The scheduler performs
resource accounting so that no more than the available amount is
simultaneously allocated to pods.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Opaque integer resources are Alpha currently, and only resource accounting is
implemented. There is no resource quota or limit range support for these
resources, and they have no impact on QoS.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="nodes-nodes-opaque-resources-about-nodes-nodes-opaque-resources">Understanding opaque resources</h4>
<div class="paragraph">
<p>Opaque integer resources are called <em>opaque</em> because OpenShift Enterprise
does not know what the resource is, but will schedule a pod on a node
only if enough of that resource is available. They are called <em>integer resources</em>
because they must be available, or <em>advertised</em>, in integer amounts. The API server
restricts quantities of these resources to whole numbers. Examples of
<em>valid</em> quantities are <code>3</code>, <code>3000m</code>, and <code>3Ki</code>.</p>
</div>
<div class="paragraph">
<p>Opaque integer resources can be used to allocate:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Last-level cache (LLC)</p>
</li>
<li>
<p>Graphics processing unit (GPU) devices</p>
</li>
<li>
<p>Field-programmable gate array (FPGA) devices</p>
</li>
<li>
<p>Slots for sharing bandwidth to a parallel file system.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, if a node has 800 GiB of a special kind of disk storage, you could create a name for the special storage,
such as <code><em>opaque-int-resource-special-storage</em></code>. You could advertise it in chunks of a certain size,
such as 100 GiB. In that case, your node would advertise that it has eight resources of type <code>opaque-int-resource-special-storage</code>.</p>
</div>
<div class="paragraph">
<p>Opaque integer resource names must begin with the prefix <code>pod.alpha.kubernetes.io/opaque-int-resource-</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-opaque-resources-creating-nodes-nodes-opaque-resources">Creating Opaque Integer Resources</h4>
<div class="paragraph">
<p>There are two steps required to use opaque integer resources. First, the cluster
operator must name and advertise a per-node opaque resource on one or more nodes. Second,
application developer must request the opaque resource in pods.</p>
</div>
<div class="paragraph">
<p>To make opaque integer resources available:</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Allocate the resource and assign a name starting with <code>pod.alpha.kubernetes.io/opaque-int-resource-</code></p>
</li>
<li>
<p>Advertise a new opaque integer resource by submitting
a PATCH HTTP request to the API server that specifies the available
quantity in the <code>status.capacity</code> for a node in the cluster.</p>
<div class="paragraph">
<p>For example, the following HTTP request advertises five <code>foo</code> resources on the
<code>openshift-node-1</code> node.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">PATCH /api/v1/nodes/openshift-node-1/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: openshift-master:8080

[
  {
    "op": "add",
    "path": "/status/capacity/pod.alpha.kubernetes.io~1opaque-int-resource-foo",
    "value": "5"
  }
]</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>~1</code> in the <code>path</code> is the encoding for the character <code>/</code>.
The operation path value in the JSON-Patch is interpreted as a
JSON-Pointer. For more details, refer to
<a href="https://tools.ietf.org/html/rfc6901#section-3">IETF RFC 6901, section 3</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After this operation, the node <code>status.capacity</code> includes a new resource. The
<code>status.allocatable</code> field is updated automatically with the new resource
asynchronously.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Since the scheduler uses the node <code>status.allocatable</code> value when evaluating pod
fitness, there might be a short delay between patching the node capacity with a
new resource and the first pod that requests the resource to be scheduled on
that node.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-opaque-resources-consuming-nodes-nodes-opaque-resources">Consuming Opaque Integer Resources</h4>
<div class="paragraph">
<p>An application developer can consume the opaque resources by editing the pod configuration.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>Edit the pod configuration to include the name of the opaque resource as a key in the <code>spec.containers[].resources.requests</code> field.</p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="paragraph">
<p>The following pod requests two CPUs and one <code>foo</code> (an opaque resource).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: myimage
    resources:
      requests:
        cpu: 2
        pod.alpha.kubernetes.io/opaque-int-resource-foo: 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>The pod will be scheduled only if all of the resource requests are satisfied
(including CPU, memory, and any opaque resources). The pod will remain in the
<code>PENDING</code> state while the resource request cannot be met by any node.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">Conditions:
  Type    Status
  PodScheduled  False
...
Events:
  FirstSeen  LastSeen	Count	From		  SubObjectPath	Type	  Reason	    Message
  ---------  --------	-----	----		  -------------	--------  ------	    -------
  14s	     0s		6	default-scheduler		Warning	  FailedScheduling  No nodes are available that match all of the following predicates:: Insufficient pod.alpha.kubernetes.io/opaque-int-resource-foo (1).</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="monitoring-for-problems-in-your-nodes">Monitoring for problems in your OpenShift Enterprise nodes</h3>
<div class="paragraph">
<p>The Node Problem Detector monitors the health of your nodes
by finding certain problems and reporting these problems to the API server.
The detector runs as a daemonset on each node.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>The Node Problem Detector is a Technology Preview feature
only.
Technology Preview features are not supported with Red Hat production service
level agreements (SLAs), might not be functionally complete, and Red Hat does
not recommend to use them for production. These features provide early access to
upcoming product features, enabling customers to test functionality and provide
feedback during the development process.</p>
</div>
<div class="paragraph">
<p>For more information on Red Hat Technology Preview features support scope, see
<a href="https://access.redhat.com/support/offerings/techpreview/" class="bare">https://access.redhat.com/support/offerings/techpreview/</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Procedures in this topic require your cluster to be in an unmanaged state.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="nodes-nodes-problem-detector-about-nodes-nodes-problem-detector">Understanding the OpenShift Enterprise node problem detector</h4>
<div class="paragraph">
<p>The Node Problem Detector reads system logs and watches for specific entries and makes these problems visible to the control plane,
which you can view using OpenShift Enterprise commands, such as <code>oc get node</code> and <code>oc get event</code>.
You could then take action to correct these problems as appropriate or capture the messages using a tool of your choice,
such as the OpenShift Enterprise log monitoring.</p>
</div>
<div class="paragraph">
<p>Detected problems can be in one of the following categories:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>NodeCondition</code>: A permanent problem that makes the node unavailable for pods.
The node condition will not be cleared until the host is rebooted.</p>
</li>
<li>
<p><code>Event</code>: A temporary problem that has limited impact on a node, but is informative.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Node Problem Detector can detect:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>container runtime issues:</p>
<div class="ulist">
<ul>
<li>
<p>unresponsive runtime daemons</p>
</li>
</ul>
</div>
</li>
<li>
<p>hardware issues:</p>
<div class="ulist">
<ul>
<li>
<p>bad CPU</p>
</li>
<li>
<p>bad memory</p>
</li>
<li>
<p>bad disk</p>
</li>
</ul>
</div>
</li>
<li>
<p>kernel issues:</p>
<div class="ulist">
<ul>
<li>
<p>kernel deadlock conditions</p>
</li>
<li>
<p>corrupted file systems</p>
</li>
<li>
<p>unresponsive runtime daemons</p>
</li>
</ul>
</div>
</li>
<li>
<p>infrastructure daemon issues:</p>
<div class="ulist">
<ul>
<li>
<p>NTP service outages</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="nodes-nodes-problem-detector-about-example-nodes-nodes-problem-detector">Example Node Problem Detector Output</h5>
<div class="paragraph">
<p>The following examples show output from the Node Problem Detector watching for kernel deadlock node condition on a specific node. The command
uses <code>oc get node</code> to watch a specific node filtering for a <code>KernelDeadlock</code> entry in a log.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get node &lt;node&gt; -o yaml | grep -B5 KernelDeadlock</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Node Problem Detector output with no issues</div>
<div class="content">
<pre class="nowrap">message: kernel has no deadlock
reason: KernelHasNoDeadlock
status: false
type: KernelDeadLock</pre>
</div>
</div>
<div class="paragraph">
<p>This example shows output from the Node Problem Detector watching for events on a node.
The following command uses <code>oc get event</code> against the <strong>default</strong> project watching for
events listed in the <code>kernel-monitor.json</code> section of the Node Problem Detector configuration map.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get event -n default --field-selector=source=kernel-monitor --watch</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample output showing events on nodes</div>
<div class="content">
<pre class="nowrap">LAST SEEN                       FIRST SEEN                    COUNT NAME     KIND  SUBOBJECT TYPE    REASON      SOURCE                   MESSAGE
2018-06-27 09:08:27 -0400 EDT   2018-06-27 09:08:27 -0400 EDT 3     my-node2 node            Warning KernelOops  kernel-monitor.my-node2  BUG: unable to handle kernel NULL pointer deference at nowhere
2018-06-27 09:08:27 -0400 EDT   2018-06-27 09:08:27 -0400 EDT 1     my-node1 node            Warning KernelOops  kernel-monitor.my-node2  divide error 0000 [#0] SMP</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The Node Problem Detector consumes resources. If you use the Node Problem Detector, make sure you have enough nodes to balance cluster performance.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-problem-detector-installing-nodes-nodes-problem-detector">Installing the OpenShift Enterprise Node Problem Detector</h4>
<div class="paragraph">
<p>You can use the OpenShift Enterprise console to install the Node Problem Detector Operator.</p>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>Create a Project for the NPD:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm new-project openshift-node-problem-detector --node-selector ""</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>The process to install the Node Problem Detector involves installing the Node Problem Detector Operator and creating a Node Problem Detector instance.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Install the Node Problem Detector Operator:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In the OpenShift Enterprise console, click <strong>Catalog</strong> &#8594; <strong>Operator Hub</strong>.</p>
</li>
<li>
<p>Choose  <strong>Node Problem Detector</strong> from the list of available Operators, and click <strong>Install</strong>.</p>
</li>
<li>
<p>On the <strong>Create Operator Subscription</strong> page, under <strong>A specific namespace on the cluster</strong> select <strong>openshift-node-problem-detector</strong>.
Then, click <strong>Subscribe</strong>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Verify the operator installations:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Switch to the <strong>Catalog</strong>  <strong>Installed Operators</strong> page.</p>
</li>
<li>
<p>Ensure that <strong>Node Problem Detector</strong> is listed on
the <strong>InstallSucceeded</strong> tab with a <strong>Status</strong> of <strong>InstallSucceeded</strong>. Change the project to <strong>openshift-node-problem-detector</strong> if necessary.</p>
<div class="paragraph">
<p>If either operator does not appear as installed, to troubleshoot further:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>On the <strong>Copied</strong> tab of the <strong>Installed Operators</strong> page, if an operator show a <strong>Status</strong> of
<strong>Copied</strong>, this indicates the installation is in process and is expected behavior.</p>
</li>
<li>
<p>Switch to the <strong>Catalog</strong>  <strong>Operator Management</strong> page and inspect
the <strong>Operator Subscriptions</strong> and <strong>Install Plans</strong> tabs for any failure or errors
under <strong>Status</strong>.</p>
</li>
<li>
<p>Switch to the <strong>Workloads</strong>  <strong>Pods</strong> page and check the logs in any Pods in the
<code>openshift-logging</code> and <code>openshift-operators</code> projects that are reporting issues.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create a cluster logging instance:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Switch to the the <strong>Administration</strong> &#8594; <strong>CRD</strong> page.</p>
</li>
<li>
<p>On the <strong>Custom Resource Definitions</strong> page, click <strong>NodeProblemDetector</strong>.</p>
</li>
<li>
<p>On the <strong>Node Problem Detector</strong> page, click <strong>Create Node Problem Detector</strong>.</p>
<div class="paragraph">
<p>You might need to refresh the page to load the data.</p>
</div>
</li>
<li>
<p>Specify a name and enter the <strong>openshift-node-problem-detector</strong> namespace.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: node-problem-detector.operator.k8s.io/v1alpha1
kind: NodeProblemDetector
metadata:
  name: nodeproblemdetectors.node-problem-detector.operator.k8s.io <b class="conum">(1)</b>
  namespace: openshift-node-problem-detector <b class="conum">(2)</b>
spec: {}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify a name for the Node Problem Detector.</p>
</li>
<li>
<p>Specify <code>openshift-node-problem-detector</code> as the namespace.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a Node Problem Detector RBAC (RBAC):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-problem-detector-operator
  namespace: openshift-node-problem-detector
rules:
- apiGroups:
  - node-problem-detector.operator.k8s.io
  resources:
  - "*"
  verbs:
  - "*"
- apiGroups:
  - ""
  resources:
  - pods
  - events
  - configmaps
  - secrets
  - services
  - endpoints
  - serviceaccounts
  verbs:
  - "*"
- apiGroups:
  - apps
  resources:
  - daemonsets
  verbs:
  - "*"

--

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-problem-detector-operator
  namespace: openshift-node-problem-detector
subjects:
- kind: ServiceAccount
  name: node-problem-detector-operator
roleRef:
  kind: Role
  name: node-problem-detector-operator
  apiGroup: rbac.authorization.k8s.io

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openshift-node-problem-detector-operator
rules:
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  # the operator needs to be able to bind the cluster role
  # system:node-problem-detector to the node-problem-detector service account
  - clusterrolebindings
  verbs:
  - "*"
- apiGroups:
  - security.openshift.io
  resources:
  # the operator needs to be able to add the node-problem-detector service account
  # to the list of accounts that can use the privileged SCC
  - securitycontextconstraints
  verbs:
  - "*"

---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openshift-node-problem-detector-operator-1
subjects:
- kind: ServiceAccount
  name: node-problem-detector-operator
  namespace: openshift-node-problem-detector
roleRef:
  kind: ClusterRole
  name: openshift-node-problem-detector-operator
  apiGroup: rbac.authorization.k8s.io</code></pre>
</div>
</div>
</li>
<li>
<p>Create the objects:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f deploy/rbac.yaml
$ oc create -f deploy/operator.yaml
$ oc create -f deploy/cr.yaml</pre>
</div>
</div>
</li>
<li>
<p>Create a Node Problem Detector custom resource (CR):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: node-problem-detector.operator.k8s.io/v1alpha1
kind: NodeProblemDetector
metadata:
  name: node-problem-detector
namespace: openshift-node-problem-detector</code></pre>
</div>
</div>
</li>
<li>
<p>Configure the Node Problem Detector policy as needed and click <strong>Create</strong>.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Click <strong>Create</strong>. This creates the Node Problem Detector Custom Resource, which you
can edit to make changes to the NPD.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-problem-detector-verifying-nodes-nodes-problem-detector">Verifying that the Node Problem Detector is Running</h4>
<div class="paragraph">
<p>You can use the steps in this section to verify that the Node Problem Detector is running and generate typical problems to ensure that the Node problem Detector responds properly.</p>
</div>
<div class="paragraph">
<div class="title">Prerequisite</div>
<p>The Problem Node Detector is installed in your OpenShift Enterprise cluster. The Node Problem Detector is installed by default or manually.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To verify that the Node Problem Detector is active:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Run the following command to get the name of the Problem Node Detector pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pods -n openshift-node-problem-detector

NAME                                            READY   STATUS    RESTARTS   AGE
node-problem-detector-2bl                       1/1     Running   0          9m54s
node-problem-detector-cfz                       1/1     Running   0          9m54s
node-problem-detector-operator-57f8-bd4w7       1/1     Running   0          20m
node-problem-detector-sg4                       0/1     Running   0          9m54s</pre>
</div>
</div>
</li>
<li>
<p>Run the following command to view log information on the Problem Node Detector pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs -n openshift-node-problem-detector &lt;pod_name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>The output should be similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs -n openshift-node-problem-detector node-problem-detector-c6kng
I0416 23:22:00.641354       1 log_monitor.go:63] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:kernel] LogPath:/host/log/journal Lookback:5m} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock}]</pre>
</div>
</div>
</li>
<li>
<p>Test the Node Problem Detector by simulating an event on the node:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ echo "kernel: divide error: 0000 [#0] SMP." &gt;&gt; /dev/kmsg</pre>
</div>
</div>
</li>
<li>
<p>Test the Node Problem Detector by simulating a condition on the node:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ echo "kernel: task blocked for more than 300 seconds." &gt;&gt; /dev/kmsg</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-problem-detector-customizing-nodes-nodes-problem-detector">Customizing Node Problem Detector conditions</h4>
<div class="paragraph">
<p>You can configure the Node Problem Detector to watch for any log string by editing the Node Problem Detector custom resource (CR).</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The Node Problem Detector Operator must be installed.</p>
</li>
<li>
<p>If needed, get the name of the Node Problem Detector CR:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get NodeProblemDetector
NAME                    AGE
node-problem-detector   6m6s</pre>
</div>
</div>
</li>
<li>
<p>Set the Node Problem Detector to the unmanaged state. In managed state, the Node Problem Detector Operator reverts changes made to the problem node detector configuration map.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To modify the Node Problem Detector:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open the Node Problem Detector CR for editing.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc edit problem-node-detector &lt;node&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc edit problem-node-detector problem-node-detector

apiVersion: node-problem-detector.operator.k8s.io/v1alpha1
kind: NodeProblemDetector
metadata:
  creationTimestamp: 2019-03-04T00:18:48Z
  generation: 1
  name: node-problem-detector
  namespace: default
  resourceVersion: "47179"
  selfLink: /apis/node-problem-detector.operator.k8s.io/v1alpha1/namespaces/default/nodeproblemdetectors/node-problem-detector
  uid: 14acef47-3e13-11e9-a640-0a4ad769663a
namespace: openshift-node-problem-detector</pre>
</div>
</div>
</li>
<li>
<p>Change the parameters and values as needed:</p>
<div class="listingblock">
<div class="title">Sample Node Problem Detector Configuration Map</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  kernel-monitor.json: |  <b class="conum">(1)</b>
    {
        "plugin": "journald", <b class="conum">(2)</b>
        "pluginConfig": {
                "source": "kernel"
        },
        "logPath": "/host/log/journal", <b class="conum">(3)</b>
        "lookback": "5m",
        "bufferSize": 10,
        "source": "kernel-monitor",
        "conditions": [                 <b class="conum">(4)</b>
                {
                        "type": "KernelDeadlock", <b class="conum">(5)</b>
                        "reason": "KernelHasNoDeadlock", <b class="conum">(6)</b>
                        "message": "kernel has no deadlock"  <b class="conum">(7)</b>
                }
        ],
        "rules": [                         <b class="conum">(8)</b>
                {
                        "type": "temporary",
                        "reason": "OOMKilling",
                        "pattern": "Kill process \\d+ (.+) score \\d+ or sacrifice child\\nKilled process \\d+ (.+) total-vm:\\d+kB, anon-rss:\\d+kB, file-rss:\\d+kB"
                },
                {
                        "type": "temporary",
                        "reason": "TaskHung",
                        "pattern": "task \\S+:\\w+ blocked for more than \\w+ seconds\\."
                },
                {
                        "type": "temporary",
                        "reason": "UnregisterNetDevice",
                        "pattern": "unregister_netdevice: waiting for \\w+ to become free. Usage count = \\d+"
                },
                {
                        "type": "temporary",
                        "reason": "KernelOops",
                        "pattern": "BUG: unable to handle kernel NULL pointer dereference at .*"
                },
                {
                        "type": "temporary",
                        "reason": "KernelOops",
                        "pattern": "divide error: 0000 \\[#\\d+\\] SMP"
                },
                {
                        "type": "permanent",
                        "condition": "KernelDeadlock",
                        "reason": "AUFSUmountHung",
                        "pattern": "task umount\\.aufs:\\w+ blocked for more than \\w+ seconds\\."
                },
        ]
    }

  kubelet-monitor.json: |-
    {
        "plugin": "custom",
        "pluginConfig": {
            "invoke_interval": "120s",
            "timeout": "60s",
            "concurrency": 1
        },
        "source": "kubelet-custom-plugin-monitor",
        "conditions": [{
            "type": "KubeletProblem",
            "reason": "KubeletIsUp",
            "message": "kubelet is up"
        }],
        "rules": [{
                "type": "temporary",
                "reason": "KubeletIsDown",
                "path": "/etc/npd-plugins/kubelet-health.sh",
                "timeout": "30s"
            },
            {
                "type": "permanent",
                "condition": "KubeletProblem",
                "reason": "KubeletIsDown",
                "path": "/etc/npd-plugins/kubelet-health.sh",
                "timeout": "45s"
            }
        ]
    }</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Rules and conditions that apply to container images.</p>
</li>
<li>
<p>Monitoring services, in a comma-separated list.</p>
</li>
<li>
<p>Path to the monitoring service log.</p>
</li>
<li>
<p>List of events to be monitored.</p>
</li>
<li>
<p>Label to indicate the error is an event (<code>temporary</code>) or NodeCondition (<code>permanent</code>).</p>
</li>
<li>
<p>Text message to describe the error.</p>
</li>
<li>
<p>Error message that the Node Problem Detector watches for.</p>
</li>
<li>
<p>Rules and conditions that apply to the kernel.</p>
</li>
</ol>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Optionally, you can add new node conditions or events:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">{
       "type": &lt;`temporary` or `permanent`&gt;,
       "reason": &lt;free-form text describing the error&gt;,
       "pattern": &lt;log message to watch for&gt;
},</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">{
       "type": "temporary",
       "reason": "UnregisterNetDevice",
       "pattern": "unregister_netdevice: waiting for \\w+ to become free. Usage count = \\d+"
},</code></pre>
</div>
</div>
</li>
<li>
<p>To display Node Problem Detector output to standard output (stdout) and standard error (stderr)
add the following to the configuration map:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  template:
    spec:
      containers:
      - name: node-problem-detector
        command:
        - node-problem-detector
        - --alsologtostderr=true <b class="conum">(1)</b>
        - --log_dir="/tmp" <b class="conum">(2)</b>
        - --system-log-monitors=/etc/npd/kernel-monitor.json <b class="conum">(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Sends the output to standard output (stdout).</p>
</li>
<li>
<p>Path to the error log.</p>
</li>
<li>
<p>Comma-separated path to the plug-in configuration files.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-nodes-problem-detector-uninstalling-nodes-nodes-problem-detector">Uninstall the Node Problem Detector</h4>
<div class="paragraph">
<p>You can use the OpenShift Enterprise console to uninstall the Node Problem Detector.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To uninstall the Node Problem Detector:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the OpenShift Enterprise console, click <strong>Operator Management</strong> &#8594; <strong>Operator Subscriptions</strong>.</p>
</li>
<li>
<p>Change to the <strong>node-problem-detector</strong> project.</p>
</li>
<li>
<p>On the far right side of the <strong>node-problem-detector</strong> listing, select <strong>Remove Subscription</strong> from the menu.</p>
</li>
<li>
<p>In the OpenShift Enterprise console, click <strong>Catalog</strong> &#8594; <strong>Installed Operators</strong>.</p>
</li>
<li>
<p>On the far right side of the <strong>Node Problem Detector</strong> listing, select <strong>Delete Cluster Service Version</strong> from the menu.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_working-with-containers">Working with containers</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="using-containers">Understanding Containers in OpenShift Enterprise</h3>
<div class="paragraph">
<p>The basic units of OpenShift Enterprise applications are called <em>containers</em>.
<a href="https://access.redhat.com/articles/1353593">Linux container technologies</a>
are lightweight mechanisms for isolating running processes so that they are
limited to interacting with only their designated resources.</p>
</div>
<div class="paragraph">
<p>Many application instances can be running in containers on a single host without
visibility into each others' processes, files, network, and so on. Typically,
each container provides a single service (often called a "micro-service"), such
as a web server or a database, though containers can be used for arbitrary
workloads.</p>
</div>
<div class="paragraph">
<p>The Linux kernel has been incorporating capabilities for container technologies
for years. OpenShift Enterprise and
Kubernetes add the ability to orchestrate containers across
multi-host installations.</p>
</div>
</div>
<div class="sect2">
<h3 id="using-init-containers-to-perform-tasks-before-a-pod-is-deployed">Using Init Containers to perform tasks before a pod is deployed</h3>
<div class="paragraph">
<p>OpenShift Enterprise provides <em>Init Containers</em>, which are specialized containers
that run before application containers and can contain utilities or setup scripts not present in an app image.</p>
</div>
<div class="sect3">
<h4 id="nodes-containers-init-about-nodes-containers-init">Understanding Init Containers</h4>
<div class="paragraph">
<p>You can use an Init Container resource to perform tasks before the rest of a pod is deployed.</p>
</div>
<div class="paragraph">
<p>A pod can have Init Containers in addition to application containers. Init
containers allow you to reorganize setup scripts and binding code.</p>
</div>
<div class="paragraph">
<p>An Init Container can:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Contain and run utilities that are not desirable to include in the app Container image for security reasons.</p>
</li>
<li>
<p>Contain utilities or custom code for setup that is not present in an app image. For example, there is no need to make an image FROM another image just to use a tool like sed, awk, python, or dig during setup.</p>
</li>
<li>
<p>Use Linux namespaces so that they have different filesystem views from app containers, such as access to Secrets that application containers are not able to access.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Each Init Container must complete successfully before the next one is started. So, Init Containers provide an easy way to block or delay the startup of app containers until some set of preconditions are met.</p>
</div>
<div class="paragraph">
<p>For example, the following are some ways you can use Init Containers:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Wait for a service to be created with a shell command like:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1</pre>
</div>
</div>
</li>
<li>
<p>Register this Pod with a remote server from the downward API with a command like:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d instance=$()&amp;ip=$()</pre>
</div>
</div>
</li>
<li>
<p>Wait for some time before starting the app Container with a command like <code>sleep 60</code>.</p>
</li>
<li>
<p>Clone a git repository into a volume.</p>
</li>
<li>
<p>Place values into a configuration file and run a template tool to dynamically generate a configuration file for the main app Container. For example, place the POD_IP value in a configuration and generate the main app configuration file using Jinja.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>See the <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">Kubernetes documentation</a> for more information.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-init-creating-nodes-containers-init">Creating Init Containers</h4>
<div class="paragraph">
<p>The following example outlines a simple Pod which has two Init Containers. The first waits for <code>myservice</code> and the second waits for <code>mydb</code>. Once both containers complete, the Pod begins.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a YAML file for the Init Container:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']</code></pre>
</div>
</div>
</li>
<li>
<p>Create a YAML file for the <code>myservice</code> service.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: Service
apiVersion: v1
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376</code></pre>
</div>
</div>
</li>
<li>
<p>Create a YAML file for the <code>mydb</code> service.</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">kind: Service
apiVersion: v1
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377</pre>
</div>
</div>
</li>
<li>
<p>Run the following command to create the <code>myapp-pod</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ create -f myapp.yaml

pod/myapp-pod created</pre>
</div>
</div>
</li>
<li>
<p>View the status of the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pods
NAME                          READY     STATUS              RESTARTS   AGE
myapp-pod                     0/1       Init:0/2            0          5s</pre>
</div>
</div>
<div class="paragraph">
<p>Note that the pod status indicates it is waiting</p>
</div>
</li>
<li>
<p>Run the following commands to create the services:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f mydb.yaml
$ oc create -f myservice.yaml</pre>
</div>
</div>
</li>
<li>
<p>View the status of the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pods
NAME                          READY     STATUS              RESTARTS   AGE
myapp-pod                     1/1       Running             0          2m</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="using-volumes-to-persist-container-data">Using volumes to persist container data</h3>
<div class="paragraph">
<p>Files in a container are ephemeral. As such, when a container crashes or stops, the data is lost.
You can use <em>volumes</em> to persist the data used by the containers in a pod. A volume is directory, accessible to the Containers in a Pod, where data is stored for the life of the pod.</p>
</div>
<div class="sect3">
<h4 id="nodes-containers-volumes-about-nodes-containers-volumes">Understanding volumes</h4>
<div class="paragraph">
<p>Volumes are mounted file systems available to pods and their
containers which may be backed by a number of host-local or network attached
storage endpoints. Containers are not persistent by default; on restart, their contents are
cleared.</p>
</div>
<div class="paragraph">
<p>To ensure that the file system on the volume contains no errors and, if errors
are present, to repair them when possible, OpenShift Enterprise invokes the <code>fsck</code>
utility prior to the <code>mount</code> utility. This occurs when either adding a volume or
updating an existing volume.</p>
</div>
<div class="paragraph">
<p>The simplest volume type is <code>emptyDir</code>, which is a temporary directory on a
single machine. Administrators may also allow you to request a persistent volume that is automatically attached
to your pods.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p><code>emptyDir</code> volume storage may be restricted by a quota based on the pod&#8217;s
FSGroup, if the FSGroup parameter is enabled by your cluster administrator.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-volumes-cli-nodes-containers-volumes">Working with volumes using the OpenShift Enterprise CLI</h4>
<div class="paragraph">
<p>You can use the CLI command <code>oc set volume</code> to add and remove volumes and
volume mounts for any object that has a pod template like replication controllers or
DeploymentConfigs. You can also list volumes in pods or any
object that has a pod template.</p>
</div>
<div class="paragraph">
<p>The <code>oc set volume</code> command uses the following general syntax:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume &lt;object_selection&gt; &lt;operation&gt; &lt;mandatory_parameters&gt; &lt;options&gt;</pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Object selection</dt>
<dd>
<p>Specify one of the following for <code>object_seletion</code> in the <code>oc set volume</code> command:</p>
</dd>
</dl>
</div>
<table id="vol-object-selection-nodes-containers-volumes" class="tableblock frame-all grid-all stretch">
<caption class="title">Table 5. Object Selection</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Syntax</th>
<th class="tableblock halign-left valign-top">Description</th>
<th class="tableblock halign-left valign-top">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><em>&lt;object_type&gt;</em> <em>&lt;name&gt;</em></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Selects <code><em>&lt;name&gt;</em></code> of type <code><em>&lt;object_type&gt;</em></code>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>deploymentConfig registry</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><em>&lt;object_type&gt;</em>/<em>&lt;name&gt;</em></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Selects <code><em>&lt;name&gt;</em></code> of type <code><em>&lt;object_type&gt;</em></code>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>deploymentConfig/registry</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><em>&lt;object_type&gt;</em></code>
<code>--selector=<em>&lt;object_label_selector&gt;</em></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Selects resources of type <code><em>&lt;object_type&gt;</em></code> that matched the given label
selector.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>deploymentConfig</code>
<code>--selector="name=registry"</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><em>&lt;object_type&gt;</em> --all</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Selects all resources of type <code><em>&lt;object_type&gt;</em></code>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>deploymentConfig --all</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-f</code> or
<code>--filename=<em>&lt;file_name&gt;</em></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">File name, directory, or URL to file to use to edit the resource.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-f registry-deployment-config.json</code></p></td>
</tr>
</tbody>
</table>
<div class="dlist">
<dl>
<dt class="hdlist1">Operation</dt>
<dd>
<p>Specify <code>--add</code>, <code>--remove</code>, or <code>--list</code> for <code>operation</code> in the <code>oc set volume</code> command.</p>
</dd>
<dt class="hdlist1">Mandatory parameters</dt>
<dd>
<p>Any <code><em>&lt;mandatory_parameters&gt;</em></code> are specific to the
selected operation and are discussed in later sections.</p>
</dd>
<dt class="hdlist1">Options</dt>
<dd>
<p>Any <code><em>&lt;options&gt;</em></code> are specific to the
selected operation and are discussed in later sections.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-volumes-listing-nodes-containers-volumes">Listing volumes and volume mounts in a pod</h4>
<div class="paragraph">
<p>You can list volumes and volume mounts in pods or pod templates:</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To list volumes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume &lt;object_type&gt;/&lt;name&gt; --list [options]</pre>
</div>
</div>
<div class="paragraph">
<p>List volume supported options:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Option</th>
<th class="tableblock halign-left valign-top">Description</th>
<th class="tableblock halign-left valign-top">Default</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of the volume.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-c, --containers</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Select containers by name. It can also take wildcard <code>'*'</code> that matches any
character.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>'*'</code></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To list all volumes for pod <strong>p1</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume pod/p1 --list</pre>
</div>
</div>
</li>
<li>
<p>To list volume <strong>v1</strong> defined on all DeploymentConfigs:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume dc --all --name=v1</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-volumes-adding-nodes-containers-volumes">Adding volumes to a pod</h4>
<div class="paragraph">
<p>You can add volumes and volume mounts to a pod.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To add a volume, a volume mount, or both to pod templates:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume &lt;object_type&gt;/&lt;name&gt; --add [options]</pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 6. Supported Options for Adding Volumes</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Option</th>
<th class="tableblock halign-left valign-top">Description</th>
<th class="tableblock halign-left valign-top">Default</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of the volume.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Automatically generated, if not specified.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-t, --type</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of the volume source. Supported values: <code>emptyDir</code>, <code>hostPath</code>, <code>secret</code>,
<code>configmap</code>, <code>persistentVolumeClaim</code> or <code>projected</code>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>emptyDir</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-c, --containers</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Select containers by name. It can also take wildcard <code>'*'</code> that matches any
character.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>'*'</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-m, --mount-path</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mount path inside the selected containers.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--path</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Host path. Mandatory parameter for <code>--type=hostPath</code>.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--secret-name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of the secret. Mandatory parameter for <code>--type=secret</code>.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--configmap-name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of the configmap. Mandatory parameter for <code>--type=configmap</code>.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--claim-name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of the persistent volume claim. Mandatory parameter for
<code>--type=persistentVolumeClaim</code>.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--source</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Details of volume source as a JSON string. Recommended if the desired volume
source is not supported by <code>--type</code>.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-o, --output</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Display the modified objects instead of updating them on the server. Supported
values: <code>json</code>, <code>yaml</code>.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--output-version</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Output the modified objects with the given version.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>api-version</code></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To add a new volume source <strong>emptyDir</strong> to DeploymentConfig <strong>registry</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume dc/registry --add</pre>
</div>
</div>
</li>
<li>
<p>To add volume <strong>v1</strong> with secret <strong>$ecret</strong> for replication controller <strong>r1</strong> and mount
inside the containers at <strong><em>/data</em></strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume rc/r1 --add --name=v1 --type=secret --secret-name='$ecret' --mount-path=/data</pre>
</div>
</div>
</li>
<li>
<p>To add existing persistent volume <strong>v1</strong> with claim name <strong>pvc1</strong> to deployment
configuration <strong><em>dc.json</em></strong> on disk, mount the volume on container <strong>c1</strong> at
<strong><em>/data</em></strong>, and update the DeploymentConfig on the server:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume -f dc.json --add --name=v1 --type=persistentVolumeClaim \
  --claim-name=pvc1 --mount-path=/data --containers=c1</pre>
</div>
</div>
</li>
<li>
<p>To add a volume <strong>v1</strong> based on Git repository
<strong>https://github.com/namespace1/project1</strong> with revision <strong>5125c45f9f563</strong> for
all replication controllers:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume rc --all --add --name=v1 \
  --source='{"gitRepo": {
                "repository": "https://github.com/namespace1/project1",
                "revision": "5125c45f9f563"
            }}'</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-volumes-updating-nodes-containers-volumes">Updating volumes and volume mounts in a pod</h4>
<div class="paragraph">
<p>You can modify the volumes and volume mounts in a pod.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>Updating existing volumes using the <code>--overwrite</code> option:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume &lt;object_type&gt;/&lt;name&gt; --add --overwrite [options]</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To replace existing volume <strong>v1</strong> for replication controller <strong>r1</strong> with existing
persistent volume claim <strong>pvc1</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume rc/r1 --add --overwrite --name=v1 --type=persistentVolumeClaim --claim-name=pvc1</pre>
</div>
</div>
</li>
<li>
<p>To change DeploymentConfig <strong>d1</strong> mount point to <strong><em>/opt</em></strong> for volume <strong>v1</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume dc/d1 --add --overwrite --name=v1 --mount-path=/opt</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-volumes-removing-nodes-containers-volumes">Removing volumes and volume mounts from a pod</h4>
<div class="paragraph">
<p>You can remove a volume or volume mount from a pod.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To remove a volume from pod templates:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume &lt;object_type&gt;/&lt;name&gt; --remove [options]</pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 7. Supported Options for Removing Volumes</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Option</th>
<th class="tableblock halign-left valign-top">Description</th>
<th class="tableblock halign-left valign-top">Default</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of the volume.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-c, --containers</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Select containers by name. It can also take wildcard <code>'*'</code> that matches any character.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>'*'</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--confirm</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Indicate that you want to remove multiple volumes at once.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>-o, --output</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Display the modified objects instead of updating them on the server. Supported
values: <code>json</code>, <code>yaml</code>.</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--output-version</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Output the modified objects with the given version.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>api-version</code></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To remove a volume <strong>v1</strong> from DeploymentConfig <strong>d1</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume dc/d1 --remove --name=v1</pre>
</div>
</div>
</li>
<li>
<p>To unmount volume <strong>v1</strong> from container <strong>c1</strong> for DeploymentConfig <strong>d1</strong> and
remove the volume <strong>v1</strong> if it is not referenced by any containers on <strong>d1</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume dc/d1 --remove --name=v1 --containers=c1</pre>
</div>
</div>
</li>
<li>
<p>To remove all volumes for replication controller <strong>r1</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc set volume rc/r1 --remove --confirm</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-volumes-subpath-nodes-containers-volumes">Configuring volumes for multiple uses in a pod</h4>
<div class="paragraph">
<p>You can configure a volume to allows you to share one volume for
multiple uses in a single pod using the <code>volumeMounts.subPath</code> property to specify a <code>subPath</code> inside a volume
instead of the volume&#8217;s root.
``
.Procedure</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>View the list of files in the volume, run the <code>oc rsh</code> command:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc rsh &lt;pod&gt;
sh-4.2$ ls /path/to/volume/subpath/mount
example_file1 example_file2 example_file3</pre>
</div>
</div>
</li>
<li>
<p>Specify the <code>subPath</code>:</p>
<div class="listingblock">
<div class="title">Example subPath Usage</div>
<div class="content">
<pre class="nowrap">apiVersion: v1
kind: Pod
metadata:
  name: my-site
spec:
    containers:
    - name: mysql
      image: mysql
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql <b class="conum">(1)</b>
    - name: php
      image: php
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html <b class="conum">(2)</b>
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-site-data</pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Databases are stored in the <code>mysql</code> folder.</p>
</li>
<li>
<p>HTML content is stored in the <code>html</code> folder.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="mapping-volumes-using-projected-volumes">Mapping volumes using projected volumes</h3>
<div class="paragraph">
<p>A <em>projected volume</em> maps several existing volume sources into the same directory.</p>
</div>
<div class="paragraph">
<p>The following types of volume sources can be projected:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Secrets</p>
</li>
<li>
<p>Config Maps</p>
</li>
<li>
<p>Downward API</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>All sources are required to be in the same namespace as the pod.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="nodes-containers-projected-volumes-about-nodes-containers-projected-volumes">Understanding projected volumes</h4>
<div class="paragraph">
<p>Projected volumes can map any combination of these volume sources into a single directory, allowing the user to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>automatically populate a single volume with the keys from multiple secrets, configmaps, and with downward API information,
so that I can synthesize a single directory with various sources of information;</p>
</li>
<li>
<p>populate a single volume with the keys from multiple secrets, configmaps, and with downward API information,
explicitly specifying paths for each item, so that I can have full control over the contents of that volume.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following general scenarios show how you can use projected volumes.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>ConfigMap, Secrets, Downward API.</strong></dt>
<dd>
<p>Projected volumes allow you to deploy containers with configuration data that includes passwords.
An application using these resources could be deploying OpenStack on Kubernetes. The configuration data may need to be assembled differently depending on if the services are going to be used for production or for testing. If a pod is labeled with production or testing, the downward API selector <code>metadata.labels</code> can be used to produce the correct OpenStack configs.</p>
</dd>
<dt class="hdlist1"><strong>ConfigMap + Secrets.</strong></dt>
<dd>
<p>Projected volumes allow you to deploy containers involving configuration data and passwords.
For example, you might execute a configmap with some sensitive encrypted tasks that are decrypted using a vault password file.</p>
</dd>
<dt class="hdlist1"><strong>ConfigMap + Downward API.</strong></dt>
<dd>
<p>Projected volumes allow you to generate a config including the pod name (available via the <code>metadata.name</code> selector). This application can then pass the pod name along with requests in order to easily determine the source without using IP tracking.</p>
</dd>
<dt class="hdlist1"><strong>Secrets + Downward API.</strong></dt>
<dd>
<p>Projected volumes allow you to use a secret as a public key to encrypt the namespace of the pod (available via the <code>metadata.namespace</code> selector).
This example allows the operator to use the application to deliver the namespace information securely without using an encrypted transport.</p>
</dd>
</dl>
</div>
<div class="sect4">
<h5 id="projected-volumes-examples-nodes-containers-projected-volumes">Example Pod Specifications</h5>
<div class="paragraph">
<p>The following are examples of pod specifications for creating projected volumes.</p>
</div>
<div class="listingblock">
<div class="title">Pod with a secret, a downward API, and a configmap</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts: <b class="conum">(1)</b>
    - name: all-in-one
      mountPath: "/projected-volume"<b class="conum">(2)</b>
      readOnly: true <b class="conum">(3)</b>
  volumes: <b class="conum">(4)</b>
  - name: all-in-one <b class="conum">(5)</b>
    projected:
      defaultMode: 0400 <b class="conum">(6)</b>
      sources:
      - secret:
          name: mysecret <b class="conum">(7)</b>
          items:
            - key: username
              path: my-group/my-username <b class="conum">(8)</b>
      - downwardAPI: <b class="conum">(9)</b>
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "cpu_limit"
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
      - configMap: <b class="conum">(10)</b>
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
              mode: 0777 <b class="conum">(11)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Add a <code>volumeMounts</code> section for each container that needs the secret.</p>
</li>
<li>
<p>Specify a path to an unused directory where the secret will appear.</p>
</li>
<li>
<p>Set <code>readOnly</code> to <code>true</code>.</p>
</li>
<li>
<p>Add a <code>volumes</code> block to list each projected volume source.</p>
</li>
<li>
<p>Specify any name for the volume.</p>
</li>
<li>
<p>Set the execute permission on the files.</p>
</li>
<li>
<p>Add a secret. Enter the name of the secret object. Each secret you want to use must be listed.</p>
</li>
<li>
<p>Specify the path to the secrets file under the <code>mountPath</code>. Here, the secrets file is in <strong><em>/projected-volume/my-group/my-config</em></strong>.</p>
</li>
<li>
<p>Add a Downward API source.</p>
</li>
<li>
<p>Add a ConfigMap source.</p>
</li>
<li>
<p>Set the mode for the specific projection</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>If there are multiple containers in the pod, each container needs a <code>volumeMounts</code> section, but only one <code>volumes</code> section is needed.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="title">Pod with multiple secrets with a non-default permission mode set</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      defaultMode: 0755
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - secret:
          name: mysecret2
          items:
            - key: password
              path: my-group/my-password
              mode: 511</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>defaultMode</code> can only be specified at the projected level and not for each
volume source. However, as illustrated above, you can explicitly set the <code>mode</code>
for each individual projection.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect4">
<h5 id="projected-volumes-pathing-nodes-containers-projected-volumes">Pathing Considerations</h5>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>Collisions Between Keys when Configured Paths are Identical</strong></dt>
<dd>
<p>If you configure any keys with the same path, the pod spec will not be accepted as valid.
In the following example, the specified path for <code>mysecret</code> and <code>myconfigmap</code> are the same:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/data
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/data</code></pre>
</div>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Consider the following situations related to the volume file paths.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>Collisions Between Keys without Configured Paths</strong></dt>
<dd>
<p>The only run-time validation that can occur is when all the paths are known at pod creation, similar to the above scenario. Otherwise, when a conflict occurs the most recent specified resource will overwrite anything preceding it
(this is true for resources that are updated after pod creation as well).</p>
</dd>
<dt class="hdlist1"><strong>Collisions when One Path is Explicit and the Other is Automatically Projected</strong></dt>
<dd>
<p>In the event that there is a collision due to a user specified path matching data that is automatically projected,
the latter resource will overwrite anything preceding it as before</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-projected-volumes-creating-nodes-containers-projected-volumes">Configuring a Projected Volume for a Pod</h4>
<div class="paragraph">
<p>When creating projected volumes, consider the volume file path situations described in <em>Understanding projected vol`umes</em>.</p>
</div>
<div class="paragraph">
<p>The following example shows how to use a projected volume to mount an existing Secret volume source. The steps can be used to create a user name and password Secrets from local files. You then create a pod that runs one container, using a projected volume to mount the Secrets into the same shared directory.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To use a projected volume to mount an existing Secret volume source.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create files containing the secrets, entering the following, replacing the password and user information as appropriate:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  pass: MWYyZDFlMmU2N2Rm
  user: YWRtaW4=</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>user</code> and <code>pass</code> values can be any valid string that is <strong>base64</strong> encoded.
The examples used here are base64 encoded values <code>user: admin</code>, <code>pass:1f2d1e2e67df</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ echo -n "admin" | base64
YWRtaW4=
$ echo -n "1f2d1e2e67df" | base64
MWYyZDFlMmU2N2Rm</pre>
</div>
</div>
</li>
<li>
<p>Use the following command to create the secrets:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;secrets-filename&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f secret.yaml
secret "mysecret" created</pre>
</div>
</div>
</li>
<li>
<p>You can check that the secret was created using the following commands:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get secret &lt;secret-name&gt;
$ oc get secret &lt;secret-name&gt; -o yaml</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get secret mysecret
NAME       TYPE      DATA      AGE
mysecret   Opaque    2         17h</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get secret mysecret -o yaml</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
data:
  pass: MWYyZDFlMmU2N2Rm
  user: YWRtaW4=
kind: Secret
metadata:
  creationTimestamp: 2017-05-30T20:21:38Z
  name: mysecret
  namespace: default
  resourceVersion: "2107"
  selfLink: /api/v1/namespaces/default/secrets/mysecret
  uid: 959e0424-4575-11e7-9f97-fa163e4bd54c
type: Opaque</code></pre>
</div>
</div>
</li>
<li>
<p>Create a pod configuration file similar to the following that includes a <code>volumes</code> section:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: test-projected-volume
spec:
  containers:
  - name: test-projected-volume
    image: busybox
    args:
    - sleep
    - "86400"
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:      <b class="conum">(1)</b>
          name: user
      - secret:      <b class="conum">(1)</b>
          name: pass</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The name of the secret you created.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the pod from the configuration file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;your_yaml_file&gt;.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f secret-pod.yaml
pod "test-projected-volume" created</pre>
</div>
</div>
</li>
<li>
<p>Verify that the pod container is running, and then watch for changes to
the Pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pod &lt;name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>The output should appear similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pod test-projected-volume
NAME                    READY     STATUS    RESTARTS   AGE
test-projected-volume   1/1       Running   0          14s</pre>
</div>
</div>
</li>
<li>
<p>In another terminal, use the <code>oc exec</code> command to open a shell to the running container:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc exec -it &lt;pod&gt; &lt;command&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc exec -it test-projected-volume -- /bin/sh</pre>
</div>
</div>
</li>
<li>
<p>In your shell, verify that the <code>projected-volumes</code> directory contains your projected sources:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">/ # ls
bin               home              root              tmp
dev               proc              run               usr
etc               projected-volume  sys               var</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="allowing-containers-to-consume-api-objects">Allowing containers to consume API objects</h3>
<div class="paragraph">
<p>The <em>Downward API</em> is a mechanism that allows containers to consume information
about API objects without coupling to OpenShift Container Platform.
Such information includes the pods name, namespace, and resource values.
Containers can consume information from the downward API using environment
variables or a volume plug-in.</p>
</div>
<div class="sect3">
<h4 id="nodes-containers-projected-volumes-about-nodes-containers-downward-api">Expose Pod information to Containers using the Downward API</h4>
<div class="paragraph">
<p>The Downward API contains such information as the pod&#8217;s name, project, and resource values. Containers can consume
information from the downward API using environment variables or a volume
plug-in.</p>
</div>
<div class="paragraph">
<p>Fields within the pod are selected using the <code>FieldRef</code> API type. <code>FieldRef</code>
has two fields:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>fieldPath</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The path of the field to select, relative to the pod.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>apiVersion</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The API version to interpret the <code>fieldPath</code> selector within.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Currently, the valid selectors in the v1 API include:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Selector</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>metadata.name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The pod&#8217;s name. This is supported in both environment variables and volumes.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>metadata.namespace</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The pod&#8217;s namespace.This is supported in both environment variables and volumes.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>metadata.labels</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The pod&#8217;s labels. This is only supported in volumes and not in environment variables.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>metadata.annotations</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The pod&#8217;s annotations. This is only supported in volumes and not in environment variables.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>status.podIP</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The pod&#8217;s IP. This is only supported in environment variables and not volumes.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The <code>apiVersion</code> field, if not specified, defaults to the API version of the
enclosing pod template.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-downward-api-container-values-nodes-containers-downward-api">Understanding how to consume container values using the downward API</h4>
<div class="paragraph">
<p>You containers can consume API values using environment variables or a volume plug-in.
Depending on the method you choose, containers can consume:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Pod name</p>
</li>
<li>
<p>Pod project/namespace</p>
</li>
<li>
<p>Pod annotations</p>
</li>
<li>
<p>Pod labels</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Annotations and labels are available using only a volume plug-in.</p>
</div>
<div class="sect4">
<h5 id="nodes-containers-downward-api-container-values-envars-nodes-containers-downward-api">Consuming container values using environment variables</h5>
<div class="paragraph">
<p>When using a container&#8217;s environment variables, use the <code><strong>EnvVar</strong></code> type&#8217;s <code><strong>valueFrom</strong></code> field (of type <code><strong>EnvVarSource</strong></code>)
to specify that the variable&#8217;s value should come from a <code><strong>FieldRef</strong></code>
source instead of the literal value specified by the <code><strong>value</strong></code> field.</p>
</div>
<div class="paragraph">
<p>Only constant attributes of the pod can be consumed this way, as environment
variables cannot be updated once a process is started in a way that allows the
process to be notified that the value of a variable has changed. The fields
supported using environment variables are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Pod name</p>
</li>
<li>
<p>Pod project/namespace</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To use environment variables</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a <code><strong><em>pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  restartPolicy: Never</code></pre>
</div>
</div>
</li>
<li>
<p>Create the pod from the <code><strong><em>pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f pod.yaml</pre>
</div>
</div>
</li>
<li>
<p>Check the container&#8217;s logs for the <code><strong>MY_POD_NAME</strong></code> and <code><strong>MY_POD_NAMESPACE</strong></code>
values:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs -p dapi-env-test-pod</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="nodes-containers-downward-api-container-values-plugin-nodes-containers-downward-api">Consuming container values using a volume plug-in</h5>
<div class="paragraph">
<p>You containers can consume API values using a volume plug-in.</p>
</div>
<div class="paragraph">
<p>Containers can consume:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Pod name</p>
</li>
<li>
<p>Pod project/namespace</p>
</li>
<li>
<p>Pod annotations</p>
</li>
<li>
<p>Pod labels</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To use the volume plug-in:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a <code><strong><em>volume-pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">kind: Pod
apiVersion: v1
metadata:
  labels:
    zone: us-east-coast
    cluster: downward-api-test-cluster1
    rack: rack-123
  name: dapi-volume-test-pod
  annotations:
    annotation1: "345"
    annotation2: "456"
spec:
  containers:
    - name: volume-test-container
      image: gcr.io/google_containers/busybox
      command: ["sh", "-c", "cat /tmp/etc/pod_labels /tmp/etc/pod_annotations"]
      volumeMounts:
        - name: podinfo
          mountPath: /tmp/etc
          readOnly: false
  volumes:
  - name: podinfo
    downwardAPI:
      defaultMode: 420
      items:
      - fieldRef:
          fieldPath: metadata.name
        path: pod_name
      - fieldRef:
          fieldPath: metadata.namespace
        path: pod_namespace
      - fieldRef:
          fieldPath: metadata.labels
        path: pod_labels
      - fieldRef:
          fieldPath: metadata.annotations
        path: pod_annotations
  restartPolicy: Never</code></pre>
</div>
</div>
</li>
<li>
<p>Create the pod from the <code><strong><em>volume-pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f volume-pod.yaml</pre>
</div>
</div>
</li>
<li>
<p>Check the container&#8217;s logs and verify the presence of the configured fields:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs -p dapi-volume-test-pod
cluster=downward-api-test-cluster1
rack=rack-123
zone=us-east-coast
annotation1=345
annotation2=456
kubernetes.io/config.source=api</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-downward-api-container-resources-api-nodes-containers-downward-api">Understanding how to consume container resources using the downward API</h4>
<div class="paragraph">
<p>When creating pods, you can use the downward API to inject information about
computing resource requests and limits so that image and application authors can
correctly create an image for specific environments.</p>
</div>
<div class="paragraph">
<p>You can do this using environment variable or a volume plug-in.</p>
</div>
<div class="sect4">
<h5 id="nodes-containers-downward-api-container-resources-envars-nodes-containers-downward-api">Consuming container resources using environment variables</h5>
<div class="paragraph">
<p>When creating pods, you can use the downward API to inject information about
computing resource requests and limits using environment variables.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To use environment variables:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>When creating a pod configuration, specify environment variables that
correspond to the contents of the <code><strong>resources</strong></code> field in the <code><strong>spec.container</strong></code>
field:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">....
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox:1.24
      command: [ "/bin/sh", "-c", "env" ]
      resources:
        requests:
          memory: "32Mi"
          cpu: "125m"
        limits:
          memory: "64Mi"
          cpu: "250m"
      env:
        - name: MY_CPU_REQUEST
          valueFrom:
            resourceFieldRef:
              resource: requests.cpu
        - name: MY_CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: MY_MEM_REQUEST
          valueFrom:
            resourceFieldRef:
              resource: requests.memory
        - name: MY_MEM_LIMIT
          valueFrom:
            resourceFieldRef:
              resource: limits.memory
....</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the resource limits are not included in the container configuration, the
downward API defaults to the node&#8217;s CPU and memory allocatable values.</p>
</div>
</li>
<li>
<p>Create the pod from the <code><strong><em>pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f pod.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="nodes-containers-downward-api-container-resources-plugin-nodes-containers-downward-api">Consuming container resources using a volume plug-in</h5>
<div class="paragraph">
<p>When creating pods, you can use the downward API to inject information about
computing resource requests and limits using a volume plug-in.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To use the Volume Plug-in:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>When creating a pod configuration, use the <code><strong>spec.volumes.downwardAPI.items</strong></code>
field to describe the desired resources that correspond to the
<code><strong>spec.resources</strong></code> field:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">....
spec:
  containers:
    - name: client-container
      image: gcr.io/google_containers/busybox:1.24
      command: ["sh", "-c", "while true; do echo; if [[ -e /etc/cpu_limit ]]; then cat /etc/cpu_limit; fi; if [[ -e /etc/cpu_request ]]; then cat /etc/cpu_request; fi; if [[ -e /etc/mem_limit ]]; then cat /etc/mem_limit; fi; if [[ -e /etc/mem_request ]]; then cat /etc/mem_request; fi; sleep 5; done"]
      resources:
        requests:
          memory: "32Mi"
          cpu: "125m"
        limits:
          memory: "64Mi"
          cpu: "250m"
      volumeMounts:
        - name: podinfo
          mountPath: /etc
          readOnly: false
  volumes:
    - name: podinfo
      downwardAPI:
        items:
          - path: "cpu_limit"
            resourceFieldRef:
              containerName: client-container
              resource: limits.cpu
          - path: "cpu_request"
            resourceFieldRef:
              containerName: client-container
              resource: requests.cpu
          - path: "mem_limit"
            resourceFieldRef:
              containerName: client-container
              resource: limits.memory
          - path: "mem_request"
            resourceFieldRef:
              containerName: client-container
              resource: requests.memory
....</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the resource limits are not included in the container configuration, the
downward API defaults to the node&#8217;s CPU and memory allocatable values.</p>
</div>
</li>
<li>
<p>Create the pod from the <code><strong><em>volume-pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f volume-pod.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-downward-api-container-secrets-nodes-containers-downward-api">Consuming secrets using the downward API</h4>
<div class="paragraph">
<p>When creating pods, you can use the downward API to inject Secrets
so image and application authors can create an image
for specific environments.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a <strong><em>secret.yaml</em></strong> file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
  name: mysecret
data:
  password: cGFzc3dvcmQ=
  username: ZGV2ZWxvcGVy
type: kubernetes.io/basic-auth</code></pre>
</div>
</div>
</li>
<li>
<p>Create a <code>Secret</code> from the secret.yaml file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f secret.yaml</pre>
</div>
</div>
</li>
<li>
<p>Create a <code><strong><em>pod.yaml</em></strong></code> file that references the <code>username</code> field from the above <code>Secret</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_SECRET_USERNAME
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: username
  restartPolicy: Never</code></pre>
</div>
</div>
</li>
<li>
<p>Create the pod from the <code><strong><em>pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f pod.yaml</pre>
</div>
</div>
</li>
<li>
<p>Check the container&#8217;s logs for the <code><strong>MY_SECRET_USERNAME</strong></code> value:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs -p dapi-env-test-pod</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-downward-api-container-configmaps-nodes-containers-downward-api">Consuming configuration maps using the downward API</h4>
<div class="paragraph">
<p>When creating pods, you can use the downward API to inject configuration map values
so image and application authors can create an image for specific environments.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a <code><strong><em>configmap.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: myconfigmap
data:
  mykey: myvalue</code></pre>
</div>
</div>
</li>
<li>
<p>Create a <code>ConfigMap</code> from the <code><strong><em>configmap.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f configmap.yaml</pre>
</div>
</div>
</li>
<li>
<p>Create a <code><strong><em>pod.yaml</em></strong></code> file that references the above <code>ConfigMap</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_CONFIGMAP_VALUE
          valueFrom:
            configMapKeyRef:
              name: myconfigmap
              key: mykey
  restartPolicy: Always</code></pre>
</div>
</div>
</li>
<li>
<p>Create the pod from the <code><strong><em>pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f pod.yaml</pre>
</div>
</div>
</li>
<li>
<p>Check the container&#8217;s logs for the <code><strong>MY_CONFIGMAP_VALUE</strong></code> value:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs -p dapi-env-test-pod</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-downward-api-container-envars-nodes-containers-downward-api">Referencing environment variables</h4>
<div class="paragraph">
<p>When creating pods, you can reference the value of a previously defined
environment variable by using the <code>$()</code> syntax. If the environment variable
reference can not be resolved, the value will be left as the provided
string.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a <code><strong><em>pod.yaml</em></strong></code> file that references an existing <code>environment variable</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_EXISTING_ENV
          value: my_value
        - name: MY_ENV_VAR_REF_ENV
          value: $(MY_EXISTING_ENV)
  restartPolicy: Never</code></pre>
</div>
</div>
</li>
<li>
<p>Create the pod from the <code><strong><em>pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f pod.yaml</pre>
</div>
</div>
</li>
<li>
<p>Check the container&#8217;s logs for the <code><strong>MY_ENV_VAR_REF_ENV</strong></code> value:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs -p dapi-env-test-pod</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-downward-api-container-escaping-nodes-containers-downward-api">Escaping environment variable references</h4>
<div class="paragraph">
<p>When creating a pod, you can escape an environment variable reference by using
a double dollar sign. The value will then be set to a single dollar sign version
of the provided value.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a <code><strong><em>pod.yaml</em></strong></code> file that references an existing <code>environment variable</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_NEW_ENV
          value: $$(SOME_OTHER_ENV)
  restartPolicy: Never</code></pre>
</div>
</div>
</li>
<li>
<p>Create the pod from the <code><strong><em>pod.yaml</em></strong></code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f pod.yaml</pre>
</div>
</div>
</li>
<li>
<p>Check the container&#8217;s logs for the <code><strong>MY_NEW_ENV</strong></code> value:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs -p dapi-env-test-pod</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="copying-files-to-or-from-a-container">Copying files to or from an OpenShift Enterprise container</h3>
<div class="paragraph">
<p>You can use the CLI to copy local files to or from a remote directory in a container
using the <code>rsync</code> command.</p>
</div>
<div class="sect3">
<h4 id="nodes-containers-copying-files-about-nodes-containers-copying-files">Understanding how to copy files</h4>
<div class="paragraph">
<p>The <code>oc rsync</code> command, or remote sync, is a useful tool for copying database archives to and from your pods for backup and restore purposes.
You can also use <code>oc rsync</code> to copy source code changes into a running pod for development debugging, when the running pod supports hot reload of source files.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc rsync &lt;source&gt; &lt;destination&gt; [-c &lt;container&gt;]</pre>
</div>
</div>
<div class="sect4">
<h5 id="_requirements">Requirements</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">Specifying the Copy Source</dt>
<dd>
<p>The source argument of the <code>oc rsync</code> command must point to either a local
directory or a pod directory. Individual files are not supported.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>When specifying a pod directory the directory name must be prefixed with the pod
name:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">&lt;pod name&gt;:&lt;dir&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>If the directory name ends in a path separator (<code>/</code>), only the contents of the directory are copied to the destination. Otherwise, the
directory and its contents are copied to the destination.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Specifying the Copy Destination</dt>
<dd>
<p>The destination argument of the <code>oc rsync</code> command must point to a directory. If
the directory does not exist, but <code>rsync</code> is used for copy, the directory is
created for you.</p>
</dd>
<dt class="hdlist1">Deleting Files at the Destination</dt>
<dd>
<p>The <code>--delete</code> flag may be used to delete any files in the remote directory that
are not in the local directory.</p>
</dd>
<dt class="hdlist1">Continuous Syncing on File Change</dt>
<dd>
<p>Using the <code>--watch</code> option causes the command to monitor the source path for any
file system changes, and synchronizes changes when they occur. With this
argument, the command runs forever.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Synchronization occurs after short quiet periods to ensure a
rapidly changing file system does not result in continuous synchronization
calls.</p>
</div>
<div class="paragraph">
<p>When using the <code>--watch</code> option, the behavior is effectively the same as
manually invoking <code>oc rsync</code> repeatedly, including any arguments normally passed
to <code>oc rsync</code>. Therefore, you can control the behavior via the same flags used
with manual invocations of <code>oc rsync</code>, such as <code>--delete</code>.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-copying-files-procedure-nodes-containers-copying-files">Copying files to and from containers</h4>
<div class="paragraph">
<p>Support for copying local files to or from a container is built into the CLI.</p>
</div>
<div class="paragraph">
<div class="title">Prerequisites</div>
<p>When working with <code>oc sync</code>, note the following:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">rsync must be installed</dt>
<dd>
<p>The <code>oc rsync</code> command uses the local <code>rsync</code> tool if present on the client
machine and the remote container.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>If <code>rsync</code> is not found locally or in the remote container, a <strong>tar</strong> archive
is created locally and sent to the container where the <strong>tar</strong> utility is used to
extract the files. If <strong>tar</strong> is not available in the remote container, the
copy will fail.</p>
</div>
<div class="paragraph">
<p>The <strong>tar</strong> copy method does not provide the same functionality as <code>oc rsync</code>. For
example, <code>oc rsync</code> creates the destination directory if it does not exist and
only sends files that are different between the source and the destination.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>In Windows, the <code>cwRsync</code> client should be installed and added to the PATH for
use with the <code>oc rsync</code> command.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>To copy a local directory to a pod directory:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">oc rsync &lt;local-dir&gt; &lt;pod-name&gt;:/&lt;remote-dir&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc rsync /home/user/source devpod1234:/src

WARNING: cannot use rsync: rsync not available in container
status.txt</pre>
</div>
</div>
</li>
<li>
<p>To copy a pod directory to a local directory:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc rsync devpod1234:/src /home/user/source

oc rsync devpod1234:/src/status.txt /home/user/
WARNING: cannot use rsync: rsync not available in container
status.txt</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-copying-files-rsync-nodes-containers-copying-files">Using advanced Rsync features in OpenShift Enterprise</h4>
<div class="paragraph">
<p>The <code>oc rsync</code> command exposes fewer command line options than standard <code>rsync</code>.
In the case that you wish to use a standard <code>rsync</code> command line option which is
not available in <code>oc rsync</code> (for example the <code>--exclude-from=FILE</code> option), it
may be possible to use standard <code>rsync</code> 's <code>--rsh</code> (<code>-e</code>) option or <code>RSYNC_RSH</code>
environment variable as a workaround, as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ rsync --rsh='oc rsh' --exclude-from=FILE SRC POD:DEST</pre>
</div>
</div>
<div class="paragraph">
<p>or:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ export RSYNC_RSH='oc rsh'
$ rsync --exclude-from=FILE SRC POD:DEST</pre>
</div>
</div>
<div class="paragraph">
<p>Both of the above examples configure standard <code>rsync</code> to use <code>oc rsh</code> as its
remote shell program to enable it to connect to the remote pod, and are an
alternative to running <code>oc rsync</code>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="executing-remote-commands-in-a-container">Executing remote commands in an OpenShift Enterprise container</h3>
<div class="paragraph">
<p>You can use the CLI to execute remote commands in an OpenShift Enterprise container.</p>
</div>
<div class="sect3">
<h4 id="nodes-containers-remote-commands-about-nodes-containers-remote-commands">Executing remote commands in containers</h4>
<div class="paragraph">
<p>Support for remote container command execution is built into the CLI.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To run a command in a container:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc exec &lt;pod&gt; [-c &lt;container&gt;] &lt;command&gt; [&lt;arg_1&gt; ... &lt;arg_n&gt;]</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc exec mypod date
Thu Apr  9 02:21:53 UTC 2015</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p><a href="https://access.redhat.com/errata/RHSA-2015:1650">For security purposes</a>, the
<code>oc exec</code> command does not work when accessing privileged containers except when
the command is executed by a <code>cluster-admin</code> user. See the CLI operations topic for more information.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-remote-commands-protocol-nodes-containers-remote-commands">Protocol for initiating a remote command from a client</h4>
<div class="paragraph">
<p>Clients initiate the execution of a remote command in a container by issuing a
request to the Kubernetes API server:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">/proxy/nodes/&lt;node_name&gt;/exec/&lt;namespace&gt;/&lt;pod&gt;/&lt;container&gt;?command=&lt;command&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>In the above URL:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>&lt;node_name&gt;</code> is the FQDN of the node.</p>
</li>
<li>
<p><code>&lt;namespace&gt;</code> is the project of the target pod.</p>
</li>
<li>
<p><code>&lt;pod&gt;</code> is the name of the target pod.</p>
</li>
<li>
<p><code>&lt;container&gt;</code> is the name of the target container.</p>
</li>
<li>
<p><code>&lt;command&gt;</code> is the desired command to be executed.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">/proxy/nodes/node123.openshift.com/exec/myns/mypod/mycontainer?command=date</pre>
</div>
</div>
<div class="paragraph">
<p>Additionally, the client can add parameters to the request to indicate if:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the client should send input to the remote container&#8217;s command (stdin).</p>
</li>
<li>
<p>the client&#8217;s terminal is a TTY.</p>
</li>
<li>
<p>the remote container&#8217;s command should send output from stdout to the client.</p>
</li>
<li>
<p>the remote container&#8217;s command should send output from stderr to the client.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>After sending an <code>exec</code> request to the API server, the client upgrades the
connection to one that supports multiplexed streams; the current implementation
uses <strong>SPDY</strong>.</p>
</div>
<div class="paragraph">
<p>The client creates one stream each for stdin, stdout, and stderr. To distinguish
among the streams, the client sets the <code>streamType</code> header on the stream to one
of <code>stdin</code>, <code>stdout</code>, or <code>stderr</code>.</p>
</div>
<div class="paragraph">
<p>The client closes all streams, the upgraded connection, and the underlying
connection when it is finished with the remote command execution request.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Administrators can see the Architecture guide for more information.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="using-port-forwarding-to-access-applications-in-a-container">Using port forwarding to access applications in a container</h3>
<div class="paragraph">
<p>OpenShift Enterprise supports port forwarding to pods.</p>
</div>
<div class="sect3">
<h4 id="nodes-containers-port-forwarding-about-nodes-containers-port-forwarding">Understanding port forwarding</h4>
<div class="paragraph">
<p>You can use the CLI to forward one or more local ports to a pod. This allows you
to listen on a given or random port locally, and have data forwarded to and from
given ports in the pod.</p>
</div>
<div class="paragraph">
<p>Support for port forwarding is built into the CLI:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc port-forward &lt;pod&gt; [&lt;local_port&gt;:]&lt;remote_port&gt; [...[&lt;local_port_n&gt;:]&lt;remote_port_n&gt;]</pre>
</div>
</div>
<div class="paragraph">
<p>The CLI listens on each local port specified by the user, forwarding via the protocol described below.</p>
</div>
<div class="paragraph">
<p>Ports may be specified using the following formats:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>5000</code>
</td>
<td class="hdlist2">
<p>The client listens on port 5000 locally and forwards to 5000 in the
pod.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>6000:5000</code>
</td>
<td class="hdlist2">
<p>The client listens on port 6000 locally and forwards to 5000 in
the pod.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>:5000</code> or <code>0:5000</code>
</td>
<td class="hdlist2">
<p>The client selects a free local port and forwards to 5000
in the pod.</p>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>OpenShift Enterprise handles port-forward requests from clients. Upon receiving a request, OpenShift Enterprise upgrades the response and waits for the client
to create port-forwarding streams. When OpenShift Enterprise receives a new stream, it copies data between the stream and the pods port.</p>
</div>
<div class="paragraph">
<p>Architecturally, there are options for forwarding to a pods port. The supported OpenShift Enterprise implementation invokes <code>nsenter</code> directly on the node host
to enter the pods network namespace, then invokes <code>socat</code> to copy data between the stream and the pods port. However, a custom implementation could
include running a <em>helper</em> pod that then runs <code>nsenter</code> and <code>socat</code>, so that those binaries are not required to be installed on the host.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-port-forwarding-using-nodes-containers-port-forwarding">Using port forwarding</h4>
<div class="paragraph">
<p>You can use the CLI to port-forward one or more local ports to a pod.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>Use the following command to listen on the specified port in a pod:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc port-forward &lt;pod&gt; [&lt;local_port&gt;:]&lt;remote_port&gt; [...[&lt;local_port_n&gt;:]&lt;remote_port_n&gt;]</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use the following command to listen on ports <code>5000</code> and <code>6000</code> locally and forward data to and from ports <code>5000</code> and <code>6000</code> in the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc port-forward &lt;pod&gt; 5000 6000

Forwarding from 127.0.0.1:5000 -&gt; 5000
Forwarding from [::1]:5000 -&gt; 5000
Forwarding from 127.0.0.1:6000 -&gt; 6000
Forwarding from [::1]:6000 -&gt; 6000</pre>
</div>
</div>
</li>
<li>
<p>Use the following command to listen on port <code>8888</code> locally and forward to <code>5000</code> in the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc port-forward &lt;pod&gt; 8888:5000

Forwarding from 127.0.0.1:8888 -&gt; 5000
Forwarding from [::1]:8888 -&gt; 5000</pre>
</div>
</div>
</li>
<li>
<p>Use the following command to listen on a free port locally and forward to <code>5000</code> in the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc port-forward &lt;pod&gt; :5000

Forwarding from 127.0.0.1:42390 -&gt; 5000
Forwarding from [::1]:42390 -&gt; 5000</pre>
</div>
</div>
<div class="paragraph">
<p>Or:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc port-forward &lt;pod&gt; 0:5000</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-port-forwarding-protocol-nodes-containers-port-forwarding">Protocol for initiating port forwarding from a client</h4>
<div class="paragraph">
<p>Clients initiate port forwarding to a pod by issuing a request to the
Kubernetes API server:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">/proxy/nodes/&lt;node_name&gt;/portForward/&lt;namespace&gt;/&lt;pod&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>In the above URL:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>&lt;node_name&gt;</code> is the FQDN of the node.</p>
</li>
<li>
<p><code>&lt;namespace&gt;</code> is the namespace of the target pod.</p>
</li>
<li>
<p><code>&lt;pod&gt;</code> is the name of the target pod.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">/proxy/nodes/node123.openshift.com/portForward/myns/mypod</pre>
</div>
</div>
<div class="paragraph">
<p>After sending a port forward request to the API server, the client upgrades the
connection to one that supports multiplexed streams; the current implementation
uses <a href="http://www.chromium.org/spdy"><strong>SPDY</strong></a>.</p>
</div>
<div class="paragraph">
<p>The client creates a stream with the <code>port</code> header containing the target port in
the pod. All data written to the stream is delivered via the Kubelet to the
target pod and port. Similarly, all data sent from the pod for that forwarded
connection is delivered back to the same stream in the client.</p>
</div>
<div class="paragraph">
<p>The client closes all streams, the upgraded connection, and the underlying
connection when it is finished with the port forwarding request.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="monitoring-container-health">Monitoring container health</h3>
<div class="paragraph">
<p>In software systems, components can become unhealthy due to transient issues such as temporary connectivity loss, configuration errors, or problems with external dependencies. OpenShift Enterprise applications have a number of options to detect and handle unhealthy containers.</p>
</div>
<div class="sect3">
<h4 id="nodes-containers-health-about-nodes-containers-health">Understanding health checks</h4>
<div class="paragraph">
<p>A probe is a Kubernetes action that periodically performs diagnostics on a
running container. Currently, two types of probes exist, each serving a
different purpose.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Readiness Probe</dt>
<dd>
<p>A Readiness check determines if the container in which it is scheduled is ready to service requests. If
the readiness probe fails a container, the endpoints controller ensures the
container has its IP address removed from the endpoints of all services. A
readiness probe can be used to signal to the endpoints controller that even
though a container is running, it should not receive any traffic from a proxy.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>For example, a Readiness check can control which Pods are used. When a Pod is not ready,
it is removed.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Liveness Probe</dt>
<dd>
<p>A Liveness checks determines if the container in which it is scheduled is still
running. If the liveness probe fails due to a condition such as a deadlock, the kubelet kills the container The container then
responds based on its restart policy.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>For example, a liveness probe with on a node with a <code>restartPolicy</code> of <code>Always</code> or <code>OnFailure</code>
kills and restarts the Container on the node.</p>
</div>
<div class="listingblock">
<div class="title">Sample Liveness Check</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness-http
    image: k8s.gcr.io/liveness <b class="conum">(1)</b>
    args:
    - /server
    livenessProbe: <b class="conum">(2)</b>
      httpGet:   <b class="conum">(3)</b>
        # host: my-host
        # scheme: HTTPS
        path: /healthz
        port: 8080
        httpHeaders:
        - name: X-Custom-Header
          value: Awesome
      initialDelaySeconds: 15  <b class="conum">(4)</b>
      timeoutSeconds: 1   <b class="conum">(5)</b>
    name: liveness   <b class="conum">(6)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specifies the image to use for the liveness probe.</p>
</li>
<li>
<p>Specifies the type of heath check.</p>
</li>
<li>
<p>Specifies the type of Liveness check:</p>
<div class="ulist">
<ul>
<li>
<p>HTTP Checks. Specify <code>httpGet</code>.</p>
</li>
<li>
<p>Container Execution Checks. Specify <code>exec</code>.</p>
</li>
<li>
<p>TCP Socket Check. Specify <code>tcpSocket</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Specifies the number of seconds before performing the first probe after the container starts.</p>
</li>
<li>
<p>Specifies the number of seconds between probes.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">Sample Liveness check output wth unhealthy container</div>
<div class="content">
<pre class="nowrap">$ oc describe pod pod1

....

FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
37s       37s     1   {default-scheduler }                            Normal      Scheduled   Successfully assigned liveness-exec to worker0
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image "k8s.gcr.io/busybox"
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image "k8s.gcr.io/busybox"
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e
2s        2s      1   {kubelet worker0}   spec.containers{liveness}   Warning     Unhealthy   Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory</pre>
</div>
</div>
<div class="sect4">
<h5 id="nodes-containers-health-about_types-nodes-containers-health">Understanding the types of health checks</h5>
<div class="paragraph">
<p>Liveness checks and Readiness checks can be configured in three ways:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">HTTP Checks</dt>
<dd>
<p>The kubelet uses a web hook to determine the healthiness of the container. The
check is deemed successful if the HTTP response code is between 200 and 399.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>A HTTP check is ideal for applications that return HTTP status codes
when completely initialized.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Container Execution Checks</dt>
<dd>
<p>The kubelet executes a command inside the container. Exiting the check with
status 0 is considered a success.</p>
</dd>
<dt class="hdlist1">TCP Socket Checks</dt>
<dd>
<p>The kubelet attempts to open a socket to the container. The container is only
considered healthy if the check can establish a connection. A TCP socket check is ideal for applications that do not start listening until
initialization is complete.</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-health-configuring-nodes-containers-health">Configuring health checks</h4>
<div class="paragraph">
<p>To configure health checks, create a pod for each type of check you want.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To create health checks:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a Liveness Container Execution Check:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a YAML file similar to the following:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - args:
    image: k8s.gcr.io/liveness
    livenessProbe:
      exec:  <b class="conum">(1)</b>
        command: <b class="conum">(2)</b>
        - cat
        - /tmp/health
      initialDelaySeconds: 15 <b class="conum">(3)</b>
...</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify a Liveness check and the type of Liveness check.</p>
</li>
<li>
<p>Specify the commands to use in the container.</p>
</li>
<li>
<p>Specify the number of seconds before performing the first probe after the container starts.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Verify the state of the health check pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe pod liveness-exec

Events:
  Type    Reason     Age   From                                  Message
  ----    ------     ----  ----                                  -------
  Normal  Scheduled  9s    default-scheduler                     Successfully assigned openshift-logging/liveness-exec to ip-10-0-143-40.ec2.internal
  Normal  Pulling    2s    kubelet, ip-10-0-143-40.ec2.internal  pulling image "k8s.gcr.io/liveness"
  Normal  Pulled     1s    kubelet, ip-10-0-143-40.ec2.internal  Successfully pulled image "k8s.gcr.io/liveness"
  Normal  Created    1s    kubelet, ip-10-0-143-40.ec2.internal  Created container
  Normal  Started    1s    kubelet, ip-10-0-143-40.ec2.internal  Started container</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>timeoutSeconds</code> parameter has no effect on the Readiness and Liveness
probes for Container Execution Checks. You can implement a timeout
inside the probe itself, as OpenShift Enterprise cannot time out on an exec call into
the container. One way to implement a timeout in a probe is by using the <code>timeout</code> parameter to run your
liveness or readiness probe:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">spec:
  containers:
    livenessProbe:
      exec:
        command:
          - /bin/bash
          - '-c'
          - timeout 60 /opt/eap/bin/livenessProbe.sh <b class="conum">(1)</b>
      timeoutSeconds: 1
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 3</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Timeout value and path to the probe script.</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create the check:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create a Liveness TCP Socket Check:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a YAML file similar to the following:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-tcp
spec:
  containers:
  - name: contaier1 <b class="conum">(1)</b>
    image: k8s.gcr.io/liveness
    ports:
    - containerPort: 8080 <b class="conum">(1)</b>
    livenessProbe:  <b class="conum">(2)</b>
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15 <b class="conum">(3)</b>
      timeoutSeconds: 1  <b class="conum">(4)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify the container name and port for the check to connect to.</p>
</li>
<li>
<p>Specify the Liveness heath check and the type of Liveness check.</p>
</li>
<li>
<p>Specify the number of seconds before performing the first probe after the container starts.</p>
</li>
<li>
<p>Specify the number of seconds between probes.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the check:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create an Readiness HTTP Check:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a YAML file similar to the following:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  labels:
    test: readiness
  name: readiness-http
spec:
  containers:
  - args:
    image: k8s.gcr.io/readiness <b class="conum">(1)</b>
    readinessProbe: <b class="conum">(2)</b>
    httpGet:
    # host: my-host <b class="conum">(3)</b>
    # scheme: HTTPS <b class="conum">(4)</b>
      path: /healthz
      port: 8080
    initialDelaySeconds: 15  <b class="conum">(5)</b>
    timeoutSeconds: 1  <b class="conum">(6)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify the image to use for the liveness probe.</p>
</li>
<li>
<p>Specify the Readiness heath check and the type of Readiness check.</p>
</li>
<li>
<p>Specify a host IP address. When <code>host</code> is not defined, the <code>PodIP</code> is used.</p>
</li>
<li>
<p>Specify <code>HTTP</code> or <code>HTTPS</code>. When <code>scheme</code> is not defined, the <code>HTTP</code> scheme is used.</p>
</li>
<li>
<p>Specify the number of seconds before performing the first probe after the container starts.</p>
</li>
<li>
<p>Specify the number of seconds between probes.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Create the check:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_working-with-clusters">Working with clusters</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="viewing-system-event-information-in-a-cluster">Viewing system event information in an OpenShift Enterprise cluster</h3>
<div class="paragraph">
<p>Events in OpenShift Enterprise are modeled based on events that happen to API objects
in an OpenShift Enterprise cluster.</p>
</div>
<div class="sect3">
<h4 id="nodes-containers-events-about-nodes-containers-events">Understanding events in OpenShift Enterprise</h4>
<div class="paragraph">
<p>Events allow OpenShift Enterprise to record
information about real-world events in a resource-agnostic manner. They also
allow developers and administrators to consume information about system
components in a unified way.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-events-viewing-cli-nodes-containers-events">Viewing events using the CLI</h4>
<div class="paragraph">
<p>You can get a list of events in a given project using the CLI.</p>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>To view events in a project use the following command:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get events [-n &lt;project&gt;] <b class="conum">(1)</b></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The name of the project.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get events -n openshift-config

LAST SEEN   TYPE      REASON                   OBJECT                      MESSAGE
97m         Normal    Scheduled                pod/dapi-env-test-pod       Successfully assigned openshift-config/dapi-env-test-pod to ip-10-0-171-202.ec2.internal
97m         Normal    Pulling                  pod/dapi-env-test-pod       pulling image "gcr.io/google_containers/busybox"
97m         Normal    Pulled                   pod/dapi-env-test-pod       Successfully pulled image "gcr.io/google_containers/busybox"
97m         Normal    Created                  pod/dapi-env-test-pod       Created container
9m5s        Warning   FailedCreatePodSandBox   pod/dapi-volume-test-pod    Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_dapi-volume-test-pod_openshift-config_6bc60c1f-452e-11e9-9140-0eec59c23068_0(748c7a40db3d08c07fb4f9eba774bd5effe5f0d5090a242432a73eee66ba9e22): Multus: Err adding pod to network "openshift-sdn": cannot set "openshift-sdn" ifname to "eth0": no netns: failed to Statfs "/proc/33366/ns/net": no such file or directory
8m31s       Normal    Scheduled                pod/dapi-volume-test-pod    Successfully assigned openshift-config/dapi-volume-test-pod to ip-10-0-171-202.ec2.internal</pre>
</div>
</div>
</li>
<li>
<p>To view events in your project from the OpenShift Enterprise console.</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Launch the OpenShift Enterprise console.</p>
</li>
<li>
<p>Click <strong>Home</strong> &#8594; <strong>Events</strong> and select your project.</p>
</li>
<li>
<p>Move to resource that you want to see events. For example: <strong>Home</strong> &#8594; <strong>Projects</strong> &#8594; &lt;project-name&gt; &#8594; &lt;resource-name&gt;.</p>
<div class="paragraph">
<p>Many objects, such as pods and deployments, have their own
<strong>Events</strong> tab as well, which shows events related to that object.</p>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="nodes-containers-events-list-nodes-containers-events">List of events</h4>
<div class="paragraph">
<p>This section describes the events of OpenShift Enterprise.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 8. Configuration Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedValidation</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed pod configuration validation.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 9. Container Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>BackOff</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Back-off restarting failed the container.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Created</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container created.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Failed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pull/Create/Start failed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Killing</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Killing the container.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Started</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container started.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Preempting</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Preempting other pods.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ExceededGracePeriod</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container runtime did not stop the pod within specified grace period.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 10. Health Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Unhealthy</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container is unhealthy.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 11. Image Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>BackOff</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Back off Ctr Start, image pull.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ErrImageNeverPull</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The image&#8217;s <strong>NeverPull Policy</strong> is violated.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Failed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to pull the image.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>InspectFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to inspect the image.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Pulled</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Successfully pulled the image or the container image is already present on the machine.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Pulling</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pulling the image.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 12. Image Manager Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FreeDiskSpaceFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Free disk space failed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>InvalidDiskCapacity</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Invalid disk capacity.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 13. Node Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedMount</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Volume mount failed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>HostNetworkNotSupported</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Host network not supported.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>HostPortConflict</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Host/port conflict.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>InsufficientFreeCPU</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Insufficient free CPU.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>InsufficientFreeMemory</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Insufficient free memory.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>KubeletSetupFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kubelet setup failed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NilShaper</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Undefined shaper.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NodeNotReady</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Node is not ready.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NodeNotSchedulable</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Node is not schedulable.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NodeReady</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Node is ready.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NodeSchedulable</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Node is schedulable.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NodeSelectorMismatching</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Node selector mismatch.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>OutOfDisk</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Out of disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Rebooted</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Node rebooted.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Starting</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Starting kubelet.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedAttachVolume</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to attach volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedDetachVolume</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to detach volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>VolumeResizeFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to expand/reduce volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>VolumeResizeSuccessful</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Successfully expanded/reduced volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FileSystemResizeFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to expand/reduce file system.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FileSystemResizeSuccessful</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Successfully expanded/reduced file system.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedUnMount</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to unmount volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedMapVolume</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to map a volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedUnmapDevice</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed unmaped device.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>AlreadyMountedVolume</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Volume is already mounted.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SuccessfulDetachVolume</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Volume is successfully detached.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SuccessfulMountVolume</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Volume is successfully mounted.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SuccessfulUnMountVolume</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Volume is successfully unmounted.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ContainerGCFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container garbage collection failed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ImageGCFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Image garbage collection failed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedNodeAllocatableEnforcement</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to enforce System Reserved Cgroup limit.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NodeAllocatableEnforced</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enforced System Reserved Cgroup limit.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>UnsupportedMountOption</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unsupported mount option.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SandboxChanged</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pod sandbox changed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedCreatePodSandBox</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to create pod sandbox.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedPodSandBoxStatus</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed pod sandbox status.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 14. Pod Worker Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedSync</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pod sync failed.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 15. System Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SystemOOM</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">There is an OOM (out of memory) situation on the cluster.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 16. Pod Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedKillPod</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to stop a pod.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedCreatePodContainer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to create a pod contianer.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Failed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to make pod data directories.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NetworkNotReady</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Network is not ready.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedCreate</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Error creating: <code>&lt;error-msg&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SuccessfulCreate</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Created pod: <code>&lt;pod-name&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedDelete</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Error deleting: <code>&lt;error-msg&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SuccessfulDelete</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deleted pod: <code>&lt;pod-id&gt;</code>.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 17. Horizontal Pod AutoScaler Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SelectorRequired</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Selector is required.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>InvalidSelector</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Could not convert selector into a corresponding internal selector object.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedGetObjectMetric</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">HPA was unable to compute the replica count.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>InvalidMetricSourceType</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unknown metric source type.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ValidMetricFound</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">HPA was able to successfully calculate a replica count.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedConvertHPA</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to convert the given HPA.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedGetScale</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">HPA controller was unable to get the target&#8217;s current scale.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SucceededGetScale</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">HPA controller was able to get the target&#8217;s current scale.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedComputeMetricsReplicas</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to compute desired number of replicas based on listed metrics.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedRescale</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">New size: <code>&lt;size&gt;</code>; reason: <code>&lt;msg&gt;</code>; error: <code>&lt;error-msg&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SuccessfulRescale</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">New size: <code>&lt;size&gt;</code>; reason: <code>&lt;msg&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedUpdateStatus</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to update status.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 18. Network Events (openshift-sdn)</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Starting</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Starting OpenShift-SDN.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NetworkFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The pod&#8217;s network interface has been lost and the pod will be stopped.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 19. Network Events (kube-proxy)</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>NeedPods</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The service-port <code>&lt;serviceName&gt;:&lt;port&gt;</code> needs pods.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 20. Volume Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedBinding</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">There are no persistent volumes available and no storage class is set.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>VolumeMismatch</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Volume size or class is different from what is requested in claim.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>VolumeFailedRecycle</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Error creating recycler pod.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>VolumeRecycled</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Occurs when volume is recycled.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>RecyclerPod</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Occurs when pod is recycled.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>VolumeDelete</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Occurs when volume is deleted.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>VolumeFailedDelete</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Error when deleting the volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ExternalProvisioning</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Occurs when volume for the claim is provisioned either manually or via external software.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ProvisioningFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to provision volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ProvisioningCleanupFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Error cleaning provisioned volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ProvisioningSucceeded</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Occurs when the volume is provisioned successfully.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>WaitForFirstConsumer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Delay binding until pod scheduling.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 21. Lifecycle hooks</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedPostStartHook</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Handler failed for pod start.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedPreStopHook</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Handler failed for pre-stop.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>UnfinishedPreStopHook</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pre-stop hook unfinished.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 22. Deployments</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>DeploymentCancellationFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to cancel deployment.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>DeploymentCancelled</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cancelled deployment.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>DeploymentCreated</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Created new replication controller.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>IngressIPRangeFull</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No available ingress IP to allocate to service.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 23. Scheduler  Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedScheduling</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to schedule pod: <code>&lt;pod-namespace&gt;/&lt;pod-name&gt;</code>. This event is raised for
multiple reasons, for example: <code>AssumePodVolumes</code> failed, Binding rejected etc.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Preempted</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">By <code>&lt;preemptor-namespace&gt;/&lt;preemptor-name&gt;</code> on node <code>&lt;node-name&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>Scheduled</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Successfully assigned <code>&lt;pod-name&gt;</code> to <code>&lt;node-name&gt;</code>.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 24. DaemonSet Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>SelectingAll</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This daemon set is selecting all pods. A non-empty selector is required.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedPlacement</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Failed to place pod on <code>&lt;node-name&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>FailedDaemonPod</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Found failed daemon pod <code>&lt;pod-name&gt;</code> on node <code>&lt;node-name&gt;</code>, will try to kill it.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 25. LoadBalancer Service Events</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>CreatingLoadBalancerFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Error creating load balancer.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>DeletingLoadBalancer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deleting load balancer.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>EnsuringLoadBalancer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ensuring load balancer.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>EnsuredLoadBalancer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ensured load balancer.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>UnAvailableLoadBalancer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">There are no available nodes for <code>LoadBalancer</code> service.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>LoadBalancerSourceRanges</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lists the new <code>LoadBalancerSourceRanges</code>. For example, <code>&lt;old-source-range&gt; &#8594; &lt;new-source-range&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>LoadbalancerIP</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lists the new IP address. For example, <code>&lt;old-ip&gt; &#8594; &lt;new-ip&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ExternalIP</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lists external IP address. For example, <code>Added: &lt;external-ip&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>UID</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lists the new UID. For example, <code>&lt;old-service-uid&gt; &#8594; &lt;new-service-uid&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>ExternalTrafficPolicy</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lists the new <code>ExternalTrafficPolicy</code>. For example, <code>&lt;old-policy&gt; &#8594; &lt;new-ploicy&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>HealthCheckNodePort</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lists the new <code>HealthCheckNodePort</code>. For example, <code>&lt;old-node-port&gt; &#8594; new-node-port&gt;</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>UpdatedLoadBalancer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Updated load balancer with new hosts.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>LoadBalancerUpdateFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Error updating load balancer with new hosts.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>DeletingLoadBalancer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deleting load balancer.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>DeletingLoadBalancerFailed</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Error deleting load balancer.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><strong>DeletedLoadBalancer</strong></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deleted load balancer.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect2">
<h3 id="analyzing-cluster-resource-levels">Estimating the number of pods your OpenShift Enterprise nodes can hold</h3>
<div class="paragraph">
<p>As a cluster administrator, you can use the cluster capacity tool to view the
number of pods that can be scheduled to increase the current resources before
they become exhausted, and to ensure any future pods can be scheduled. This
capacity comes from an individual node host in a cluster, and includes CPU,
memory, disk space, and others.</p>
</div>
<div class="sect3">
<h4 id="nodes-cluster-resource-levels-about-nodes-cluster-resource-levels">Understanding the OpenShift Enterprise cluster capacity tool</h4>
<div class="paragraph">
<p>The cluster capacity tool simulates a sequence of scheduling decisions to
determine how many instances of an input pod can be scheduled on the cluster
before it is exhausted of resources to provide a more accurate estimation.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The remaining allocatable capacity is a rough estimation, because it does not
count all of the resources being distributed among nodes. It analyzes only the
remaining resources and estimates the available capacity that is still
consumable in terms of a number of instances of a pod with given requirements
that can be scheduled in a cluster.</p>
</div>
<div class="paragraph">
<p>Also, pods might only have scheduling support on particular sets of nodes based
on its selection and affinity criteria. As a result, the estimation of which
remaining pods a cluster can schedule can be difficult.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can run the cluster capacity analysis tool as a stand-alone utility from
the command line, or as a job in a pod inside an OpenShift Enterprise cluster.
Running it as job inside of a pod enables you to run it multiple times without intervention.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-resource-levels-command-nodes-cluster-resource-levels">Running the cluster capacity tool on the command line</h4>
<div class="paragraph">
<p>You can run the OpenShift Enterprise cluster capacity tool from the command line
to estimate the number of pods that can be scheduled onto your cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Download and install <a href="https://github.com/kubernetes-incubator/cluster-capacity">the <strong>cluster-capacity</strong> tool</a>.</p>
</li>
<li>
<p>Create a sample pod specification file, which the tool uses for estimating resource usage. The <code>podspec</code> specifies its resource
requirements as <code>limits</code> or <code>requests</code>. The cluster capacity tool takes the
pod&#8217;s resource requirements into account for its estimation analysis.</p>
<div class="paragraph">
<p>An example of the pod specification input is:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: small-pod
  labels:
    app: guestbook
    tier: frontend
spec:
  containers:
  - name: php-redis
    image: gcr.io/google-samples/gb-frontend:v4
    imagePullPolicy: Always
    resources:
      limits:
        cpu: 150m
        memory: 100Mi
      requests:
        cpu: 150m
        memory: 100Mi</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To run the tool on the command line:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Run the following command:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ ./cluster-capacity --kubeconfig &lt;path-to-kubeconfig&gt; \ <b class="conum">(1)</b>
    --podspec &lt;path-to-pod-spec&gt; <b class="conum">(2)</b></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Specify the path to your Kubernetes configuration file.</p>
</li>
<li>
<p>Specify the path to the sample pod specification file</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>You can also add the <code>--verbose</code> option to output a detailed description of how
many pods can be scheduled on each node in the cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ ./cluster-capacity --kubeconfig &lt;path-to-kubeconfig&gt; \
    --podspec &lt;path-to-pod-spec&gt; --verbose</pre>
</div>
</div>
</li>
<li>
<p>View the output, which looks similar to the following:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">small-pod pod requirements:
	- CPU: 150m
	- Memory: 100Mi

The cluster can schedule 52 instance(s) of the pod small-pod.

Termination reason: Unschedulable: No nodes are available that match all of the
following predicates:: Insufficient cpu (2).

Pod distribution among nodes:
small-pod
	- 192.168.124.214: 26 instance(s)
	- 192.168.124.120: 26 instance(s)</pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, the number of estimated pods that can be scheduled onto
the cluster is 52.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-resource-levels-job-nodes-cluster-resource-levels">Running the cluster capacity tool as a job inside a pod</h4>
<div class="paragraph">
<p>Running the cluster capacity tool as a job inside of a pod has the advantage of
being able to be run multiple times without needing user intervention. Running
the cluster capacity tool as a job involves using a <code>ConfigMap</code>.</p>
</div>
<div class="paragraph">
<div class="title">Prerequisites</div>
<p>Download and install <a href="https://github.com/kubernetes-incubator/cluster-capacity">the <strong>cluster-capacity</strong> tool</a>.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To run the cluster capacity tool:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the cluster role:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ cat &lt;&lt; EOF| oc create -f -
kind: ClusterRole
apiVersion: v1
metadata:
  name: cluster-capacity-role
rules:
- apiGroups: [""]
  resources: ["pods", "nodes", "persistentvolumeclaims", "persistentvolumes", "services"]
  verbs: ["get", "watch", "list"]
EOF</pre>
</div>
</div>
</li>
<li>
<p>Create the service account:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create sa cluster-capacity-sa</pre>
</div>
</div>
</li>
<li>
<p>Add the role to the service account:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc adm policy add-cluster-role-to-user cluster-capacity-role \
    system:serviceaccount:default:cluster-capacity-sa</pre>
</div>
</div>
</li>
<li>
<p>Define and create the pod specification:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: small-pod
  labels:
    app: guestbook
    tier: frontend
spec:
  containers:
  - name: php-redis
    image: gcr.io/google-samples/gb-frontend:v4
    imagePullPolicy: Always
    resources:
      limits:
        cpu: 150m
        memory: 100Mi
      requests:
        cpu: 150m
        memory: 100Mi</code></pre>
</div>
</div>
</li>
<li>
<p>The cluster capacity analysis is mounted in a volume using a
<code>ConfigMap</code> named <code>cluster-capacity-configmap</code> to mount input pod spec file
<code>pod.yaml</code> into a volume <code>test-volume</code> at the path <code>/test-pod</code>.</p>
<div class="paragraph">
<p>If you haven&#8217;t created a <code>ConfigMap</code>, create one before creating the job:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create configmap cluster-capacity-configmap \
    --from-file=pod.yaml=pod.yaml</pre>
</div>
</div>
</li>
<li>
<p>Create the job using the below example of a job specification file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: cluster-capacity-job
spec:
  parallelism: 1
  completions: 1
  template:
    metadata:
      name: cluster-capacity-pod
    spec:
        containers:
        - name: cluster-capacity
          image: openshift/origin-cluster-capacity
          imagePullPolicy: "Always"
          volumeMounts:
          - mountPath: /test-pod
            name: test-volume
          env:
          - name: CC_INCLUSTER <b class="conum">(1)</b>
            value: "true"
          command:
          - "/bin/sh"
          - "-ec"
          - |
            /bin/cluster-capacity --podspec=/test-pod/pod.yaml --verbose
        restartPolicy: "Never"
        serviceAccountName: cluster-capacity-sa
        volumes:
        - name: test-volume
          configMap:
            name: cluster-capacity-configmap</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>A required environment variable letting the cluster capacity tool
 know that it is running inside a cluster as a pod.
<br>
The <code>pod.yaml</code> key of the <code>ConfigMap</code> is the same as the pod specification file
name, though it is not required. By doing this, the input pod spec file can be
accessed inside the pod as <code>/test-pod/pod.yaml</code>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Run the cluster capacity image as a job in a pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f cluster-capacity-job.yaml</pre>
</div>
</div>
</li>
<li>
<p>Check the job logs to find the number of pods that can be scheduled in the
cluster:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc logs jobs/cluster-capacity-job
small-pod pod requirements:
        - CPU: 150m
        - Memory: 100Mi

The cluster can schedule 52 instance(s) of the pod small-pod.

Termination reason: Unschedulable: No nodes are available that match all of the
following predicates:: Insufficient cpu (2).

Pod distribution among nodes:
small-pod
        - 192.168.124.214: 26 instance(s)
        - 192.168.124.120: 26 instance(s)</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="configuring-cluster-memory-to-meet-container-memory-and-risk-requirements">Configuring cluster memory to meet container memory and risk requirements in OpenShift Enterprise</h3>
<div class="paragraph">
<p>As a cluster administrator, you can help your clusters operate efficiently through
managing application memory by:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Determining the memory and risk requirements of a containerized application
component and configuring the container memory parameters to suit those
requirements.</p>
</li>
<li>
<p>Configuring containerized application runtimes (for example, OpenJDK) to adhere
optimally to the configured container memory parameters.</p>
</li>
<li>
<p>Diagnosing and resolving memory-related error conditions associated with
running in a container.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="nodes-cluster-resource-configure-about-nodes-cluster-resource-configure">Understanding managing application memory</h4>
<div class="paragraph">
<p>It is recommended to read fully the overview of how OpenShift Enterprise manages
Compute Resources before proceeding.</p>
</div>
<div class="paragraph">
<p>For each kind of resource (memory, CPU, storage), OpenShift Enterprise allows
optional <strong>request</strong> and <strong>limit</strong> values to be placed on each container in a
pod.</p>
</div>
<div class="paragraph">
<p>Note the following about memory requests and memory limits:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Memory request</strong></p>
<div class="ulist">
<ul>
<li>
<p>The memory request value, if specified, influences the OpenShift Enterprise
scheduler. The scheduler considers the memory request when scheduling a
container to a node, then fences off the requested memory on the chosen node
for the use of the container.</p>
</li>
<li>
<p>If a nodes memory is exhausted, OpenShift Enterprise prioritizes evicting its
containers whose memory usage most exceeds their memory request. In serious
cases of memory exhaustion, the node OOM killer may select and kill a
process in a container based on a similar metric.</p>
</li>
<li>
<p>The cluster administrator can assign quota or assign default values for the memory request value.</p>
</li>
<li>
<p>The cluster administrator may override the memory request values that a developer specifies, in order to manage cluster overcommit.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Memory limit</strong></p>
<div class="ulist">
<ul>
<li>
<p>The memory limit value, if specified, provides a hard limit on the memory
that can be allocated across all the processes in a container.</p>
</li>
<li>
<p>If the memory allocated by all of the processes in a container exceeds the
memory limit, the node OOM killer will immediately select and kill a
process in the container.</p>
</li>
<li>
<p>If both memory request and limit are specified, the memory limit value must
be greater than or equal to the memory request.</p>
</li>
<li>
<p>The cluster administrator can assign quota or assign default values for the memory limit value.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="nodes-cluster-resource-configure-about-memory-nodes-cluster-resource-configure">Managing application memory strategy</h5>
<div class="paragraph">
<p>The steps for sizing application memory on OpenShift Enterprise are as follows:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Determine expected container memory usage</strong></p>
<div class="paragraph">
<p>Determine expected mean and peak container memory usage, empirically if
necessary (for example, by separate load testing). Remember to consider all the
processes that may potentially run in parallel in the container: for example,
does the main application spawn any ancillary scripts?</p>
</div>
</li>
<li>
<p><strong>Determine risk appetite</strong></p>
<div class="paragraph">
<p>Determine risk appetite for eviction. If the risk appetite is low, the
container should request memory according to the expected peak usage plus a
percentage safety margin. If the risk appetite is higher, it may be more
appropriate to request memory according to the expected mean usage.</p>
</div>
</li>
<li>
<p><strong>Set container memory request</strong></p>
<div class="paragraph">
<p>Set container memory request based on the above. The more accurately the
request represents the application memory usage, the better. If the request is
too high, cluster and quota usage will be inefficient. If the request is too
low, the chances of application eviction increase.</p>
</div>
</li>
<li>
<p><strong>Set container memory limit, if required</strong></p>
<div class="paragraph">
<p>Set container memory limit, if required. Setting a limit has the effect of
immediately killing a container process if the combined memory usage of all
processes in the container exceeds the limit, and is therefore a mixed blessing.
On the one hand, it may make unanticipated excess memory usage obvious early
("fail fast"); on the other hand it also terminates processes abruptly.</p>
</div>
<div class="paragraph">
<p>Note that some OpenShift Enterprise clusters may require a limit value to be set;
some may override the request based on the limit; and some application images
rely on a limit value being set as this is easier to detect than a request
value.</p>
</div>
<div class="paragraph">
<p>If the memory limit is set, it should not be set to less than the expected peak
container memory usage plus a percentage safety margin.</p>
</div>
</li>
<li>
<p><strong>Ensure application is tuned</strong></p>
<div class="paragraph">
<p>Ensure application is tuned with respect to configured request and limit values,
if appropriate. This step is particularly relevant to applications which pool
memory, such as the JVM. The rest of this page discusses this.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-resource-configure-jdk-nodes-cluster-resource-configure">Understanding OpenJDK settings for OpenShift Enterprise</h4>
<div class="paragraph">
<p>The default OpenJDK settings do not work well with containerized
environments. As a result, some additional Java memory
settings must always be provided whenever running the OpenJDK in a container.</p>
</div>
<div class="paragraph">
<p>The JVM memory layout is complex, version dependent, and describing it in detail
is beyond the scope of this documentation. However, as a starting point for
running OpenJDK in a container, at least the following three memory-related
tasks are key:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Overriding the JVM maximum heap size.</p>
</li>
<li>
<p>Encouraging the JVM to release unused memory to the operating system, if
appropriate.</p>
</li>
<li>
<p>Ensuring all JVM processes within a container are appropriately configured.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Optimally tuning JVM workloads for running in a container is beyond the scope of
this documentation, and may involve setting multiple additional JVM options.</p>
</div>
<div class="sect4">
<h5 id="nodes-cluster-resource-configure-jdk-heap-nodes-cluster-resource-configure">Understanding how to override the JVM maximum heap size</h5>
<div class="paragraph">
<p>For many Java workloads, the JVM heap is the largest single consumer of memory.
Currently, the OpenJDK defaults to allowing up to 1/4 (1/<code>-XX:MaxRAMFraction</code>)
of the compute nodes memory to be used for the heap, regardless of whether the
OpenJDK is running in a container or not. It is therefore <strong>essential</strong> to
override this behavior, especially if a container memory limit is also set.</p>
</div>
<div class="paragraph">
<p>There are at least two ways the above can be achieved:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If the container memory limit is set and the experimental options are
supported by the JVM, set <code>-XX:+UnlockExperimentalVMOptions
-XX:+UseCGroupMemoryLimitForHeap</code>.</p>
<div class="paragraph">
<p>This sets <code>-XX:MaxRAM</code> to the container memory limit, and the maximum heap size
(<code>-XX:MaxHeapSize</code> / <code>-Xmx</code>) to 1/<code>-XX:MaxRAMFraction</code> (1/4 by default).</p>
</div>
</li>
<li>
<p>Directly override one of <code>-XX:MaxRAM</code>, <code>-XX:MaxHeapSize</code> or <code>-Xmx</code>.</p>
<div class="paragraph">
<p>This option involves hard-coding a value, but has the advantage of allowing a
safety margin to be calculated.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="nodes-cluster-resource-configure-jdk-unused-nodes-cluster-resource-configure">Understanding how to encourage the JVM to release unused memory to the operating system</h5>
<div class="paragraph">
<p>By default, the OpenJDK does not aggressively return unused memory to the
operating system. This may be appropriate for many containerized Java
workloads, but notable exceptions include workloads where additional active
processes co-exist with a JVM within a container, whether those additional
processes are native, additional JVMs, or a combination of the two.</p>
</div>
<div class="paragraph">
<p>The OpenShift Enterprise Jenkins maven slave image uses the following JVM arguments to encourage the JVM
to release unused memory to the operating system: <code>-XX:+UseParallelGC
-XX:MinHeapFreeRatio=5 -XX:MaxHeapFreeRatio=10 -XX:GCTimeRatio=4
-XX:AdaptiveSizePolicyWeight=90</code>. These arguments are intended to return heap
memory to the operating system whenever allocated memory exceeds 110% of in-use
memory (<code>-XX:MaxHeapFreeRatio</code>), spending up to 20% of CPU time in the garbage
collector (<code>-XX:GCTimeRatio</code>). At no time will the application heap allocation
be less than the initial heap allocation (overridden by <code>-XX:InitialHeapSize</code> /
<code>-Xms</code>). Detailed additional information is available
<a href="https://developers.redhat.com/blog/2014/07/15/dude-wheres-my-paas-memory-tuning-javas-footprint-in-openshift-part-1/">Tuning Java&#8217;s footprint in OpenShift (Part 1)</a>,
<a href="https://developers.redhat.com/blog/2014/07/22/dude-wheres-my-paas-memory-tuning-javas-footprint-in-openshift-part-2/">Tuning Java&#8217;s footprint in OpenShift (Part 2)</a>,
and at
<a href="https://developers.redhat.com/blog/2017/04/04/openjdk-and-containers/">OpenJDK
and Containers</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="nodes-cluster-resource-configure-jdk-proc-nodes-cluster-resource-configure">Understanding how to ensure all JVM processes within a container are appropriately configured</h5>
<div class="paragraph">
<p>In the case that multiple JVMs run in the same container, it is essential to
ensure that they are all configured appropriately. For many workloads it will
be necessary to grant each JVM a percentage memory budget, leaving a perhaps
substantial additional safety margin.</p>
</div>
<div class="paragraph">
<p>Many Java tools use different environment variables (<code>JAVA_OPTS</code>, <code>GRADLE_OPTS</code>,
<code>MAVEN_OPTS</code>, and so on) to configure their JVMs and it can be challenging to ensure
that the right settings are being passed to the right JVM.</p>
</div>
<div class="paragraph">
<p>The <code>JAVA_TOOL_OPTIONS</code> environment variable is always respected by the OpenJDK,
and values specified in <code>JAVA_TOOL_OPTIONS</code> will be overridden by other options
specified on the JVM command line. By default, the OpenShift Enterprise Jenkins
maven slave image sets <code>JAVA_TOOL_OPTIONS="-XX:+UnlockExperimentalVMOptions
-XX:+UseCGroupMemoryLimitForHeap -Dsun.zip.disableMemoryMapping=true"</code> to ensure
that these options are used by default for all JVM workloads run in the slave
image. This does not guarantee that additional options are not required, but is
intended to be a helpful starting point.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-resource-configure-request-limit-nodes-cluster-resource-configure">Finding the memory request and limit from within a pod</h4>
<div class="paragraph">
<p>An application wishing to dynamically discover its memory request and limit from
within a pod should use the Downward API.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Configure the pod to add the <code>MEMORY_REQUEST</code> and <code>MEMORY_LIMIT</code> stanzas:</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - name: test
    image: fedora:latest
    command:
    - sleep
    - "3600"
    env:
    - name: MEMORY_REQUEST <b class="conum">(1)</b>
      valueFrom:
        resourceFieldRef:
          containerName: test
          resource: requests.memory
    - name: MEMORY_LIMIT <b class="conum">(2)</b>
      valueFrom:
        resourceFieldRef:
          containerName: test
          resource: limits.memory
    resources:
      requests:
        memory: 384Mi
      limits:
        memory: 512Mi</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Add this stanza to discover the application memory request value.</p>
</li>
<li>
<p>Add this stanza to discover the application memory limit value.</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
<li>
<p>Access the pod using a remote shell:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc rsh test</pre>
</div>
</div>
</li>
<li>
<p>Check that the requested values were applied:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ env | grep MEMORY | sort
MEMORY_LIMIT=536870912
MEMORY_REQUEST=402653184</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The memory limit value can also be read from inside the container by the
<code>/sys/fs/cgroup/memory/memory.limit_in_bytes</code> file.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-resource-configure-oom-nodes-cluster-resource-configure">Understanding OOM kill policy in OpenShift Enterprise</h4>
<div class="paragraph">
<p>OpenShift Enterprise may kill a process in a container if the total memory usage of
all the processes in the container exceeds the memory limit, or in serious cases
of node memory exhaustion.</p>
</div>
<div class="paragraph">
<p>When a process is OOM killed, this may or may not result in the container
exiting immediately. If the container PID 1 process receives the <strong>SIGKILL</strong>, the
container will exit immediately. Otherwise, the container behavior is
dependent on the behavior of the other processes.</p>
</div>
<div class="paragraph">
<p>For example, a container process exited with code 137, indicating it received a SIGKILL signal.</p>
</div>
<div class="paragraph">
<p>If the container does not exit immediately, an OOM kill is detectable as
follows:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Access the pod using a remote shell:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap"># oc rsh test</pre>
</div>
</div>
</li>
<li>
<p>The oom_kill counter in <code>/sys/fs/cgroup/memory/memory.oom_control</code> is
incremented</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ grep '^oom_kill ' /sys/fs/cgroup/memory/memory.oom_control
oom_kill 0
$ sed -e '' &lt;/dev/zero  # provoke an OOM kill
Killed
$ echo $?
137
$ grep '^oom_kill ' /sys/fs/cgroup/memory/memory.oom_control
oom_kill 1</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>If one or more processes in a pod are OOM killed, when the pod subsequently
exits, whether immediately or not, it will have phase <strong>Failed</strong> and reason
<strong>OOMKilled</strong>. An OOM killed pod may be restarted depending on the value of
<code>restartPolicy</code>. If not restarted, controllers such as the
ReplicationController will notice the pods failed status and create a new pod
to replace the old one.</p>
</div>
<div class="paragraph">
<p>If not restarted, the pod status is as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pod test
NAME      READY     STATUS      RESTARTS   AGE
test      0/1       OOMKilled   0          1m

$ oc get pod test -o yaml
...
status:
  containerStatuses:
  - name: test
    ready: false
    restartCount: 0
    state:
      terminated:
        exitCode: 137
        reason: OOMKilled
  phase: Failed</pre>
</div>
</div>
<div class="paragraph">
<p>If restarted, its status is as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pod test
NAME      READY     STATUS    RESTARTS   AGE
test      1/1       Running   1          1m

$ oc get pod test -o yaml
...
status:
  containerStatuses:
  - name: test
    ready: true
    restartCount: 1
    lastState:
      terminated:
        exitCode: 137
        reason: OOMKilled
    state:
      running:
  phase: Running</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-resource-configure-evicted-nodes-cluster-resource-configure">Understanding pod eviction</h4>
<div class="paragraph">
<p>OpenShift Enterprise may evict a pod from its node when the nodes memory is
exhausted. Depending on the extent of memory exhaustion, the eviction may or
may not be graceful. Graceful eviction implies the main process (PID 1) of each
container receiving a SIGTERM signal, then some time later a SIGKILL signal if
the process has not exited already. Non-graceful eviction implies the main
process of each container immediately receiving a SIGKILL signal.</p>
</div>
<div class="paragraph">
<p>An evicted pod will have phase <strong>Failed</strong> and reason <strong>Evicted</strong>. It will not be
restarted, regardless of the value of <code>restartPolicy</code>. However, controllers
such as the ReplicationController will notice the pods failed status and create
a new pod to replace the old one.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get pod test
NAME      READY     STATUS    RESTARTS   AGE
test      0/1       Evicted   0          1m

$ oc get pod test -o yaml
...
status:
  message: 'Pod The node was low on resource: [MemoryPressure].'
  phase: Failed
  reason: Evicted</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="configuring-your-cluster-to-place-pods-on-overcommited-nodes">Configuring your cluster to place pods on overcommited nodes in OpenShift Enterprise</h3>
<div class="paragraph">
<p>In an <em>overcommited</em> state, the sum of the container compute resource requests and limits exceeds the resources available on the system.
Overcommitment might be desirable in development environments where a tradeoff of guaranteed performance for capacity is acceptable.</p>
</div>
<div class="sect3">
<h4 id="nodes-cluster-overcommit-about-nodes-cluster-overcommit">Understanding overcommitment in OpenShift Enterprise</h4>
<div class="paragraph">
<p>Requests and limits enable administrators to allow and manage the overcommitment of resources on a node. The scheduler uses requests for scheduling your container and providing a minimum service guarantee. Limits constrain the amount of compute resource that may be consumed on your node.</p>
</div>
<div class="paragraph">
<p>OpenShift Enterprise administrators can control the level of overcommit and manage container density on nodes by configuring masters to override the ratio between request and limit set on developer containers. In conjunction with a per-project LimitRange specifying limits and defaults, this adjusts the container limit and request to achieve the desired level of overcommit.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>That these overrides have no effect if no limits have been set on containers. Create a LimitRange object with default limits (per individual project, or in the project template) in order to ensure that the overrides apply.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After these overrides, the container limits and requests must still be validated by any LimitRange objects in the project. It is possible, for example, for developers to specify a limit close to the minimum limit, and have the request then be overridden below the minimum limit, causing the pod to be forbidden. This unfortunate user experience should be addressed with future work, but for now, configure this capability and LimitRanges with caution.</p>
</div>
</div>
<div class="sect3">
<h4 id="resource-requests-nodes-cluster-overcommit">Understanding resource requests and overcommitment in OpenShift Enterprise</h4>
<div class="paragraph">
<p>For each compute resource, a container may specify a resource request and limit.
Scheduling decisions are made based on the request to ensure that a node has
enough capacity available to meet the requested value. If a container specifies
limits, but omits requests, the requests are defaulted to the limits. A
container is not able to exceed the specified limit on the node.</p>
</div>
<div class="paragraph">
<p>The enforcement of limits is dependent upon the compute resource type. If a
container makes no request or limit, the container is scheduled to a node with
no resource guarantees. In practice, the container is able to consume as much of
the specified resource as is available with the lowest local priority. In low
resource situations, containers that specify no resource requests are given the
lowest quality of service.</p>
</div>
<div class="paragraph">
<p>Scheduling is based on resources requested, while quota and hard limits refer
to resource limits, which can be set higher than requested resources. The
difference between request and limit determines the level of overcommit;
for instance, if a container is given a memory request of 1Gi and a memory limit
of 2Gi, it is scheduled based on the 1Gi request being available on the node,
but could use up to 2Gi; so it is 200% overcommitted.</p>
</div>
<div class="sect4">
<h5 id="understandin-fluentd-buffering-nodes-cluster-overcommit">Understanding Fluentd file buffering</h5>
<div class="paragraph">
<p>If the Fluentd logger is unable to keep up with a high number of logs, it will need
to switch to file buffering to reduce memory usage and prevent data loss.</p>
</div>
<div class="paragraph">
<p>Fluentd file buffering stores records in <em>chunks</em>. Chunks are stored in <em>buffers</em>.</p>
</div>
<div class="paragraph">
<p>The Fluentd <code>buffer_chunk_limit</code> is determined by the environment variable
<code>BUFFER_SIZE_LIMIT</code>, which has the default value <code>8m</code>. The file buffer size per
output is determined by the environment variable <code>FILE_BUFFER_LIMIT</code>, which has
the default value <code>256Mi</code>. The permanent volume size must be larger than
<code>FILE_BUFFER_LIMIT</code> multiplied by the output.</p>
</div>
<div class="paragraph">
<p>On the Fluentd pods, permanent volume <strong>/var/lib/fluentd</strong> should be
prepared by the PVC or hostmount, for example. That area is then used for the
file buffers.</p>
</div>
<div class="paragraph">
<p>The <code>buffer_type</code> and <code>buffer_path</code> are configured in the Fluentd configuration files as
follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ egrep "buffer_type|buffer_path" *.conf
output-es-config.conf:
  buffer_type file
  buffer_path `/var/lib/fluentd/buffer-output-es-config`
output-es-ops-config.conf:
  buffer_type file
  buffer_path `/var/lib/fluentd/buffer-output-es-ops-config`</pre>
</div>
</div>
<div class="paragraph">
<p>The Fluentd <code>buffer_queue_limit</code> is the value of the variable <code>BUFFER_QUEUE_LIMIT</code>. This value is <code>32</code> by default.</p>
</div>
<div class="paragraph">
<p>The environment variable <code>BUFFER_QUEUE_LIMIT</code> is calculated as <code>(FILE_BUFFER_LIMIT / (number_of_outputs * BUFFER_SIZE_LIMIT))</code>.</p>
</div>
<div class="paragraph">
<p>If the <code>BUFFER_QUEUE_LIMIT</code> variable has the default set of values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>FILE_BUFFER_LIMIT = 256Mi</code></p>
</li>
<li>
<p><code>number_of_outputs = 1</code></p>
</li>
<li>
<p><code>BUFFER_SIZE_LIMIT = 8Mi</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The value of <code>buffer_queue_limit</code> will be <code>32</code>. To change the <code>buffer_queue_limit</code>, you need to change the value of <code>FILE_BUFFER_LIMIT</code>.</p>
</div>
<div class="paragraph">
<p>In this formula, <code>number_of_outputs</code> is <code>1</code> if all the logs are sent to a single resource, and it is incremented by <code>1</code> for each additional resource. For example, the value of <code>number_of_outputs</code> is:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>1</code> - if all logs are sent to a single ElasticSearch pod</p>
</li>
<li>
<p><code>2</code> - if application logs are sent to an ElasticSearch pod and ops logs are sent to
another ElasticSearch pod</p>
</li>
<li>
<p><code>4</code> - if application logs are sent to an ElasticSearch pod, ops logs are sent to
another ElasticSearch pod, and both of them are forwarded to other Fluentd instances</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-overcommit-reserving-memory-nodes-cluster-overcommit">Understanding compute resources and containers</h4>
<div class="paragraph">
<p>The node-enforced behavior for compute resources is specific to the resource
type.</p>
</div>
<div class="sect4">
<h5 id="understanding-container-CPU-requests-nodes-cluster-overcommit">Understanding container CPU requests</h5>
<div class="paragraph">
<p>A container is guaranteed the amount of CPU it requests and is additionally able
to consume excess CPU available on the node, up to any limit specified by the
container. If multiple containers are attempting to use excess CPU, CPU time is
distributed based on the amount of CPU requested by each container.</p>
</div>
<div class="paragraph">
<p>For example, if one container requested 500m of CPU time and another container
requested 250m of CPU time, then any extra CPU time available on the node is
distributed among the containers in a 2:1 ratio. If a container specified a
limit, it will be throttled not to use more CPU than the specified limit.
CPU requests are enforced using the CFS shares support in the Linux kernel. By
default, CPU limits are enforced using the CFS quota support in the Linux kernel
over a 100ms measuring interval, though this can be disabled.</p>
</div>
</div>
<div class="sect4">
<h5 id="understanding-memory-requests-container-nodes-cluster-overcommit">Understanding container memory requests</h5>
<div class="paragraph">
<p>A container is guaranteed the amount of memory it requests. A container can use
more memory than requested, but once it exceeds its requested amount, it could
be terminated in a low memory situation on the node.
If a container uses less memory than requested, it will not be terminated unless
system tasks or daemons need more memory than was accounted for in the node&#8217;s
resource reservation. If a container specifies a limit on memory, it is
immediately terminated if it exceeds the limit amount.</p>
</div>
</div>
<div class="sect4">
<h5 id="containers-ephemeral-nodes-cluster-overcommit">Understanding containers and ephemeral storage</h5>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>This topic applies only if you enabled the ephemeral storage technology preview.
This feature is disabled by default. If enabled, the
OpenShift Enterprise cluster uses ephemeral storage to store information that does
not need to persist after the cluster is destroyed. To enable this feature, see
configuring for ephemeral storage.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>A container is guaranteed the amount of ephemeral storage it requests. A
container can use more ephemeral storage than requested, but once it exceeds its
requested amount, it can be terminated if the available ephemeral disk space gets
too low.</p>
</div>
<div class="paragraph">
<p>If a container uses less ephemeral storage than requested, it will not be
terminated unless system tasks or daemons need more local ephemeral storage than
was accounted for in the node&#8217;s resource reservation. If a container specifies a
limit on ephemeral storage, it is immediately terminated if it exceeds the limit
amount.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-overcommit-qos-about-nodes-cluster-overcommit">Understanding overcomitment and quality of service classes</h4>
<div class="paragraph">
<p>A node is <em>overcommitted</em> when it has a pod scheduled that makes no request, or
when the sum of limits across all pods on that node exceeds available machine
capacity.</p>
</div>
<div class="paragraph">
<p>In an overcommitted environment, it is possible that the pods on the node will
attempt to use more compute resource than is available at any given point in
time. When this occurs, the node must give priority to one pod over another. The
facility used to make this decision is referred to as a Quality of Service (QoS)
Class.</p>
</div>
<div class="paragraph">
<p>For each compute resource, a container is divided into one of three QoS classes
with decreasing order of priority:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 26. Quality of Service Classes</caption>
<colgroup>
<col style="width: 14.2857%;">
<col style="width: 14.2857%;">
<col style="width: 71.4286%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Priority</th>
<th class="tableblock halign-left valign-top">Class Name</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1 (highest)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Guaranteed</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">If limits and optionally requests are set (not equal to 0) for all resources
and they are equal, then the container is classified as <strong>Guaranteed</strong>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Burstable</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">If requests and optionally limits are set (not equal to 0) for all resources,
and they are not equal, then the container is classified as <strong>Burstable</strong>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">3 (lowest)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>BestEffort</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">If requests and limits are not set for any of the resources, then the container
is classified as <strong>BestEffort</strong>.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Memory is an incompressible resource, so in low memory situations, containers
that have the lowest priority are terminated first:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Guaranteed</strong> containers are considered top priority, and are guaranteed to
only be terminated if they exceed their limits, or if the system is under memory
pressure and there are no lower priority containers that can be evicted.</p>
</li>
<li>
<p><strong>Burstable</strong> containers under system memory pressure are more likely to be
terminated once they exceed their requests and no other <strong>BestEffort</strong> containers
exist.</p>
</li>
<li>
<p><strong>BestEffort</strong> containers are treated with the lowest priority. Processes in
these containers are first to be terminated if the system runs out of memory.</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="qos-about-reserve-nodes-cluster-overcommit">Understanding how to reserve memory across quality of service tiers</h5>
<div class="paragraph">
<p>You can use the <code>qos-reserved</code> parameter to specify a percentage of memory to be reserved
by a pod in a particular QoS level. This feature attempts to reserve requested resources to exclude pods
from lower OoS classes from using resources requested by pods in higher QoS classes.</p>
</div>
<div class="paragraph">
<p>OpenShift Enterprise uses the <code>qos-reserved</code> parameter as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A value of <code>qos-reserved=memory=100%</code> will prevent the <code>Burstable</code> and <code>BestEffort</code> QOS classes from consuming memory
that was requested by a higher QoS class. This increases the risk of inducing OOM
on <code>BestEffort</code> and <code>Burstable</code> workloads in favor of increasing memory resource guarantees
for <code>Guaranteed</code> and <code>Burstable</code> workloads.</p>
</li>
<li>
<p>A value of <code>qos-reserved=memory=50%</code> will allow the <code>Burstable</code> and <code>BestEffort</code> QOS classes
to consume half of the memory requested by a higher QoS class.</p>
</li>
<li>
<p>A value of <code>qos-reserved=memory=0%</code>
will allow a <code>Burstable</code> and <code>BestEffort</code> QoS classes to consume up to the full node
allocatable amount if available, but increases the risk that a <code>Guaranteed</code> workload
will not have access to requested memory. This condition effectively disables this feature.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="qos-about-swap-nodes-cluster-overcommit">Understanding swap memory and QOS</h4>
<div class="paragraph">
<p>You can disable swap by default on your nodes in order to preserve quality of
service (QOS) guarantees. Otherwise, physical resources on a node can oversubscribe,
affecting the resource guarantees the Kubernetes scheduler makes during pod
placement.</p>
</div>
<div class="paragraph">
<p>For example, if two guaranteed pods have reached their memory limit, each
container could start using swap memory. Eventually, if there is not enough swap
space, processes in the pods can be terminated due to the system being
oversubscribed.</p>
</div>
<div class="paragraph">
<p>Failing to disable swap results in nodes not recognizing that they are
experiencing <strong>MemoryPressure</strong>, resulting in pods not receiving the memory they
made in their scheduling request. As a result, additional pods are placed on the
node to further increase memory pressure, ultimately increasing your risk of
experiencing a system out of memory (OOM) event.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>If swap is enabled, any out-of-resource handling eviction thresholds for available memory will not work as
expected. Take advantage of out-of-resource handling to allow pods to be evicted
from a node when it is under memory pressure, and rescheduled on an alternative
node that has no such pressure.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-overcommit-configure-nodes-nodes-cluster-overcommit">Understanding nodes overcommitment</h4>
<div class="paragraph">
<p>In an overcommitted environment, it is important to properly configure your node to provide best system behavior.</p>
</div>
<div class="paragraph">
<p>When the node starts, it ensures that the kernel tunable flags for memory
management are set properly. The kernel should never fail memory allocations
unless it runs out of physical memory.</p>
</div>
<div class="paragraph">
<p>In an overcommitted environment, it is important to properly configure your node to provide best system behavior.</p>
</div>
<div class="paragraph">
<p>When the node starts, it ensures that the kernel tunable flags for memory
management are set properly. The kernel should never fail memory allocations
unless it runs out of physical memory.</p>
</div>
<div class="paragraph">
<p>To ensure this behavior, OpenShift Enterprise configures the kernel to always overcommit
memory by setting the <code>vm.overcommit_memory</code> parameter to <code>1</code>, overriding the
default operating system setting.</p>
</div>
<div class="paragraph">
<p>OpenShift Enterprise also configures the kernel not to panic when it runs out of memory
by setting the <code>vm.panic_on_oom</code> parameter to <code>0</code>. A setting of 0 instructs the
kernel to call oom_killer in an Out of Memory (OOM) condition, which kills
processes based on priority</p>
</div>
<div class="paragraph">
<p>You can view the current setting by running the following commands on your node:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ sysctl -a |grep commit

vm.overcommit_memory = 0</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ sysctl -a |grep panic
vm.panic_on_oom = 0</pre>
</div>
</div>
<div class="paragraph">
<p>You can change these settings using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ sysctl -w vm.overcommit_memory=1
$ sysctl -w vm.panic_on_oom=0</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The above flags should already be set on nodes, and no further action is
required.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can also perform the following configurations for each node:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Disable or enforce CPU limits using CPU CFS quotas</p>
</li>
<li>
<p>Reserve resources for system processes</p>
</li>
<li>
<p>Reserve memory across quality of service tiers</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="nodes-cluster-overcommit-node-enforcing-nodes-cluster-overcommit">Disabling or enforcing CPU limits using CPU CFS quotas</h5>
<div class="paragraph">
<p>Nodes by default enforce specified CPU limits using the Completely Fair Scheduler (CFS) quota support in
the Linux kernel.</p>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>Obtain the label associated with the static Machine Config Pool CRD for the type of node you want to configure.
Perform one of the following steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>View the Machine Config Pool:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc describe machineconfigpool &lt;name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">$ oc describe machineconfigpool worker

apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: 2019-02-08T14:52:39Z
  generation: 1
  labels:
    custom-kubelet: small-pods <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>If a label has been added it appears under <code>labels</code>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>If the label is not present, add a key/value pair:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a Custom Resource (CR) for your configuration change.</p>
<div class="listingblock">
<div class="title">Sample configuration for a disabling CPU limits</div>
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: disable-cpu-units <b class="conum">(1)</b>
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: small-pods <b class="conum">(2)</b>
  kubeletConfig:
    cpu-cfs-quota: <b class="conum">(3)</b>
      - "false"</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Assign a name to CR.</p>
</li>
<li>
<p>Specify the label to apply the configuration change.</p>
</li>
<li>
<p>Set the <code>cpu-cfs-quota</code> parameter to <code>false</code>.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>If CPU limit enforcement is disabled, it is important to understand the impact that will have on your node:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If a container makes a request for CPU, it will continue to be enforced by CFS
shares in the Linux kernel.</p>
</li>
<li>
<p>If a container makes no explicit request for CPU, but it does specify a limit,
the request will default to the specified limit, and be enforced by CFS shares
in the Linux kernel.</p>
</li>
<li>
<p>If a container specifies both a request and a limit for CPU, the request will
be enforced by CFS shares in the Linux kernel, and the limit will have no
impact on the node.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="nodes-cluster-overcommit-node-resources-nodes-cluster-overcommit">Reserving resources for system processes</h5>
<div class="paragraph">
<p>To provide more reliable scheduling and minimize node resource overcommitment,
each node can reserve a portion of its resources for use by system daemons
that are required to run on your node for your cluster to function (<strong>sshd</strong>, etc.).
In particular, it is recommended that you reserve resources for incompressible resources such as memory.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To explicitly reserve resources for non-pod processes, allocate node resources by specifying resources
available for scheduling. See Allocating Node Resources for more details.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-overcommit-project-disable-nodes-cluster-overcommit">Disabling overcommitment for a project</h4>
<div class="paragraph">
<p>When configured, overcommitment can be disabled per-project.
For example, you can allow infrastructure components to be configured independently of overcommitment.</p>
</div>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To disable overcommitment in a project:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Edit the project object file</p>
</li>
<li>
<p>Add the following annotation:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">quota.openshift.io/cluster-resource-override-enabled: "false"</pre>
</div>
</div>
</li>
<li>
<p>Create the project object:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc create -f &lt;file-name&gt;.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="disabling-features-using-feature-gates">Disabling OpenShift Enterprise features using feature gates</h3>
<div class="paragraph">
<p>As an administrator, you can turn on and off features that are Technical Preview features.</p>
</div>
<div class="sect3">
<h4 id="nodes-pods-cluster-features-about-nodes-cluster-disabling">Understanding how to disable features</h4>
<div class="paragraph">
<p>You can use the Feature Gates Custom Resource to toggle on and off Technical Preview features throughout your cluster.</p>
</div>
<div class="paragraph">
<p>This allows you, for example, to ensure that Technical Preview features are off for production clusters while leaving the features on for test clusters where you can
fully test them.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodes-cluster-disabling-features-cluster-nodes-cluster-disabling">Enabling Technical Preview features using feature gates</h4>
<div class="paragraph">
<p>You can turn Technical Preview features on and off for all nodes in the cluster
by editing the Feature Gates Custom Resource, named <strong>cluster</strong>, in the
<strong>openshift-config</strong> project.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Turning on Technical Preview features cannot be undone and prevents upgrades.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The following features are affected by Feature Gates.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Feature gate</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>ExperimentalCriticalPodAnnotation</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enables annotating specific pods as critical so that their scheduling is guaranteed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>RotateKubeletServerCertificate</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enables the rotation of the server TLS certificate on the cluster.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>SupportPodPidsLimit</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enables support for limiting the number of processes (PIDs) running in a pod.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To turn on the Technical Preview features for the entire cluster:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the Feature Gates instance:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Switch to the the <strong>Administration</strong> &#8594; <strong>Custom Resource Definitions</strong> page.</p>
</li>
<li>
<p>On the <strong>Custom Resource Definitions</strong> page, click <strong>FeatureGate</strong>.</p>
</li>
<li>
<p>On the <strong>Custom Resource Definitions</strong> page, click the <strong>Actions Menu</strong> and select <strong>View Instances</strong>.</p>
</li>
<li>
<p>On the <strong>Feature Gates</strong> page, click <strong>Create Feature Gates</strong>.</p>
</li>
<li>
<p>Replace the code with following:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight nowrap"><code class="language-yaml" data-lang="yaml">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster
spec: {}</code></pre>
</div>
</div>
</li>
<li>
<p>Click <strong>Create</strong>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>To turn on the Technical Preview features, change the <code>spec</code> parameter to:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster
spec:
  featureSet: TechPreviewNoUpgrade <b class="conum">(1)</b></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Add <code>featureSet: TechPreviewNoUpgrade</code> to enable the <code>ExperimentalCriticalPodAnnotation</code>, <code>RotateKubeletServerCertificate</code>, and <code>SupportPodPidsLimit</code> features.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Turning on Technical Preview features cannot be undone and prevents upgrades.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>To turn off the Technical Preview features, change the <code>spec</code> parameter to:</p>
<div class="listingblock">
<div class="content">
<pre class="nowrap">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster
spec: {} <b class="conum">(1)</b></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Replace <code>featureSet: TechPreviewNoUpgrade</code> with <code>{}</code> to disable the <code>ExperimentalCriticalPodAnnotation</code>, <code>RotateKubeletServerCertificate</code>, and <code>SupportPodPidsLimit</code> features.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Turning off Technical Preview features removes the features but upgrading of your cluster remains disabled.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>After creating the CR, use the following command to get the name of the CR for editing as needed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">$ oc get FeatureGate cluster -n openshift-config
NAME      AGE
cluster   18d</pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2019-04-12 23:56:12 UTC
</div>
</div>
</body>
</html>